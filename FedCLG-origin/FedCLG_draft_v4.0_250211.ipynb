{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict, defaultdict\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.functional as func\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from utils.language_utils import word_to_indices, letter_to_vec\n",
    "from utils.ShakeSpeare_reduce import ShakeSpeare\n",
    "\n",
    "from models.lstm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v4.0\n",
    "\n",
    "加上shake数据集的测试，也保留原有CIFAR100的逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # 解决由于多次加载 OpenMP 相关动态库而引起的冲突"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.executable)\n",
    "\n",
    "# print(torch.cuda.is_available())\n",
    "# print(torch.cuda.get_device_capability())\n",
    "\n",
    "# gpu_info = !nvidia-smi\n",
    "# env = !env\n",
    "# print(env)\n",
    "\n",
    "# gpu_info = '\\n'.join(gpu_info)\n",
    "# if gpu_info.find('failed') >= 0:\n",
    "#   print('Not connected to a GPU')\n",
    "# else:\n",
    "#   print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearBottleNeck(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride, t=6, class_num=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * t, 1),\n",
    "            nn.BatchNorm2d(in_channels * t),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels * t, in_channels * t, 3, stride=stride, padding=1, groups=in_channels * t),\n",
    "            nn.BatchNorm2d(in_channels * t),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels * t, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.stride = stride\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = self.residual(x)\n",
    "\n",
    "        if self.stride == 1 and self.in_channels == self.out_channels:\n",
    "            residual += x\n",
    "\n",
    "        return residual\n",
    "\n",
    "# MobileNetV2（比lenet更复杂的CNN网络）网络中的线性瓶颈结构，原文中用于CIFAR-100任务\n",
    "class MobileNetV2(nn.Module):\n",
    "\n",
    "    def __init__(self, class_num=20):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pre = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.stage1 = LinearBottleNeck(32, 16, 1, 1)\n",
    "        self.stage2 = self._make_stage(2, 16, 24, 2, 6)\n",
    "        self.stage3 = self._make_stage(3, 24, 32, 2, 6)\n",
    "        self.stage4 = self._make_stage(4, 32, 64, 2, 6)\n",
    "        self.stage5 = self._make_stage(3, 64, 96, 1, 6)\n",
    "        self.stage6 = self._make_stage(3, 96, 160, 1, 6)\n",
    "        self.stage7 = LinearBottleNeck(160, 320, 1, 6)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(320, 1280, 1),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(1280, class_num, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "        x = self.stage6(x)\n",
    "        x = self.stage7(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _make_stage(self, repeat, in_channels, out_channels, stride, t):\n",
    "\n",
    "        layers = []\n",
    "        layers.append(LinearBottleNeck(in_channels, out_channels, stride, t))\n",
    "\n",
    "        while repeat - 1:\n",
    "            layers.append(LinearBottleNeck(out_channels, out_channels, 1, t))\n",
    "            repeat -= 1\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "def mobilenetv2():\n",
    "    return MobileNetV2()\n",
    "\n",
    "\n",
    "# FedMut中采用的cnn模型\n",
    "class CNNCifar(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNCifar, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x, start_layer_idx=0, logit=False):\n",
    "        if start_layer_idx < 0:  #\n",
    "            return self.mapping(x, start_layer_idx=start_layer_idx, logit=logit)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        result = {'activation' : x}\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        result['hint'] = x\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        result['representation'] = x\n",
    "        x = self.fc3(x)\n",
    "        result['output'] = x\n",
    "        return result\n",
    "\n",
    "    def mapping(self, z_input, start_layer_idx=-1, logit=True):\n",
    "        z = z_input\n",
    "        z = self.fc3(z)\n",
    "\n",
    "        result = {'output': z}\n",
    "        if logit:\n",
    "            result['logit'] = z\n",
    "        return result\n",
    "    \n",
    "def cnncifar():\n",
    "    return CNNCifar()\n",
    "\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetCifar10(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNetCifar10, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        result = {}\n",
    "        x = self.layer1(x)\n",
    "        result['activation1'] = x\n",
    "        x = self.layer2(x)\n",
    "        result['activation2'] = x\n",
    "        x = self.layer3(x)\n",
    "        result['activation3'] = x\n",
    "        x = self.layer4(x)\n",
    "        result['activation4'] = x\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        result['representation'] = x\n",
    "        x = self.fc(x)\n",
    "        result['output'] = x\n",
    "\n",
    "        return result\n",
    "\n",
    "    def mapping(self, z_input, start_layer_idx=-1, logit=True):\n",
    "        z = z_input\n",
    "        z = self.fc(z)\n",
    "\n",
    "        result = {'output': z}\n",
    "        if logit:\n",
    "            result['logit'] = z\n",
    "        return result\n",
    "\n",
    "    def forward(self, x, start_layer_idx=0, logit=False):\n",
    "        if start_layer_idx < 0:  #\n",
    "            return self.mapping(x, start_layer_idx=start_layer_idx, logit=logit)\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def ResNet8(**kwargs):\n",
    "    return ResNetCifar10(BasicBlock, [1, 1, 1], **kwargs)\n",
    "\n",
    "def ResNet18_cifar10(**kwargs):\n",
    "    r\"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return ResNetCifar10(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def ResNet50_cifar10(**kwargs):\n",
    "    r\"\"\"ResNet-50 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return ResNetCifar10(Bottleneck, [3, 4, 6, 3], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新的测试：针对整个测试数据集的测试\n",
    "def test_inference(net_glob, dataset_test):\n",
    "    # testing\n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test)\n",
    "\n",
    "    # print(\"Testing accuracy: {:.2f}\".format(acc_test))\n",
    "\n",
    "    return acc_test.item()\n",
    "\n",
    "def test_img(net_g, datatest):\n",
    "    net_g.eval()\n",
    "    # testing\n",
    "    # test loss代表在测试集上的平均损失（对测试数据的预测输出与真实标签的差距）\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    data_loader = DataLoader(datatest, batch_size=test_bc_size)\n",
    "    l = len(data_loader)\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, target) in enumerate(data_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            log_probs = net_g(data)['output']\n",
    "            # sum up batch loss\n",
    "            test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n",
    "            # get the index of the max log-probability\n",
    "            y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "            correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.00 * correct / len(data_loader.dataset)\n",
    "    if verbose:\n",
    "        print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(data_loader.dataset), accuracy))\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将CIFAR-100的100个类别转为20个类别（粒度更粗，降低任务复杂度）\n",
    "def sparse2coarse(targets):\n",
    "    \"\"\"Convert Pytorch CIFAR100 sparse targets to coarse targets.\n",
    "\n",
    "    Usage:\n",
    "        trainset = torchvision.datasets.CIFAR100(path)\n",
    "        trainset.targets = sparse2coarse(trainset.targets)\n",
    "    \"\"\"\n",
    "    coarse_labels = np.array([ 4,  1, 14,  8,  0,  6,  7,  7, 18,  3,\n",
    "                               3, 14,  9, 18,  7, 11,  3,  9,  7, 11,\n",
    "                               6, 11,  5, 10,  7,  6, 13, 15,  3, 15,\n",
    "                               0, 11,  1, 10, 12, 14, 16,  9, 11,  5,\n",
    "                               5, 19,  8,  8, 15, 13, 14, 17, 18, 10,\n",
    "                               16, 4, 17,  4,  2,  0, 17,  4, 18, 17,\n",
    "                               10, 3,  2, 12, 12, 16, 12,  1,  9, 19,\n",
    "                               2, 10,  0,  1, 16, 12,  9, 13, 15, 13,\n",
    "                              16, 19,  2,  4,  6, 19,  5,  5,  8, 19,\n",
    "                              18,  1,  2, 15,  6,  0, 17,  8, 14, 13])\n",
    "    return coarse_labels[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 共有6w个图像，其中5w训练，1w测试\n",
    "def CIFAR100():\n",
    "    '''Return Cifar100\n",
    "    '''\n",
    "    \n",
    "    # 参考FedMut进行正则化变化\n",
    "    trans_cifar100 = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    train_dataset = torchvision.datasets.CIFAR100(root='../data/CIFAR-100',\n",
    "                                            train=True,\n",
    "                                            transform=trans_cifar100,\n",
    "                                            download=True)\n",
    "    test_dataset = torchvision.datasets.CIFAR100(root='../data/CIFAR-100',\n",
    "                                            train=False,\n",
    "                                            transform=trans_cifar100,\n",
    "                                            download=True)\n",
    "    \n",
    "    # 将图片转换成 numpy 数组格式，并对标签做了 coarse 处理\n",
    "    total_img,total_label = [],[]\n",
    "    for imgs,labels in train_dataset:\n",
    "        total_img.append(imgs.numpy())\n",
    "        total_label.append(labels)\n",
    "    total_img = np.array(total_img)\n",
    "    total_label = np.array(sparse2coarse(total_label))\n",
    "\n",
    "    cifar = [total_img, total_label]\n",
    "    \n",
    "    return cifar, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于 Dirichlet 分布 来模拟non-IID。返回一个形状为 (client_num, class_num) 的概率矩阵，每一行代表一个客户端对各类别的概率分布。\n",
    "def get_prob(non_iid, client_num, class_num = 20):\n",
    "    # Modify：我之后加上的\n",
    "    if data_random_fix:\n",
    "        np.random.seed(seed_num)  # 固定种子，确保数据抽样一致\n",
    "    \n",
    "    return np.random.dirichlet(np.repeat(non_iid, class_num), client_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全部用于构建训练集\n",
    "def create_data_all_train(prob, size_per_client, dataset, N=20):\n",
    "    total_each_class = size_per_client * np.sum(prob, 0)\n",
    "    data, label = dataset\n",
    "    \n",
    "\n",
    "    if data_random_fix:\n",
    "        np.random.seed(seed_num)  # 固定种子，确保数据抽样一致\n",
    "        random.seed(seed_num)\n",
    "\n",
    "    # 为每个类别随机采样数据\n",
    "    all_class_set = []\n",
    "    for i in range(N):\n",
    "        size = total_each_class[i]\n",
    "        sub_data = data[label == i]\n",
    "        sub_label = label[label == i]\n",
    "\n",
    "        num_samples = int(size)\n",
    "        if num_samples > len(sub_data):\n",
    "            print(f\"类别 {i} 的数据样本不足，采样数从 {num_samples} 调整为 {len(sub_data)}\")\n",
    "            num_samples = len(sub_data)\n",
    "        rand_indx = np.random.choice(len(sub_data), size=num_samples, replace=False).astype(int)\n",
    "        \n",
    "        sub2_data, sub2_label = sub_data[rand_indx], sub_label[rand_indx]\n",
    "        all_class_set.append((sub2_data, sub2_label))\n",
    "\n",
    "    index = [0] * N\n",
    "    clients = []\n",
    "\n",
    "    for m in range(prob.shape[0]):  # 遍历客户端\n",
    "        labels, images = [], []  # 训练数据\n",
    "\n",
    "        for n in range(N):\n",
    "            # 100%用于训练\n",
    "            start, end = index[n], index[n] + int(prob[m][n] * size_per_client)\n",
    "            image, label = all_class_set[n][0][start:end], all_class_set[n][1][start:end]\n",
    "\n",
    "            # 记录当前类别的数据分配进度\n",
    "            index[n] += int(prob[m][n] * size_per_client)\n",
    "\n",
    "            labels.extend(label)\n",
    "            images.extend(image)\n",
    "\n",
    "        clients.append((np.array(images), np.array(labels)))\n",
    "\n",
    "    return clients\n",
    "\n",
    "# 80%构建训练集，20%构建测试集\n",
    "def create_data(prob, size_per_client, dataset, N=20):\n",
    "    total_each_class = size_per_client * np.sum(prob, 0)\n",
    "    data, label = dataset\n",
    "\n",
    "    # Modify：我之后加上的\n",
    "    if data_random_fix:\n",
    "        np.random.seed(seed_num)  # 固定种子，确保数据抽样一致\n",
    "        random.seed(seed_num)\n",
    "\n",
    "    # 为每个类别随机采样数据\n",
    "    all_class_set = []\n",
    "    for i in range(N):\n",
    "        size = total_each_class[i]\n",
    "        sub_data = data[label == i]\n",
    "        sub_label = label[label == i]\n",
    "\n",
    "        rand_indx = np.random.choice(len(sub_data), size=int(size), replace=False).astype(int)\n",
    "        sub2_data, sub2_label = sub_data[rand_indx], sub_label[rand_indx]\n",
    "        all_class_set.append((sub2_data, sub2_label))\n",
    "\n",
    "    index = [0] * N\n",
    "    clients, test = [], []\n",
    "\n",
    "    for m in range(prob.shape[0]):  # 遍历客户端\n",
    "        labels, images = [], []  # 训练数据\n",
    "        tlabels, timages = [], [] # 测试数据\n",
    "\n",
    "        for n in range(N):\n",
    "            # 80%用于训练，20%用于测试\n",
    "            # 这里的int向下取整，会导致实际的数据量比计算略小\n",
    "            start, end = index[n], index[n] + int(prob[m][n] * size_per_client * 0.8)\n",
    "            test_start, test_end = end, index[n] + int(prob[m][n] * size_per_client)\n",
    "\n",
    "            image, label = all_class_set[n][0][start:end], all_class_set[n][1][start:end]\n",
    "            test_image, test_label = all_class_set[n][0][test_start:test_end], all_class_set[n][1][test_start:test_end]\n",
    "\n",
    "            # 记录当前类别的数据分配进度\n",
    "            index[n] += int(prob[m][n] * size_per_client)\n",
    "\n",
    "            labels.extend(label)\n",
    "            images.extend(image)\n",
    "\n",
    "            tlabels.extend(test_label)\n",
    "            timages.extend(test_image)\n",
    "\n",
    "        clients.append((np.array(images), np.array(labels)))\n",
    "        test.append((np.array(timages), np.array(tlabels)))\n",
    "\n",
    "    return clients, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 合并所有客户端的测试数据 （上面讲测试数据分成了不同的客户端）\n",
    "# 但并没有使用，用途不明\n",
    "def comb_client_test_func(client_test_data):\n",
    "    comb_client_test_image = []\n",
    "    comb_client_test_label = []\n",
    "    for i in range(client_num):\n",
    "        comb_client_test_image.extend(list(client_test_data[i][0]))\n",
    "        comb_client_test_label.extend(list(client_test_data[i][1]))\n",
    "    \n",
    "    # 将测试图片和标签合并为 numpy 数组\n",
    "    comb_client_test_image = np.array(comb_client_test_image)\n",
    "    comb_client_test_label = np.array(comb_client_test_label)\n",
    "    \n",
    "    label_count = Counter(comb_client_test_label)\n",
    "    print(\"测试集类别分布：\")\n",
    "    for label, count in sorted(label_count.items()):\n",
    "        print(f\"类别 {label}: {count} 个样本\")\n",
    "    \n",
    "    return [comb_client_test_image, comb_client_test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 样服务器子集的函数\n",
    "def select_server_subset(cifar, percentage=0.1, mode='iid', dirichlet_alpha=1.0):\n",
    "    \"\"\"\n",
    "    从 cifar 数据集中挑选服务器数据子集（cifar 已经是 [N, C, H, W] 格式）。\n",
    "    \n",
    "    参数：\n",
    "      - cifar: 一个列表，格式为 [images, labels]，images 形状为 [N, C, H, W]\n",
    "      - percentage: 挑选比例，例如 0.1 表示取 10% 的数据\n",
    "      - mode: 'iid' 表示各类别均匀采样；'non-iid' 表示使用 Dirichlet 分布采样\n",
    "      - dirichlet_alpha: 当 mode 为 'non-iid' 时的 Dirichlet 分布参数\n",
    "    返回：\n",
    "      - subset_images: 选出的图片数组（numpy.array）\n",
    "      - subset_labels: 选出的标签数组（numpy.array）\n",
    "    \"\"\"\n",
    "    images, labels = cifar\n",
    "    unique_classes = np.unique(labels)\n",
    "    total_num = len(labels)\n",
    "    server_total = int(total_num * percentage)\n",
    "    \n",
    "    selected_indices = []\n",
    "    \n",
    "    if mode == 'iid':\n",
    "        for cls in unique_classes:\n",
    "            cls_indices = np.where(labels == cls)[0]\n",
    "            num_cls = int(len(cls_indices) * percentage)\n",
    "            if num_cls > len(cls_indices):\n",
    "                num_cls = len(cls_indices)\n",
    "            sampled = np.random.choice(cls_indices, size=num_cls, replace=False)\n",
    "            selected_indices.extend(sampled)\n",
    "    elif mode == 'non-iid':\n",
    "        num_classes = len(unique_classes)\n",
    "        prob = np.random.dirichlet(np.repeat(dirichlet_alpha, num_classes))\n",
    "        cls_sample_numbers = {}\n",
    "        total_assigned = 0\n",
    "        for i, cls in enumerate(unique_classes):\n",
    "            n_cls = int(prob[i] * server_total)\n",
    "            cls_sample_numbers[cls] = n_cls\n",
    "            total_assigned += n_cls\n",
    "        diff = server_total - total_assigned\n",
    "        if diff > 0:\n",
    "            for cls in np.random.choice(unique_classes, size=diff, replace=True):\n",
    "                cls_sample_numbers[cls] += 1\n",
    "        \n",
    "        for cls in unique_classes:\n",
    "            cls_indices = np.where(labels == cls)[0]\n",
    "            n_sample = cls_sample_numbers[cls]\n",
    "            if n_sample > len(cls_indices):\n",
    "                n_sample = len(cls_indices)\n",
    "            sampled = np.random.choice(cls_indices, size=n_sample, replace=False)\n",
    "            selected_indices.extend(sampled)\n",
    "    else:\n",
    "        raise ValueError(\"mode 参数必须为 'iid' 或 'non-iid'\")\n",
    "    \n",
    "    selected_indices = np.array(selected_indices)\n",
    "    np.random.shuffle(selected_indices)\n",
    "    \n",
    "    subset_images = images[selected_indices]\n",
    "    subset_labels = labels[selected_indices]\n",
    "    \n",
    "    return subset_images, subset_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本地训练并更新权重，返回更新后的模型权重、平均训练损失以及第一个迭代的梯度信息\n",
    "def update_weights(model_weight, dataset, learning_rate, local_epoch):\n",
    "    if origin_model == 'resnet':\n",
    "        model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        model = CharLSTM().to(device)\n",
    "    \n",
    "    model.load_state_dict(model_weight)\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if origin_model == 'resnet':\n",
    "        Tensor_set = TensorDataset(torch.Tensor(dataset[0]).to(device), torch.Tensor(dataset[1]).to(device))\n",
    "    elif origin_model == 'lstm':\n",
    "        Tensor_set = TensorDataset(torch.LongTensor(dataset[0]).to(device), torch.Tensor(dataset[1]).to(device))\n",
    "    \n",
    "    data_loader = DataLoader(Tensor_set, batch_size=bc_size, shuffle=True)\n",
    "\n",
    "    first_iter_gradient = None  # 初始化变量来保存第一个iter的梯度\n",
    "\n",
    "    for iter in range(local_epoch):\n",
    "        batch_loss = []\n",
    "        for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs['output'], labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss.append(loss.item()/images.shape[0])\n",
    "\n",
    "            # 保存第一个iter的梯度\n",
    "            if iter == 0 and batch_idx == 0:\n",
    "                first_iter_gradient = {}\n",
    "                for name, param in model.named_parameters():\n",
    "                    first_iter_gradient[name] = param.grad.clone()\n",
    "                # 保存 BatchNorm 层的 running mean 和 running variance\n",
    "                for name, module in model.named_modules():\n",
    "                    if isinstance(module, nn.BatchNorm2d):\n",
    "                        first_iter_gradient[name + '.running_mean'] = module.running_mean.clone()\n",
    "                        first_iter_gradient[name + '.running_var'] = module.running_var.clone()\n",
    "\n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "\n",
    "    return model.state_dict(), sum(epoch_loss) / len(epoch_loss), first_iter_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型权重的差异，并根据学习率 lr 对权重差异进行缩放\n",
    "def weight_differences(n_w, p_w, lr):\n",
    "    w_diff = copy.deepcopy(n_w)\n",
    "    for key in w_diff.keys():\n",
    "        if 'num_batches_tracked' in key:\n",
    "            continue\n",
    "        w_diff[key] = (p_w[key] - n_w[key]) * lr\n",
    "    return w_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 也是本地训练，不过引入了Fed-C的权重修正机制\n",
    "def update_weights_correction(model_weight, dataset, learning_rate, local_epoch, c_i, c_s):\n",
    "    if origin_model == 'resnet':\n",
    "        model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        model = CharLSTM().to(device)\n",
    "        \n",
    "    model.load_state_dict(model_weight)\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    Tensor_set = TensorDataset(torch.Tensor(dataset[0]).to(device), torch.Tensor(dataset[1]).to(device))\n",
    "    data_loader = DataLoader(Tensor_set, batch_size=bc_size, shuffle=True)\n",
    "\n",
    "    for iter in range(local_epoch):\n",
    "        batch_loss = []\n",
    "        for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs['output'], labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss.append(loss.sum().item()/images.shape[0])\n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "        corrected_graident = weight_differences(c_i, c_s, learning_rate)\n",
    "        orginal_model_weight = model.state_dict()\n",
    "        corrected_model_weight = weight_differences(corrected_graident, orginal_model_weight, 1)  # 这里缩放权重为1\n",
    "        model.load_state_dict(corrected_model_weight)\n",
    "\n",
    "    return model.state_dict(),  sum(epoch_loss) / len(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(w):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        if 'num_batches_tracked' in key:\n",
    "            continue\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], len(w))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline: server-only\n",
    "def server_only(initial_w, global_round, gamma, E):\n",
    "    if origin_model == 'resnet':\n",
    "        test_model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        test_model = CharLSTM().to(device)\n",
    "        \n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        # Server side local training\n",
    "\n",
    "                \n",
    "        update_server_w, round_loss, _ = update_weights(train_w, server_data, gamma, E)\n",
    "        train_w = update_server_w\n",
    "        test_model.load_state_dict(train_w)\n",
    "        train_loss.append(round_loss)\n",
    "        \n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "    \n",
    "        # Test Accuracy\n",
    "        # test_a = 0\n",
    "        # for i in client_test_data:\n",
    "        #     ac = test_inference(test_model,i)[0]\n",
    "        #     test_a = test_a + ac\n",
    "        # test_a = test_a/len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "        # print(test_a)\n",
    "    return test_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fedavg(initial_w, global_round, eta, K, M):\n",
    "    if origin_model == 'resnet':\n",
    "        test_model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        test_model = CharLSTM().to(device)\n",
    "        \n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    for round in tqdm(range(global_round)):\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "        for i in sampled_client:\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        train_w = average_weights(local_weights)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "        \n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "            \n",
    "        # test_a = 0\n",
    "        # for i in client_test_data:\n",
    "        #     ac = test_inference(test_model,i)[0]\n",
    "        #     test_a = test_a + ac\n",
    "        # test_a = test_a/len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "#         print(test_a)\n",
    "    return test_acc, train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybridFL(initial_w, global_round, eta, K, M):\n",
    "    \"\"\"\n",
    "    HybridFL算法：FedAvg改进，服务器也作为一个普通客户端参与训练。\n",
    "    \n",
    "    参数:\n",
    "    - initial_w: 初始模型权重\n",
    "    - global_round: 全局训练轮数\n",
    "    - eta: 学习率\n",
    "    - K: 本地训练轮数\n",
    "    - M: 每轮采样的客户端数量\n",
    "    \"\"\"\n",
    "    \n",
    "    if origin_model == 'resnet':\n",
    "        test_model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        test_model = CharLSTM().to(device)\n",
    "        \n",
    "    train_w = copy.deepcopy(initial_w)     # 当前全局权重\n",
    "    test_acc = []                          # 保存每轮测试精度\n",
    "    train_loss = []                        # 保存每轮训练损失\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        local_weights, local_loss = [], []  # 存储每个客户端/服务器的权重和损失\n",
    "\n",
    "        # 随机采样 M 个客户端\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "\n",
    "        # 客户端本地训练\n",
    "        for i in sampled_client:\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        # 服务器参与训练\n",
    "        update_server_w, server_round_loss, _ = update_weights(train_w, server_data, eta, K)\n",
    "        local_weights.append(update_server_w)   # 将服务器权重加入列表\n",
    "        local_loss.append(server_round_loss)    # 将服务器损失加入列表\n",
    "\n",
    "        # 权重聚合\n",
    "        train_w = average_weights(local_weights)\n",
    "\n",
    "        # 评估模型性能\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss) / len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "    \n",
    "        \n",
    "        # test_a = 0\n",
    "        # for i in client_test_data:  # 遍历所有客户端测试数据\n",
    "        #     ac = test_inference(test_model, i)[0]\n",
    "        #     test_a += ac\n",
    "        # test_a = test_a / len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "        \n",
    "        # # 打印每轮的结果\n",
    "        # print(f\"Round {round + 1}: Test Accuracy = {test_a:.4f}, Train Loss = {loss_avg:.4f}\")\n",
    "    \n",
    "    return test_acc, train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CLG_SGD(initial_w, global_round, eta, gamma, K, E, M):\n",
    "    if origin_model == 'resnet':\n",
    "        test_model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        test_model = CharLSTM().to(device)\n",
    "        \n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # 学习率衰减，这里默认注释掉了\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # 从总共client_num客户端中选择M个训练\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "        for i in sampled_client:\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "        train_w = average_weights(local_weights)\n",
    "        # Server side local training\n",
    "    \n",
    "        \n",
    "        update_server_w, round_loss, _ = update_weights(train_w, server_data, gamma, E)\n",
    "        train_w = update_server_w\n",
    "        local_loss.append(round_loss)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)   # 计算所有客户端和服务器一起的平均损失\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "    \n",
    "        # test_a = 0\n",
    "        # # 遍历客户端测试数据，计算平均准确率\n",
    "        # for i in client_test_data:\n",
    "        #     ac = test_inference(test_model,i)[0]\n",
    "        #     test_a = test_a + ac\n",
    "        # test_a = test_a/len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "#         print(test_a)\n",
    "    return test_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FedMut(net_glob, global_round, eta, K, M):\n",
    "    \n",
    "    net_glob.train()\n",
    "    \n",
    "    if origin_model == 'resnet':\n",
    "        test_model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        test_model = CharLSTM().to(device)\n",
    "        \n",
    "    train_w = copy.deepcopy(net_glob.state_dict())\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    w_locals = []\n",
    "    for i in range(M):\n",
    "        w_locals.append(copy.deepcopy(net_glob.state_dict()))\n",
    "        \n",
    "    max_rank = 0\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        w_old = copy.deepcopy(net_glob.state_dict())\n",
    "        \n",
    "        # 学习率衰减，这里默认注释掉了\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # 从总共client_num客户端中选择M个训练\n",
    "        idxs_users = np.random.choice(range(client_num), M, replace=False)\n",
    "        for i, idx in enumerate(idxs_users):\n",
    "            net_glob.load_state_dict(w_locals[i])\n",
    "            \n",
    "            update_client_w, client_round_loss, _ = update_weights(copy.deepcopy(net_glob.state_dict()), client_data[idx], eta, K)\n",
    "            w_locals[i] = copy.deepcopy(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        # Global Model Generation\n",
    "        w_agg = Aggregation(w_locals, None)  \n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_agg)\n",
    "        \n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(w_agg)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)   # 计算所有客户端的平均损失\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "\n",
    "        # 按照server训练的方向，进行mutation\n",
    "        w_delta = FedSub(w_agg, w_old, 1.0)\n",
    "        # 计算模型更新w_delta的L2范数（平方和），衡量模型更新程度的大小\n",
    "        rank = delta_rank(w_delta)\n",
    "        # print(rank)\n",
    "        if rank > max_rank:\n",
    "            max_rank = rank\n",
    "        alpha = radius  # 论文中的alpha，衡量Mutation的幅度\n",
    "        # alpha = min(max(args.radius, max_rank/rank),(10.0-args.radius) * (1 - iter/args.epochs) + args.radius)\n",
    "        w_locals = mutation_spread(\n",
    "            round, w_agg, M, w_delta, alpha\n",
    "        )\n",
    "\n",
    "    return test_acc, train_loss   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mutation方向设置为server更新的方向\n",
    "def CLG_Mut(net_glob, global_round, eta, gamma, K, E, M):\n",
    "    \n",
    "    net_glob.train()\n",
    "    \n",
    "    if origin_model == 'resnet':\n",
    "        test_model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        test_model = CharLSTM().to(device)\n",
    "        \n",
    "    train_w = copy.deepcopy(net_glob.state_dict())\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    w_locals = []\n",
    "    for i in range(M):\n",
    "        w_locals.append(copy.deepcopy(net_glob.state_dict()))\n",
    "    \n",
    "    delta_list = []\n",
    "    max_rank = 0\n",
    "    w_old = copy.deepcopy(net_glob.state_dict())\n",
    "    w_old_s1 = copy.deepcopy(net_glob.state_dict())\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # 学习率衰减，这里默认注释掉了\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # 从总共client_num客户端中选择M个训练\n",
    "        idxs_users = np.random.choice(range(client_num), M, replace=False)\n",
    "        for i, idx in enumerate(idxs_users):\n",
    "            net_glob.load_state_dict(w_locals[i])\n",
    "            \n",
    "            update_client_w, client_round_loss, _ = update_weights(copy.deepcopy(net_glob.state_dict()), client_data[idx], eta, K)\n",
    "            w_locals[i] = copy.deepcopy(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        # Global Model Generation\n",
    "        w_agg = Aggregation(w_locals, None)  \n",
    "        \n",
    "        # Server side local training\n",
    "        update_server_w, round_loss, _ = update_weights(w_agg, server_data, gamma, E)\n",
    "        local_loss.append(round_loss)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(update_server_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)   # 计算所有客户端和服务器一起的平均损失\n",
    "\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "\n",
    "        # 按照server训练的方向，进行mutation\n",
    "        w_delta = FedSub(update_server_w, w_agg, 1.0)\n",
    "        # 计算模型更新w_delta的L2范数（平方和），衡量模型更新程度的大小\n",
    "        rank = delta_rank(w_delta)\n",
    "        # print(rank)\n",
    "        if rank > max_rank:\n",
    "            max_rank = rank\n",
    "        alpha = radius  # 论文中的alpha，衡量Mutation的幅度\n",
    "        # alpha = min(max(args.radius, max_rank/rank),(10.0-args.radius) * (1 - iter/args.epochs) + args.radius)\n",
    "        w_locals = mutation_spread(\n",
    "            round, update_server_w, M, w_delta, alpha\n",
    "        )\n",
    "\n",
    "    return test_acc, train_loss\n",
    "\n",
    "\n",
    "# 将mutation的方向设置为新方向（server更新之后）减去上一轮全局方向（其余不变）\n",
    "def CLG_Mut_2(net_glob, global_round, eta, gamma, K, E, M):\n",
    "    \n",
    "    net_glob.train()\n",
    "    \n",
    "    if origin_model == 'resnet':\n",
    "        test_model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        test_model = CharLSTM().to(device)\n",
    "        \n",
    "    train_w = copy.deepcopy(net_glob.state_dict())\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    w_locals = []\n",
    "    for i in range(M):\n",
    "        w_locals.append(copy.deepcopy(net_glob.state_dict()))\n",
    "    \n",
    "    delta_list = []\n",
    "    max_rank = 0\n",
    "    w_old = copy.deepcopy(net_glob.state_dict())\n",
    "    w_old_s1 = copy.deepcopy(net_glob.state_dict())\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        w_old = copy.deepcopy(net_glob.state_dict())\n",
    "        \n",
    "        # 学习率衰减，这里默认注释掉了\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # 从总共client_num客户端中选择M个训练\n",
    "        idxs_users = np.random.choice(range(client_num), M, replace=False)\n",
    "        for i, idx in enumerate(idxs_users):\n",
    "            net_glob.load_state_dict(w_locals[i])\n",
    "            \n",
    "            update_client_w, client_round_loss, _ = update_weights(copy.deepcopy(net_glob.state_dict()), client_data[idx], eta, K)\n",
    "            w_locals[i] = copy.deepcopy(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        # Global Model Generation\n",
    "        w_agg = Aggregation(w_locals, None)  \n",
    "        \n",
    "        # Server side local training\n",
    "        update_server_w, round_loss, _ = update_weights(w_agg, server_data, gamma, E)\n",
    "        local_loss.append(round_loss)\n",
    "        \n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(update_server_w)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(update_server_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)   # 计算所有客户端和服务器一起的平均损\n",
    "\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "\n",
    "        # 按照server训练的方向，进行mutation\n",
    "        w_delta = FedSub(update_server_w, w_old, 1.0)\n",
    "        # 计算模型更新w_delta的L2范数（平方和），衡量模型更新程度的大小\n",
    "        rank = delta_rank(w_delta)\n",
    "        # print(rank)\n",
    "        if rank > max_rank:\n",
    "            max_rank = rank\n",
    "        alpha = radius  # 论文中的alpha，衡量Mutation的幅度\n",
    "        # alpha = min(max(args.radius, max_rank/rank),(10.0-args.radius) * (1 - iter/args.epochs) + args.radius)\n",
    "        w_locals = mutation_spread(\n",
    "            round, update_server_w, M, w_delta, alpha\n",
    "        )\n",
    "\n",
    "    return test_acc, train_loss\n",
    "\n",
    "\n",
    "# 将mutation的方向设置为client训练更新的方向\n",
    "def CLG_Mut_3(net_glob, global_round, eta, gamma, K, E, M):\n",
    "    \n",
    "    net_glob.train()\n",
    "    if origin_model == 'resnet':\n",
    "        test_model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        test_model = CharLSTM().to(device)\n",
    "        \n",
    "    train_w = copy.deepcopy(net_glob.state_dict())\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    w_locals = []\n",
    "    for i in range(M):\n",
    "        w_locals.append(copy.deepcopy(net_glob.state_dict()))\n",
    "    \n",
    "    delta_list = []\n",
    "    max_rank = 0\n",
    "    w_old = copy.deepcopy(net_glob.state_dict())\n",
    "    w_old_s1 = copy.deepcopy(net_glob.state_dict())\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        w_old = copy.deepcopy(net_glob.state_dict())\n",
    "        \n",
    "        # 学习率衰减，这里默认注释掉了\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # 从总共client_num客户端中选择M个训练\n",
    "        idxs_users = np.random.choice(range(client_num), M, replace=False)\n",
    "        for i, idx in enumerate(idxs_users):\n",
    "            net_glob.load_state_dict(w_locals[i])\n",
    "            \n",
    "            update_client_w, client_round_loss, _ = update_weights(copy.deepcopy(net_glob.state_dict()), client_data[idx], eta, K)\n",
    "            w_locals[i] = copy.deepcopy(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        # Global Model Generation\n",
    "        w_agg = Aggregation(w_locals, None)  \n",
    "        \n",
    "        # Server side local training\n",
    "        update_server_w, round_loss, _ = update_weights(w_agg, server_data, gamma, E)\n",
    "        local_loss.append(round_loss)\n",
    "        \n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(update_server_w)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(update_server_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)   # 计算所有客户端和服务器一起的平均损\n",
    "\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "\n",
    "        # 按照client训练的方向，进行mutation\n",
    "        w_delta = FedSub(w_agg, w_old, 1.0)\n",
    "        # 计算模型更新w_delta的L2范数（平方和），衡量模型更新程度的大小\n",
    "        rank = delta_rank(w_delta)\n",
    "        # print(rank)\n",
    "        if rank > max_rank:\n",
    "            max_rank = rank\n",
    "        alpha = radius  # 论文中的alpha，衡量Mutation的幅度\n",
    "        # alpha = min(max(args.radius, max_rank/rank),(10.0-args.radius) * (1 - iter/args.epochs) + args.radius)\n",
    "        w_locals = mutation_spread(\n",
    "            round, update_server_w, M, w_delta, alpha\n",
    "        )\n",
    "\n",
    "    return test_acc, train_loss\n",
    "\n",
    "\n",
    "def mutation_spread(iter, w_glob, m, w_delta, alpha):\n",
    "\n",
    "    w_locals_new = []\n",
    "    ctrl_cmd_list = []\n",
    "    ctrl_rate = mut_acc_rate * (\n",
    "        1.0 - min(iter * 1.0 / mut_bound, 1.0)\n",
    "    )  # 论文中的βt，随着iter逐渐从β0减小到0\n",
    "\n",
    "    # k代表模型中的参数数量，对每个参数按照client数量分配v（论文中是按照每一层分配）\n",
    "    for k in w_glob.keys():\n",
    "        ctrl_list = []\n",
    "        for i in range(0, int(m / 2)):\n",
    "            ctrl = random.random()  # 随机数，范围：[0,1)\n",
    "            # 这里分ctrl感觉没什么必要，shuffle后都会随机掉\n",
    "            if ctrl > 0.5:\n",
    "                ctrl_list.append(1.0)\n",
    "                ctrl_list.append(1.0 * (-1.0 + ctrl_rate))\n",
    "            else:\n",
    "                ctrl_list.append(1.0 * (-1.0 + ctrl_rate))\n",
    "                ctrl_list.append(1.0)\n",
    "        random.shuffle(ctrl_list)  # 打乱列表\n",
    "        ctrl_cmd_list.append(ctrl_list)\n",
    "    cnt = 0\n",
    "    for j in range(m):\n",
    "        w_sub = copy.deepcopy(w_glob)\n",
    "        if not (cnt == m - 1 and m % 2 == 1):\n",
    "            ind = 0\n",
    "            for k in w_sub.keys():\n",
    "                w_sub[k] = w_sub[k] + w_delta[k] * ctrl_cmd_list[ind][j] * alpha\n",
    "                ind += 1\n",
    "        cnt += 1\n",
    "        w_locals_new.append(w_sub)\n",
    "\n",
    "    return w_locals_new\n",
    "\n",
    "\n",
    "\n",
    "# 加权平均聚合，lens代表了权重，如果没有定义就是普通平均（FedMut就每定义）\n",
    "def Aggregation(w, lens):\n",
    "    w_avg = None\n",
    "    if lens == None:\n",
    "        total_count = len(w)\n",
    "        lens = []\n",
    "        for i in range(len(w)):\n",
    "            lens.append(1.0)\n",
    "    else:\n",
    "        total_count = sum(lens)\n",
    "\n",
    "    for i in range(0, len(w)):\n",
    "        if i == 0:\n",
    "            w_avg = copy.deepcopy(w[0])\n",
    "            for k in w_avg.keys():\n",
    "                w_avg[k] = w[i][k] * lens[i]\n",
    "        else:\n",
    "            for k in w_avg.keys():\n",
    "                w_avg[k] += w[i][k] * lens[i]\n",
    "\n",
    "    for k in w_avg.keys():\n",
    "        w_avg[k] = torch.div(w_avg[k], total_count)\n",
    "\n",
    "    return w_avg\n",
    "\n",
    "\n",
    "\n",
    "def FedSub(w, w_old, weight):\n",
    "    w_sub = copy.deepcopy(w)\n",
    "    for k in w_sub.keys():\n",
    "        w_sub[k] = (w[k] - w_old[k]) * weight\n",
    "\n",
    "    return w_sub\n",
    "\n",
    "def delta_rank(delta_dict):\n",
    "    cnt = 0\n",
    "    dict_a = torch.Tensor(0)\n",
    "    s = 0\n",
    "    for p in delta_dict.keys():\n",
    "        a = delta_dict[p]\n",
    "        a = a.view(-1)\n",
    "        if cnt == 0:\n",
    "            dict_a = a\n",
    "        else:\n",
    "            dict_a = torch.cat((dict_a, a), dim=0)\n",
    "\n",
    "        cnt += 1\n",
    "        # print(sim)\n",
    "    s = torch.norm(dict_a, dim=0)\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机性（这里暂时不采用，先固定为false）\n",
    "data_random_fix = False  # 是否固定数据采样的随机性\n",
    "seed_num = 42\n",
    "# 新采用的全局随机性机制\n",
    "random_fix = True\n",
    "seed = 2\n",
    "\n",
    "GPU = 1  # 决定使用哪个gpu 0或1\n",
    "verbose = False  # 调试模式，输出一些中间信息\n",
    "\n",
    "client_num = 100\n",
    "non_iid = 0.5  # Dirichlet 分布参数，数值越小数据越不均匀可根据需要调整\n",
    "size_per_client = 400  # 每个客户端的数据量（训练）\n",
    "\n",
    "server_iid = True # True代表server数据iid分布，否则为Non-iid分布\n",
    "server_percentage = 0.1  # 服务器端用于微调的数据比例\n",
    "\n",
    "# 模型相关\n",
    "origin_model = 'lstm' # 采用模型\n",
    "\n",
    "momentum = 0.5\n",
    "weight_decay = 0  # 模型权重衰减参数，强制参数向0靠拢（和学习率衰减不一样！）这个是给我的原始代码中就是这样（设为0表示不引入）\n",
    "bc_size = 50\n",
    "test_bc_size = 128\n",
    "num_classes = 20  # 分别数量，CIFAR100中是20（FedMut和CLGG都是这么采用的）\n",
    "\n",
    "# 联邦训练的超参数\n",
    "global_round = 100  # 全局训练轮数，可根据需要调整\n",
    "eta = 0.01  # 客户端端学习率，从{0.01, 0.1, 1}中调优\n",
    "gamma = 0.01  # 服务器端学习率 从{0.005， 0.05， 0.5中调有}\n",
    "K = 5  # 客户端本地训练轮数，从1，3，5中选\n",
    "E = 1  # 服务器本地训练轮数，从1，3，5中选\n",
    "M = 20  # 每一轮抽取客户端\n",
    "\n",
    "# FedMut中参数\n",
    "radius = 4.0  # alpha，控制mutation的幅度\n",
    "mut_acc_rate = 0.3  # 论文中的β0\n",
    "mut_bound = 50  # Tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shake数据集中类别数量： 75\n",
      "69096, 747, 19, 2413, 19, 20, 6639, 726, 3685, 1, 1, 1, 1, 2, 1, 1, 1, 1, 154, 1407, 859, 1426, 558, 434, 343, 584, 400, 351, 692, 2605, 117, 128, 376, 525, 465, 553, 359, 43, 303, 759, 1561, 137, 67, 1060, 6, 290, 10, 145, 144, 19936, 3809, 5404, 10768, 32195, 5631, 4576, 17609, 16125, 233, 2414, 11817, 7684, 17316, 23100, 3771, 199, 16609, 17563, 23553, 9268, 2819, 5909, 347, 7029, 93\n",
      "shake数据集中客户端数量： 128\n",
      "Traning Client Total: 366012 0 69096 747 19 0 2413 19 20 6639 726 3685 0 1 1 1 1 2 1 1 1 1 154 1407 0 859 1426 558 434 343 584 400 351 692 2605 117 128 376 525 465 553 359 43 303 759 1561 137 67 1060 6 290 10 145 144 19936 3809 5404 10768 32195 5631 4576 17609 16125 233 2414 11817 7684 17316 23100 3771 199 16609 17563 23553 9268 2819 5909 347 7029 93\n",
      "Client 0: 4730 0 915 9 1 0 33 0 0 71 5 54 0 0 0 0 0 0 0 0 0 0 1 20 0 9 16 7 3 4 7 6 4 8 46 0 2 4 8 4 2 2 0 1 9 18 1 0 14 0 3 0 2 2 231 50 67 134 430 66 57 228 202 2 25 138 113 223 315 41 4 234 223 291 119 48 81 8 108 1\n",
      "Client 1: 5158 0 975 8 1 0 40 0 0 82 9 52 0 0 0 0 0 0 0 0 0 0 1 21 0 23 11 8 4 3 8 4 4 12 37 0 1 5 8 7 4 3 0 5 6 26 2 0 18 0 7 0 1 1 276 42 60 135 477 72 62 270 212 1 33 156 103 249 338 54 4 240 252 337 137 50 83 4 113 1\n",
      "Client 2: 2298 0 460 1 0 0 20 0 0 35 3 31 0 0 0 0 0 0 0 0 0 0 2 9 0 4 11 3 1 2 3 2 2 7 25 1 1 2 5 2 1 1 0 2 3 10 1 0 6 0 4 0 0 0 117 22 24 61 188 24 29 106 110 2 18 75 63 108 150 21 1 99 100 132 63 25 42 1 56 1\n",
      "Client 3: 8400 0 1600 6 2 0 50 0 0 143 9 82 0 0 0 0 0 0 0 0 0 0 3 36 0 14 24 18 6 5 10 8 5 14 73 1 3 8 17 7 13 5 0 6 12 45 1 0 29 0 6 0 2 2 442 81 113 233 760 134 107 414 399 3 52 262 197 394 504 94 3 391 420 519 216 85 141 9 161 1\n",
      "Client 4: 799 0 147 0 0 0 3 0 0 15 0 7 0 0 0 0 0 0 0 0 0 0 0 3 0 0 1 1 0 1 0 1 0 2 5 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 44 4 10 29 75 17 9 39 36 0 3 20 21 36 54 8 2 40 41 52 22 9 17 1 13 1\n",
      "Client 5: 835 0 161 1 0 0 2 0 0 16 1 6 0 0 0 0 0 0 0 0 0 0 0 5 0 0 1 0 1 2 1 1 1 1 6 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 42 7 11 28 82 11 13 45 39 0 4 31 21 37 56 3 1 45 31 51 20 11 13 1 19 1\n",
      "Client 6: 1201 0 241 1 0 0 6 0 0 19 1 11 0 0 0 0 0 0 0 0 0 0 0 6 0 2 5 2 2 1 1 1 0 1 13 0 0 1 1 2 3 7 0 1 4 6 0 0 3 0 1 0 0 0 63 21 14 33 110 17 13 57 46 0 7 36 25 53 77 10 0 48 60 82 29 17 15 2 24\n",
      "Client 7: 4877 0 930 12 0 0 44 0 0 66 16 54 0 0 0 0 0 0 0 0 0 0 0 20 0 15 11 4 7 3 10 3 3 8 47 1 1 3 5 7 5 6 1 1 8 13 1 1 16 0 6 0 1 1 260 49 72 124 433 71 63 238 230 3 26 147 116 246 290 40 2 225 249 337 118 44 73 5 86\n",
      "Client 8: 663 0 126 1 0 0 4 0 0 7 0 8 0 0 0 0 0 0 0 0 0 0 0 2 0 1 1 1 1 0 1 0 1 4 7 0 0 2 1 0 2 1 0 1 2 1 1 0 3 0 1 0 0 0 33 10 8 11 61 9 11 26 35 0 4 19 16 39 45 5 1 33 28 44 18 3 10 1 13\n",
      "Client 9: 3553 0 668 9 0 0 31 0 0 76 12 41 0 0 0 0 0 0 0 0 0 0 1 9 0 8 12 7 2 2 6 3 2 8 30 1 1 3 8 7 10 3 1 3 8 19 1 0 10 0 5 0 3 3 187 37 50 118 296 48 46 156 153 1 20 123 102 156 223 28 3 159 186 205 93 25 53 6 66\n",
      "测试集总数量 71560\n",
      "13531, 208, 490, 1, 1, 1307, 113, 879, 15, 323, 164, 310, 109, 95, 82, 198, 80, 76, 157, 506, 17, 34, 86, 131, 112, 168, 84, 4, 93, 166, 323, 31, 17, 173, 1, 52, 3, 53, 52, 3817, 765, 987, 2240, 6249, 1040, 940, 3319, 3173, 59, 486, 2258, 1500, 3236, 4422, 711, 37, 3323, 3340, 4423, 1863, 498, 1085, 107, 1415, 22\n",
      "Server: 36566 0 6909 74 1 0 241 1 2 663 72 368 0 0 0 0 0 0 0 0 0 0 15 140 0 85 142 55 43 34 58 40 35 69 260 11 12 37 52 46 55 35 4 30 75 156 13 6 106 0 29 1 14 14 1993 380 540 1076 3219 563 457 1760 1612 23 241 1181 768 1731 2310 377 19 1660 1756 2355 926 281 590 34 702 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [02:43<24:31, 163.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 169\u001b[0m\n\u001b[1;32m    166\u001b[0m results_train_loss \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# CLG_Mut 训练\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m test_acc_CLG_Mut, train_loss_CLG_Mut \u001b[38;5;241m=\u001b[39m \u001b[43mCLG_Mut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_round\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m results_test_acc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLG_Mut\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_acc_CLG_Mut\n\u001b[1;32m    171\u001b[0m results_train_loss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLG_Mut\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_loss_CLG_Mut\n",
      "Cell \u001b[0;32mIn[44], line 37\u001b[0m, in \u001b[0;36mCLG_Mut\u001b[0;34m(net_glob, global_round, eta, gamma, K, E, M)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(idxs_users):\n\u001b[1;32m     35\u001b[0m     net_glob\u001b[38;5;241m.\u001b[39mload_state_dict(w_locals[i])\n\u001b[0;32m---> 37\u001b[0m     update_client_w, client_round_loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet_glob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     w_locals[i] \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(update_client_w)\n\u001b[1;32m     39\u001b[0m     local_loss\u001b[38;5;241m.\u001b[39mappend(client_round_loss)\n",
      "Cell \u001b[0;32mIn[35], line 30\u001b[0m, in \u001b[0;36mupdate_weights\u001b[0;34m(model_weight, dataset, learning_rate, local_epoch)\u001b[0m\n\u001b[1;32m     28\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m], labels\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m batch_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m/\u001b[39mimages\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/home/anaconda/envs/env8/lib/python3.8/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/anaconda/envs/env8/lib/python3.8/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/anaconda/envs/env8/lib/python3.8/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def set_random_seed(seed):\n",
    "    \"\"\"\n",
    "        set random seed\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:\" + str(GPU) if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# 固定随机数\n",
    "if random_fix:\n",
    "    set_random_seed(seed)\n",
    "\n",
    "if origin_model == 'resnet':\n",
    "# 准备CIFAR100数据集\n",
    "    cifar, test_dataset = CIFAR100()\n",
    "    prob = get_prob(non_iid, client_num, class_num=20)\n",
    "    client_data = create_data_all_train(prob, size_per_client, cifar, N=20)   # 这里改为全部构建训练集\n",
    "\n",
    "    # 将测试标签转换为粗类别\n",
    "    test_dataset.targets = sparse2coarse(test_dataset.targets)\n",
    "\n",
    "    # 如果需要确保测试标签为整数类型\n",
    "    test_dataset.targets = test_dataset.targets.astype(int)\n",
    "\n",
    "\n",
    "    #CIFAR1--IID 挑选服务器子集：\n",
    "    if server_iid:\n",
    "        server_images, server_labels = select_server_subset(cifar, percentage=server_percentage, mode='iid')\n",
    "    else:\n",
    "        server_images, server_labels = select_server_subset(cifar, percentage=server_percentage,\n",
    "                                                        mode='non-iid', dirichlet_alpha=0.5)\n",
    "\n",
    "    # CIFAR100——用了FedMut中定义的CNN网络\n",
    "    init_model = ResNet18_cifar10().to(device)\n",
    "    initial_w = copy.deepcopy(init_model.state_dict())\n",
    "elif origin_model =='lstm':\n",
    "    # 准备shakespeare数据集\n",
    "    train_dataset = ShakeSpeare(True)\n",
    "    test_dataset = ShakeSpeare(False)\n",
    "\n",
    "    total_shake,total_label = [],[]\n",
    "    for item,labels in train_dataset:\n",
    "        total_shake.append(item.numpy())\n",
    "        total_label.append(labels)\n",
    "    total_shake = np.array(total_shake)\n",
    "    total_label = np.array(total_label)\n",
    "\n",
    "    shake = [total_shake, total_label]\n",
    "\n",
    "    # 构建每个client的数据量\n",
    "    dict_users = train_dataset.get_client_dic()\n",
    "\n",
    "    # 统计类别数量\n",
    "    unique_classes = np.unique(total_label)\n",
    "    num_classes = len(unique_classes)\n",
    "    print(\"shake数据集中类别数量：\", num_classes)\n",
    "    # 对于每个类别计算样本数量\n",
    "    class_counts = [np.sum(total_label == cls) for cls in unique_classes]\n",
    "    # 将数量转换成字符串后，用逗号隔开，并打印（只输出数字）\n",
    "    print(\", \".join(map(str, class_counts)))\n",
    "\n",
    "    # 统计客户端数量\n",
    "    num_clients = len(dict_users)\n",
    "    print(\"shake数据集中客户端数量：\", num_clients)\n",
    "\n",
    "\n",
    "    # 构建client_data\n",
    "    client_data = []\n",
    "    for client in sorted(dict_users.keys()):\n",
    "        indices = np.array(list(dict_users[client]), dtype=np.int64)\n",
    "        client_images = total_shake[indices]\n",
    "        client_labels = total_label[indices]\n",
    "        client_data.append((client_images, client_labels))\n",
    "\n",
    "\n",
    "    # Shake 挑选服务器子集，通过 Dirichlet 分布参数控制（例如 dirichlet_alpha=0.5）：\n",
    "    if server_iid:\n",
    "        server_images, server_labels = select_server_subset(shake, percentage=server_percentage,\n",
    "                                                      mode='iid')\n",
    "    else:\n",
    "        server_images, server_labels = select_server_subset(shake, percentage=server_percentage,\n",
    "                                                        mode='non-iid', dirichlet_alpha=0.5)\n",
    "\n",
    "\n",
    "    # Shakespeare —— 用FedMut中提出的LSTM网络\n",
    "    init_model = CharLSTM().to(device)\n",
    "    initial_w = copy.deepcopy(init_model.state_dict())\n",
    "\n",
    "\n",
    "#  打印数据集情况\n",
    "all_images = []\n",
    "all_labels = []\n",
    "for data in client_data:\n",
    "    all_images.extend(data[0])\n",
    "    all_labels.extend(data[1])\n",
    "comb_client_data = [np.array(all_images), np.array(all_labels)]\n",
    "\n",
    "# 输出comb_client_data情况\n",
    "imgs, lbls = comb_client_data\n",
    "lbls = np.array(lbls)\n",
    "total_count = len(lbls)\n",
    "unique_classes, counts = np.unique(lbls, return_counts=True)\n",
    "\n",
    "num_classes = int(unique_classes.max()) + 1  # 列表长度应该为最大类别\n",
    "class_counts = [0] * num_classes\n",
    "\n",
    "for cls, cnt in zip(unique_classes, counts):\n",
    "    class_counts[cls] = cnt\n",
    "\n",
    "# 打印格式：Total: 总数 类别0计数 类别1计数 ... 类别19计数\n",
    "print(\"Traning Client Total: {}\".format(\" \".join([str(total_count)] + [str(c) for c in class_counts])))\n",
    "\n",
    "\n",
    "# 打印每个客户端训练数据情况（只输出前10个）\n",
    "for i, (imgs, lbls) in enumerate(client_data[:10]):\n",
    "    lbls = np.array(lbls)\n",
    "    total_count = len(lbls)\n",
    "    unique_classes, counts = np.unique(lbls, return_counts=True)\n",
    "    \n",
    "    num_classes = int(unique_classes.max()) + 1  # 列表长度应该为最大类别\n",
    "    class_counts = [0] * num_classes\n",
    "    \n",
    "    for cls, cnt in zip(unique_classes, counts):\n",
    "        class_counts[cls] = cnt\n",
    "    # 打印格式：Client i: 总数 类别0计数 类别1计数 ... 类别19计数\n",
    "    print(\"Client {}: {}\".format(i, \" \".join([str(total_count)] + [str(c) for c in class_counts])))\n",
    "    \n",
    "\n",
    "# 为了与后续代码兼容，这里将 server_data 定义为一个列表：[images, labels]\n",
    "server_data = [server_images, server_labels]\n",
    "\n",
    "# 输出测试集数据\n",
    "total_count = len(test_dataset)\n",
    "labels = np.array(test_dataset.label)\n",
    "_, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "print(f\"测试集总数量 {total_count}\")\n",
    "print(\", \".join(str(c) for c in counts))\n",
    "\n",
    "# 打印服务器数据情况\n",
    "s_imgs, s_lbls = server_data\n",
    "s_lbls = np.array(s_lbls)\n",
    "total_count = len(s_lbls)\n",
    "unique_classes, counts = np.unique(s_lbls, return_counts=True)\n",
    "\n",
    "num_classes = int(unique_classes.max()) + 1  # 列表长度应该为最大类别+1\n",
    "class_counts = [0] * num_classes\n",
    "\n",
    "for cls, cnt in zip(unique_classes, counts):\n",
    "    class_counts[cls] = cnt\n",
    "# 输出格式: Server: 总数 类别0计数 类别1计数 ... 类别19计数\n",
    "print(\"Server: {}\".format(\" \".join([str(total_count)] + [str(c) for c in class_counts])))\n",
    "# print(\"  前5个标签: \", lbls[:5])\n",
    "# print(\"  前5个数据形状: \", [server_data[0][j].shape for j in range(min(5, len(server_data[0])))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 初始化结果存储字典\n",
    "results_test_acc = {}\n",
    "results_train_loss = {}\n",
    "\n",
    "# CLG_Mut 训练\n",
    "test_acc_CLG_Mut, train_loss_CLG_Mut = CLG_Mut(copy.deepcopy(init_model), global_round, eta, gamma, K, E, M)\n",
    "results_test_acc['CLG_Mut'] = test_acc_CLG_Mut\n",
    "results_train_loss['CLG_Mut'] = train_loss_CLG_Mut\n",
    "\n",
    "# CLG_Mut_2 训练\n",
    "test_acc_CLG_Mut_2, train_loss_CLG_Mut_2 = CLG_Mut_2(copy.deepcopy(init_model), global_round, eta, gamma, K, E, M)\n",
    "results_test_acc['CLG_Mut_2'] = test_acc_CLG_Mut_2\n",
    "results_train_loss['CLG_Mut_2'] = train_loss_CLG_Mut_2\n",
    "\n",
    "# CLG_Mut_3 训练\n",
    "test_acc_CLG_Mut_3, train_loss_CLG_Mut_3 = CLG_Mut_3(copy.deepcopy(init_model), global_round, eta, gamma, K, E, M)\n",
    "results_test_acc['CLG_Mut_3'] = test_acc_CLG_Mut_3\n",
    "results_train_loss['CLG_Mut_3'] = train_loss_CLG_Mut_3\n",
    "\n",
    "\n",
    "# FedMut 训练\n",
    "test_acc_FedMut, train_loss_FedMut = FedMut(copy.deepcopy(init_model), global_round, eta, K, M)\n",
    "results_test_acc['FedMut'] = test_acc_FedMut\n",
    "results_train_loss['FedMut'] = train_loss_FedMut\n",
    "\n",
    "# Server-only 训练\n",
    "test_acc_server_only, train_loss_server_only = server_only(initial_w, global_round, gamma, E)\n",
    "results_test_acc['Server_only'] = test_acc_server_only\n",
    "results_train_loss['Server_only'] = train_loss_server_only\n",
    "\n",
    "# FedAvg 训练\n",
    "test_acc_fedavg, train_loss_fedavg = fedavg(initial_w, global_round, eta, K, M)\n",
    "results_test_acc['FedAvg'] = test_acc_fedavg\n",
    "results_train_loss['FedAvg'] = train_loss_fedavg\n",
    "\n",
    "# CLG_SGD 训练\n",
    "test_acc_CLG_SGD, train_loss_CLG_SGD = CLG_SGD(initial_w, global_round, eta, gamma, K, E, M)\n",
    "results_test_acc['CLG_SGD'] = test_acc_CLG_SGD\n",
    "results_train_loss['CLG_SGD'] = train_loss_CLG_SGD\n",
    "\n",
    "# 如果存在至少20轮训练，则输出第二十轮的测试精度和训练损失\n",
    "for algo in results_test_acc:\n",
    "    if len(results_test_acc[algo]) >= 20:\n",
    "        print(f\"{algo} - 第二十轮测试精度: {results_test_acc[algo][19]:.2f}%, 第二十轮训练损失: {results_train_loss[algo][19]:.4f}\")\n",
    "\n",
    "# 打印最终训练结果\n",
    "for algo in results_test_acc:\n",
    "    print(f\"{algo} - 最终测试精度: {results_test_acc[algo][-1]:.2f}%, 最终训练损失: {results_train_loss[algo][-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import platform\n",
    "import datetime\n",
    "\n",
    "\n",
    "# 定义训练轮数\n",
    "rounds = range(1, global_round + 1)\n",
    "\n",
    "# 设置绘图风格（可选）\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# 获取当前时间戳，格式为 YYYYmmdd_HHMMSS\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Plot Test Accuracy Comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "for algo, acc in results_test_acc.items():\n",
    "    plt.plot(rounds, acc, label=algo)\n",
    "plt.xlabel('Training Rounds', fontsize=14)\n",
    "plt.ylabel('Test Accuracy (%)', fontsize=14)\n",
    "plt.title('Test Accuracy Comparison of Different Algorithms', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(f'output/test_accuracy_comparison_{timestamp}.png')  # 保存图像\n",
    "\n",
    "# Plot Train Loss Comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "for algo, loss in results_train_loss.items():\n",
    "    plt.plot(rounds, loss, label=algo)\n",
    "plt.xlabel('Training Rounds', fontsize=14)\n",
    "plt.ylabel('Train Loss', fontsize=14)\n",
    "plt.title('Train Loss Comparison of Different Algorithms', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(f'output/train_loss_comparison_{timestamp}.png')  # 保存图像"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
