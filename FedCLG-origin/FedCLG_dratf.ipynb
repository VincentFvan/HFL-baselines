{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "id": "gSK1TSekTVeu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.functional as func\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "id": "9-WEXWakTwf8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # 解决由于多次加载 OpenMP 相关动态库而引起的冲突"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sns3blEITybc",
    "outputId": "120095fb-5597-4ada-8c12-b4470d8ad28e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan  3 16:48:22 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:21:00.0 Off |                  Off |\n",
      "| 30%   30C    P8              11W / 350W |    561MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off | 00000000:E1:00.0 Off |                  Off |\n",
      "| 30%   32C    P8              17W / 350W |     14MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      4802      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A    314011      C   /home/anaconda/envs/env8/bin/python         544MiB |\n",
      "|    1   N/A  N/A      4802      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "id": "96gqMCQneWho"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LinearBottleNeck(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride, t=6, class_num=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * t, 1),\n",
    "            nn.BatchNorm2d(in_channels * t),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels * t, in_channels * t, 3, stride=stride, padding=1, groups=in_channels * t),\n",
    "            nn.BatchNorm2d(in_channels * t),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels * t, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.stride = stride\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = self.residual(x)\n",
    "\n",
    "        if self.stride == 1 and self.in_channels == self.out_channels:\n",
    "            residual += x\n",
    "\n",
    "        return residual\n",
    "\n",
    "# MobileNetV2（比lenet更复杂的CNN网络）网络中的线性瓶颈结构，原文中用于CIFAR-100任务\n",
    "class MobileNetV2(nn.Module):\n",
    "\n",
    "    def __init__(self, class_num=20):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pre = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.stage1 = LinearBottleNeck(32, 16, 1, 1)\n",
    "        self.stage2 = self._make_stage(2, 16, 24, 2, 6)\n",
    "        self.stage3 = self._make_stage(3, 24, 32, 2, 6)\n",
    "        self.stage4 = self._make_stage(4, 32, 64, 2, 6)\n",
    "        self.stage5 = self._make_stage(3, 64, 96, 1, 6)\n",
    "        self.stage6 = self._make_stage(3, 96, 160, 1, 6)\n",
    "        self.stage7 = LinearBottleNeck(160, 320, 1, 6)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(320, 1280, 1),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(1280, class_num, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "        x = self.stage6(x)\n",
    "        x = self.stage7(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _make_stage(self, repeat, in_channels, out_channels, stride, t):\n",
    "\n",
    "        layers = []\n",
    "        layers.append(LinearBottleNeck(in_channels, out_channels, stride, t))\n",
    "\n",
    "        while repeat - 1:\n",
    "            layers.append(LinearBottleNeck(out_channels, out_channels, 1, t))\n",
    "            repeat -= 1\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "def mobilenetv2():\n",
    "    return MobileNetV2()\n",
    "\n",
    "\n",
    "# FedMut中采用的cnn模型\n",
    "class CNNCifar(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNCifar, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x, start_layer_idx=0, logit=False):\n",
    "        if start_layer_idx < 0:  #\n",
    "            return self.mapping(x, start_layer_idx=start_layer_idx, logit=logit)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        result = {'activation' : x}\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        result['hint'] = x\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        result['representation'] = x\n",
    "        x = self.fc3(x)\n",
    "        result['output'] = x\n",
    "        return result\n",
    "\n",
    "    def mapping(self, z_input, start_layer_idx=-1, logit=True):\n",
    "        z = z_input\n",
    "        z = self.fc3(z)\n",
    "\n",
    "        result = {'output': z}\n",
    "        if logit:\n",
    "            result['logit'] = z\n",
    "        return result\n",
    "    \n",
    "def cnncifar():\n",
    "    return CNNCifar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "id": "QkvtGtMuUDmr"
   },
   "outputs": [],
   "source": [
    "# def test_inference(model, test):\n",
    "#     \"\"\" Returns the test accuracy and loss.\n",
    "#     \"\"\"\n",
    "#     tensor_x = torch.Tensor(test[0]).to(device)\n",
    "#     tensor_y = torch.Tensor(test[1]).to(device)\n",
    "#     test_dataset = TensorDataset(tensor_x, tensor_y)\n",
    "\n",
    "#     model.eval()\n",
    "#     loss, total, correct = 0.0, 0.0, 0.0\n",
    "\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     testloader = DataLoader(test_dataset, batch_size=bc_size,\n",
    "#                             shuffle=True)\n",
    "\n",
    "#     for batch_idx, (images, labels) in enumerate(testloader):\n",
    "#         with torch.no_grad():  # 在测试过程中不需要计算梯度，节省内存和加速计算\n",
    "#         # Inference\n",
    "#             outputs = model(images)\n",
    "#             batch_loss = criterion(outputs, labels.long())\n",
    "#             loss += batch_loss.item() * labels.size(0) # 计算损失值，更好反映模型输出概率分布与真实标签的差距\n",
    "\n",
    "#         # Prediction\n",
    "#             _, pred_labels = torch.max(outputs, 1)\n",
    "#             pred_labels = pred_labels.view(-1)\n",
    "#             correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "#             total += len(labels)\n",
    "#     #print(correct,\"/\",total)\n",
    "#     accuracy = correct/total\n",
    "    \n",
    "#     print(\"Testing accuracy: {:.2f}\", accuracy)\n",
    "#     return accuracy, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新的测试：针对整个测试数据集的测试\n",
    "def test_inference(net_glob, dataset_test):\n",
    "    # testing\n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test)\n",
    "\n",
    "    # print(\"Testing accuracy: {:.2f}\".format(acc_test))\n",
    "\n",
    "    return acc_test.item()\n",
    "\n",
    "def test_img(net_g, datatest):\n",
    "    net_g.eval()\n",
    "    # testing\n",
    "    # test loss代表在测试集上的平均损失（对测试数据的预测输出与真实标签的差距）\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    data_loader = DataLoader(datatest, batch_size=bc_size)\n",
    "    l = len(data_loader)\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, target) in enumerate(data_loader):\n",
    "            if gpu != -1:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            log_probs = net_g(data)['output']\n",
    "            # sum up batch loss\n",
    "            test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n",
    "            # get the index of the max log-probability\n",
    "            y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "            correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.00 * correct / len(data_loader.dataset)\n",
    "    if verbose:\n",
    "        print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(data_loader.dataset), accuracy))\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "id": "jURskA9VUJOF"
   },
   "outputs": [],
   "source": [
    "# 将CIFAR-100的100个类别转为20个类别（粒度更粗，降低任务复杂度）\n",
    "def sparse2coarse(targets):\n",
    "    \"\"\"Convert Pytorch CIFAR100 sparse targets to coarse targets.\n",
    "\n",
    "    Usage:\n",
    "        trainset = torchvision.datasets.CIFAR100(path)\n",
    "        trainset.targets = sparse2coarse(trainset.targets)\n",
    "    \"\"\"\n",
    "    coarse_labels = np.array([ 4,  1, 14,  8,  0,  6,  7,  7, 18,  3,\n",
    "                               3, 14,  9, 18,  7, 11,  3,  9,  7, 11,\n",
    "                               6, 11,  5, 10,  7,  6, 13, 15,  3, 15,\n",
    "                               0, 11,  1, 10, 12, 14, 16,  9, 11,  5,\n",
    "                               5, 19,  8,  8, 15, 13, 14, 17, 18, 10,\n",
    "                               16, 4, 17,  4,  2,  0, 17,  4, 18, 17,\n",
    "                               10, 3,  2, 12, 12, 16, 12,  1,  9, 19,\n",
    "                               2, 10,  0,  1, 16, 12,  9, 13, 15, 13,\n",
    "                              16, 19,  2,  4,  6, 19,  5,  5,  8, 19,\n",
    "                              18,  1,  2, 15,  6,  0, 17,  8, 14, 13])\n",
    "    return coarse_labels[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "id": "jcZ7eQGET_YJ"
   },
   "outputs": [],
   "source": [
    "# 共有6w个图像，其中5w训练，1w测试\n",
    "def CIFAR100():\n",
    "    '''Return Cifar100\n",
    "    '''\n",
    "    train_dataset = torchvision.datasets.CIFAR100(root='../data/CIFAR-100',\n",
    "                                            train=True,\n",
    "                                            transform=transforms.ToTensor(),\n",
    "                                            download=True)\n",
    "    test_dataset = torchvision.datasets.CIFAR100(root='../data/CIFAR-100',\n",
    "                                            train=False,\n",
    "                                            transform=transforms.ToTensor(),\n",
    "                                            download=True)\n",
    "    total_img,total_label = [],[]\n",
    "    for imgs,labels in train_dataset:\n",
    "        total_img.append(imgs.numpy())\n",
    "        total_label.append(labels)\n",
    "    # for imgs,labels in test_dataset:\n",
    "    #     total_img.append(imgs.numpy())\n",
    "    #     total_label.append(labels) \n",
    "    total_img = np.array(total_img)\n",
    "    total_label = np.array(sparse2coarse(total_label))\n",
    "\n",
    "    cifar = [total_img, total_label]\n",
    "    return cifar, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "id": "f1eQhNtPUMOF"
   },
   "outputs": [],
   "source": [
    "# 基于 Dirichlet 分布 来模拟non-IID。返回一个形状为 (client_num, class_num) 的概率矩阵，每一行代表一个客户端对各类别的概率分布。\n",
    "def get_prob(non_iid, client_num, class_num = 20):\n",
    "    # Modify：我之后加上的\n",
    "    if data_random_fix:\n",
    "        np.random.seed(seed_num)  # 固定种子，确保数据抽样一致\n",
    "    \n",
    "    return np.random.dirichlet(np.repeat(non_iid, class_num), client_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "id": "npT3idE-UaGm"
   },
   "outputs": [],
   "source": [
    "# 全部用于构建训练集\n",
    "def create_data_all_train(prob, size_per_client, dataset, N=20):\n",
    "    total_each_class = size_per_client * np.sum(prob, 0)\n",
    "    data, label = dataset\n",
    "\n",
    "    if data_random_fix:\n",
    "        np.random.seed(seed_num)  # 固定种子，确保数据抽样一致\n",
    "        random.seed(seed_num)\n",
    "\n",
    "    # 为每个类别随机采样数据\n",
    "    all_class_set = []\n",
    "    for i in range(N):\n",
    "        size = total_each_class[i]\n",
    "        sub_data = data[label == i]\n",
    "        sub_label = label[label == i]\n",
    "\n",
    "        rand_indx = np.random.choice(len(sub_data), size=int(size), replace=False).astype(int)\n",
    "        sub2_data, sub2_label = sub_data[rand_indx], sub_label[rand_indx]\n",
    "        all_class_set.append((sub2_data, sub2_label))\n",
    "\n",
    "    index = [0] * N\n",
    "    clients = []\n",
    "\n",
    "    for m in range(prob.shape[0]):  # 遍历客户端\n",
    "        labels, images = [], []  # 训练数据\n",
    "\n",
    "        for n in range(N):\n",
    "            # 100%用于训练\n",
    "            start, end = index[n], index[n] + int(prob[m][n] * size_per_client)\n",
    "            image, label = all_class_set[n][0][start:end], all_class_set[n][1][start:end]\n",
    "\n",
    "            # 记录当前类别的数据分配进度\n",
    "            index[n] += int(prob[m][n] * size_per_client)\n",
    "\n",
    "            labels.extend(label)\n",
    "            images.extend(image)\n",
    "\n",
    "        clients.append((np.array(images), np.array(labels)))\n",
    "\n",
    "    return clients\n",
    "\n",
    "# 80%构建训练集，20%构建测试集\n",
    "def create_data(prob, size_per_client, dataset, N=20):\n",
    "    total_each_class = size_per_client * np.sum(prob, 0)\n",
    "    data, label = dataset\n",
    "\n",
    "    # Modify：我之后加上的\n",
    "    if data_random_fix:\n",
    "        np.random.seed(seed_num)  # 固定种子，确保数据抽样一致\n",
    "        random.seed(seed_num)\n",
    "\n",
    "    # 为每个类别随机采样数据\n",
    "    all_class_set = []\n",
    "    for i in range(N):\n",
    "        size = total_each_class[i]\n",
    "        sub_data = data[label == i]\n",
    "        sub_label = label[label == i]\n",
    "\n",
    "        rand_indx = np.random.choice(len(sub_data), size=int(size), replace=False).astype(int)\n",
    "        sub2_data, sub2_label = sub_data[rand_indx], sub_label[rand_indx]\n",
    "        all_class_set.append((sub2_data, sub2_label))\n",
    "\n",
    "    index = [0] * N\n",
    "    clients, test = [], []\n",
    "\n",
    "    for m in range(prob.shape[0]):  # 遍历客户端\n",
    "        labels, images = [], []  # 训练数据\n",
    "        tlabels, timages = [], [] # 测试数据\n",
    "\n",
    "        for n in range(N):\n",
    "            # 80%用于训练，20%用于测试\n",
    "            # 这里的int向下取整，会导致实际的数据量比计算略小\n",
    "            start, end = index[n], index[n] + int(prob[m][n] * size_per_client * 0.8)\n",
    "            test_start, test_end = end, index[n] + int(prob[m][n] * size_per_client)\n",
    "\n",
    "            image, label = all_class_set[n][0][start:end], all_class_set[n][1][start:end]\n",
    "            test_image, test_label = all_class_set[n][0][test_start:test_end], all_class_set[n][1][test_start:test_end]\n",
    "\n",
    "            # 记录当前类别的数据分配进度\n",
    "            index[n] += int(prob[m][n] * size_per_client)\n",
    "\n",
    "            labels.extend(label)\n",
    "            images.extend(image)\n",
    "\n",
    "            tlabels.extend(test_label)\n",
    "            timages.extend(test_image)\n",
    "\n",
    "        clients.append((np.array(images), np.array(labels)))\n",
    "        test.append((np.array(timages), np.array(tlabels)))\n",
    "\n",
    "    return clients, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "id": "DCsR6_QqUzJ8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 合并所有客户端的测试数据 （上面讲测试数据分成了不同的客户端）\n",
    "# 但并没有使用，用途不明\n",
    "def comb_client_test_func(client_test_data):\n",
    "    comb_client_test_image = []\n",
    "    comb_client_test_label = []\n",
    "    for i in range(client_num):\n",
    "        comb_client_test_image.extend(list(client_test_data[i][0]))\n",
    "        comb_client_test_label.extend(list(client_test_data[i][1]))\n",
    "    \n",
    "    # 将测试图片和标签合并为 numpy 数组\n",
    "    comb_client_test_image = np.array(comb_client_test_image)\n",
    "    comb_client_test_label = np.array(comb_client_test_label)\n",
    "    \n",
    "    label_count = Counter(comb_client_test_label)\n",
    "    print(\"测试集类别分布：\")\n",
    "    for label, count in sorted(label_count.items()):\n",
    "        print(f\"类别 {label}: {count} 个样本\")\n",
    "    \n",
    "    return [comb_client_test_image, comb_client_test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "id": "JEKzDM0yW3DW"
   },
   "outputs": [],
   "source": [
    "# 从数据集中按类别均匀抽取子集，并按照指定的比例 percentage 进行缩减，同时对数据进行随机打乱\n",
    "def select_subset(whole_set, percentage):\n",
    "    \n",
    "    # Modify：我之后加上的\n",
    "    if data_random_fix:\n",
    "        np.random.seed(seed_num)  # 固定种子，确保数据抽样一致\n",
    "        random.seed(seed_num)\n",
    "    \n",
    "    a = whole_set[0]\n",
    "    b = whole_set[1]\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(\"Both arrays should have the same length.\")\n",
    "\n",
    "    if not 0 <= percentage <= 1:\n",
    "        raise ValueError(\"Percentage must be between 0 and 1.\")\n",
    "\n",
    "    unique_classes = np.unique(b)\n",
    "\n",
    "    a_prime = []\n",
    "    b_prime = []\n",
    "\n",
    "    for cls in unique_classes:\n",
    "        indices = np.where(b == cls)[0]\n",
    "        subset_size = int(len(indices) * percentage)\n",
    "\n",
    "        selected_indices = np.random.choice(indices, subset_size, replace=False)\n",
    "\n",
    "        a_prime.extend(a[selected_indices])\n",
    "        b_prime.extend(b[selected_indices])\n",
    "\n",
    "    a_prime, b_prime = np.array(a_prime), np.array(b_prime)\n",
    "\n",
    "    # Shuffle arrays to randomize the order of elements\n",
    "    shuffle_indices = np.random.permutation(len(a_prime))\n",
    "    a_prime, b_prime = a_prime[shuffle_indices], b_prime[shuffle_indices]\n",
    "\n",
    "    return [a_prime, b_prime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "id": "XvuicdZfeDvL"
   },
   "outputs": [],
   "source": [
    "# 本地训练并更新权重，返回更新后的模型权重、平均训练损失以及第一个迭代的梯度信息\n",
    "def update_weights(model_weight, dataset, learning_rate, local_epoch):\n",
    "    model = cnncifar().to(device)\n",
    "    model.load_state_dict(model_weight)\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    Tensor_set = TensorDataset(torch.Tensor(dataset[0]).to(device), torch.Tensor(dataset[1]).to(device))\n",
    "    data_loader = DataLoader(Tensor_set, batch_size=bc_size, shuffle=True)\n",
    "\n",
    "    first_iter_gradient = None  # 初始化变量来保存第一个iter的梯度\n",
    "\n",
    "    for iter in range(local_epoch):\n",
    "        batch_loss = []\n",
    "        for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs['output'], labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss.append(loss.item()/images.shape[0])\n",
    "\n",
    "            # 保存第一个iter的梯度\n",
    "            if iter == 0 and batch_idx == 0:\n",
    "                first_iter_gradient = {}\n",
    "                for name, param in model.named_parameters():\n",
    "                    first_iter_gradient[name] = param.grad.clone()\n",
    "                # 保存 BatchNorm 层的 running mean 和 running variance\n",
    "                for name, module in model.named_modules():\n",
    "                    if isinstance(module, nn.BatchNorm2d):\n",
    "                        first_iter_gradient[name + '.running_mean'] = module.running_mean.clone()\n",
    "                        first_iter_gradient[name + '.running_var'] = module.running_var.clone()\n",
    "\n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "\n",
    "    return model.state_dict(), sum(epoch_loss) / len(epoch_loss), first_iter_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "id": "qsPZGD4Iem5w"
   },
   "outputs": [],
   "source": [
    "# 计算模型权重的差异，并根据学习率 lr 对权重差异进行缩放\n",
    "def weight_differences(n_w, p_w, lr):\n",
    "    w_diff = copy.deepcopy(n_w)\n",
    "    for key in w_diff.keys():\n",
    "        if 'num_batches_tracked' in key:\n",
    "            continue\n",
    "        w_diff[key] = (p_w[key] - n_w[key]) * lr\n",
    "    return w_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "id": "040d862vbG9M"
   },
   "outputs": [],
   "source": [
    "# 也是本地训练，不过引入了Fed-C的权重修正机制\n",
    "def update_weights_correction(model_weight, dataset, learning_rate, local_epoch, c_i, c_s):\n",
    "    model = cnncifar().to(device)\n",
    "    model.load_state_dict(model_weight)\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    Tensor_set = TensorDataset(torch.Tensor(dataset[0]).to(device), torch.Tensor(dataset[1]).to(device))\n",
    "    data_loader = DataLoader(Tensor_set, batch_size=bc_size, shuffle=True)\n",
    "\n",
    "    for iter in range(local_epoch):\n",
    "        batch_loss = []\n",
    "        for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs['output'], labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss.append(loss.sum().item()/images.shape[0])\n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "        corrected_graident = weight_differences(c_i, c_s, learning_rate)\n",
    "        orginal_model_weight = model.state_dict()\n",
    "        corrected_model_weight = weight_differences(corrected_graident, orginal_model_weight, 1)  # 这里缩放权重为1\n",
    "        model.load_state_dict(corrected_model_weight)\n",
    "\n",
    "    return model.state_dict(),  sum(epoch_loss) / len(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "id": "qeFHXRuEo5Du"
   },
   "outputs": [],
   "source": [
    "def average_weights(w):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        if 'num_batches_tracked' in key:\n",
    "            continue\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], len(w))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "id": "pLG9RrFffbl8"
   },
   "outputs": [],
   "source": [
    "# baseline: server-only\n",
    "def server_only(initial_w, global_round, gamma, E):\n",
    "    test_model = cnncifar().to(device)\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        # Server side local training\n",
    "        \n",
    "        \n",
    "        # 从comb中（论文中说明为全部训练数据）选择固定比率的server数据（并且是保证类别均衡的）\n",
    "        # server_data = select_subset(comb_client_data, server_percentage)\n",
    "                \n",
    "        update_server_w, round_loss, _ = update_weights(train_w, server_data, gamma, E)\n",
    "        train_w = update_server_w\n",
    "        test_model.load_state_dict(train_w)\n",
    "        train_loss.append(round_loss)\n",
    "        \n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "    \n",
    "        # Test Accuracy\n",
    "        # test_a = 0\n",
    "        # for i in client_test_data:\n",
    "        #     ac = test_inference(test_model,i)[0]\n",
    "        #     test_a = test_a + ac\n",
    "        # test_a = test_a/len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "        # print(test_a)\n",
    "    return test_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "id": "nGtAn28aok2c"
   },
   "outputs": [],
   "source": [
    "def fedavg(initial_w, global_round, eta, K, M):\n",
    "    test_model = cnncifar().to(device)\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    for round in tqdm(range(global_round)):\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "        for i in sampled_client:\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        train_w = average_weights(local_weights)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "        \n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "            \n",
    "        # test_a = 0\n",
    "        # for i in client_test_data:\n",
    "        #     ac = test_inference(test_model,i)[0]\n",
    "        #     test_a = test_a + ac\n",
    "        # test_a = test_a/len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "#         print(test_a)\n",
    "    return test_acc, train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybridFL(initial_w, global_round, eta, K, M):\n",
    "    \"\"\"\n",
    "    HybridFL算法：FedAvg改进，服务器也作为一个普通客户端参与训练。\n",
    "    \n",
    "    参数:\n",
    "    - initial_w: 初始模型权重\n",
    "    - global_round: 全局训练轮数\n",
    "    - eta: 学习率\n",
    "    - K: 本地训练轮数\n",
    "    - M: 每轮采样的客户端数量\n",
    "    \"\"\"\n",
    "    test_model = cnncifar().to(device)  # 初始化测试模型\n",
    "    train_w = copy.deepcopy(initial_w)     # 当前全局权重\n",
    "    test_acc = []                          # 保存每轮测试精度\n",
    "    train_loss = []                        # 保存每轮训练损失\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        local_weights, local_loss = [], []  # 存储每个客户端/服务器的权重和损失\n",
    "\n",
    "        # 随机采样 M 个客户端\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "\n",
    "        # 客户端本地训练\n",
    "        for i in sampled_client:\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        # 服务器参与训练\n",
    "        update_server_w, server_round_loss, _ = update_weights(train_w, server_data, eta, K)\n",
    "        local_weights.append(update_server_w)   # 将服务器权重加入列表\n",
    "        local_loss.append(server_round_loss)    # 将服务器损失加入列表\n",
    "\n",
    "        # 权重聚合\n",
    "        train_w = average_weights(local_weights)\n",
    "\n",
    "        # 评估模型性能\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss) / len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "    \n",
    "        \n",
    "        # test_a = 0\n",
    "        # for i in client_test_data:  # 遍历所有客户端测试数据\n",
    "        #     ac = test_inference(test_model, i)[0]\n",
    "        #     test_a += ac\n",
    "        # test_a = test_a / len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "        \n",
    "        # # 打印每轮的结果\n",
    "        # print(f\"Round {round + 1}: Test Accuracy = {test_a:.4f}, Train Loss = {loss_avg:.4f}\")\n",
    "    \n",
    "    return test_acc, train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "id": "uzxW0sUxRGth"
   },
   "outputs": [],
   "source": [
    "def CLG_SGD(initial_w, global_round, eta, gamma, K, E, M):\n",
    "    test_model = cnncifar().to(device)\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # 学习率衰减，这里默认注释掉了\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # 从总共client_num客户端中选择M个训练\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "        for i in sampled_client:\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "        train_w = average_weights(local_weights)\n",
    "        # Server side local training\n",
    "        \n",
    "        \n",
    "        # 从comb中（论文中说明为全部训练数据）选择固定比率的server数据（并且是保证类别均衡的）\n",
    "        # TODO_241216:这里是每一轮都重新选择数据（但保证类别比例是一样的，都是按照comb中的比例），我的场景中可以这样吗？\n",
    "        # server_data = select_subset(comb_client_data, server_percentage)\n",
    "        \n",
    "        update_server_w, round_loss, _ = update_weights(train_w, server_data, gamma, E)\n",
    "        train_w = update_server_w\n",
    "        local_loss.append(round_loss)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)   # 计算所有客户端和服务器一起的平均损失\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "    \n",
    "        # test_a = 0\n",
    "        # # 遍历客户端测试数据，计算平均准确率\n",
    "        # for i in client_test_data:\n",
    "        #     ac = test_inference(test_model,i)[0]\n",
    "        #     test_a = test_a + ac\n",
    "        # test_a = test_a/len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "#         print(test_a)\n",
    "    return test_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def CLG_Mut(net_glob, global_round, eta, gamma, K, E, M):\n",
    "    \n",
    "    net_glob.train()\n",
    "    \n",
    "    test_model = cnncifar().to(device)\n",
    "    train_w = copy.deepcopy(net_glob.state_dict())\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    w_locals = []\n",
    "    for i in range(M):\n",
    "        w_locals.append(copy.deepcopy(net_glob.state_dict()))\n",
    "    \n",
    "    delta_list = []\n",
    "    max_rank = 0\n",
    "    w_old = copy.deepcopy(net_glob.state_dict())\n",
    "    w_old_s1 = copy.deepcopy(net_glob.state_dict())\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # 学习率衰减，这里默认注释掉了\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # 从总共client_num客户端中选择M个训练\n",
    "        idxs_users = np.random.choice(range(client_num), M, replace=False)\n",
    "        for i, idx in enumerate(idxs_users):\n",
    "            net_glob.load_state_dict(w_locals[i])\n",
    "            \n",
    "            update_client_w, client_round_loss, _ = update_weights(copy.deepcopy(net_glob.state_dict()), client_data[idx], eta, K)\n",
    "            w_locals[i] = copy.deepcopy(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        # Global Model Generation\n",
    "        w_agg = Aggregation(w_locals, None)  \n",
    "        \n",
    "        # Server side local training\n",
    "        update_server_w, round_loss, _ = update_weights(w_agg, server_data, gamma, E)\n",
    "        local_loss.append(round_loss)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(update_server_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)   # 计算所有客户端和服务器一起的平均损失\n",
    "\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "\n",
    "        # 按照server训练的方向，进行mutation\n",
    "        w_delta = FedSub(update_server_w, w_agg, 1.0)\n",
    "        # 计算模型更新w_delta的L2范数（平方和），衡量模型更新程度的大小\n",
    "        rank = delta_rank(w_delta)\n",
    "        # print(rank)\n",
    "        if rank > max_rank:\n",
    "            max_rank = rank\n",
    "        alpha = radius  # 论文中的alpha，衡量Mutation的幅度\n",
    "        # alpha = min(max(args.radius, max_rank/rank),(10.0-args.radius) * (1 - iter/args.epochs) + args.radius)\n",
    "        w_locals = mutation_spread(\n",
    "            round, update_server_w, M, w_delta, alpha\n",
    "        )\n",
    "\n",
    "    return test_acc, train_loss\n",
    "\n",
    "\n",
    "def mutation_spread(iter, w_glob, m, w_delta, alpha):\n",
    "\n",
    "    w_locals_new = []\n",
    "    ctrl_cmd_list = []\n",
    "    ctrl_rate = mut_acc_rate * (\n",
    "        1.0 - min(iter * 1.0 / mut_bound, 1.0)\n",
    "    )  # 论文中的βt，随着iter逐渐从β0减小到0\n",
    "\n",
    "    # k代表模型中的参数数量，对每个参数按照client数量分配v（论文中是按照每一层分配）\n",
    "    for k in w_glob.keys():\n",
    "        ctrl_list = []\n",
    "        for i in range(0, int(m / 2)):\n",
    "            ctrl = random.random()  # 随机数，范围：[0,1)\n",
    "            # 这里分ctrl感觉没什么必要，shuffle后都会随机掉\n",
    "            if ctrl > 0.5:\n",
    "                ctrl_list.append(1.0)\n",
    "                ctrl_list.append(1.0 * (-1.0 + ctrl_rate))\n",
    "            else:\n",
    "                ctrl_list.append(1.0 * (-1.0 + ctrl_rate))\n",
    "                ctrl_list.append(1.0)\n",
    "        random.shuffle(ctrl_list)  # 打乱列表\n",
    "        ctrl_cmd_list.append(ctrl_list)\n",
    "    cnt = 0\n",
    "    for j in range(m):\n",
    "        w_sub = copy.deepcopy(w_glob)\n",
    "        if not (cnt == m - 1 and m % 2 == 1):\n",
    "            ind = 0\n",
    "            for k in w_sub.keys():\n",
    "                w_sub[k] = w_sub[k] + w_delta[k] * ctrl_cmd_list[ind][j] * alpha\n",
    "                ind += 1\n",
    "        cnt += 1\n",
    "        w_locals_new.append(w_sub)\n",
    "\n",
    "    return w_locals_new\n",
    "\n",
    "\n",
    "\n",
    "# 加权平均聚合，lens代表了权重，如果没有定义就是普通平均（FedMut就每定义）\n",
    "def Aggregation(w, lens):\n",
    "    w_avg = None\n",
    "    if lens == None:\n",
    "        total_count = len(w)\n",
    "        lens = []\n",
    "        for i in range(len(w)):\n",
    "            lens.append(1.0)\n",
    "    else:\n",
    "        total_count = sum(lens)\n",
    "\n",
    "    for i in range(0, len(w)):\n",
    "        if i == 0:\n",
    "            w_avg = copy.deepcopy(w[0])\n",
    "            for k in w_avg.keys():\n",
    "                w_avg[k] = w[i][k] * lens[i]\n",
    "        else:\n",
    "            for k in w_avg.keys():\n",
    "                w_avg[k] += w[i][k] * lens[i]\n",
    "\n",
    "    for k in w_avg.keys():\n",
    "        w_avg[k] = torch.div(w_avg[k], total_count)\n",
    "\n",
    "    return w_avg\n",
    "\n",
    "\n",
    "\n",
    "def FedSub(w, w_old, weight):\n",
    "    w_sub = copy.deepcopy(w)\n",
    "    for k in w_sub.keys():\n",
    "        w_sub[k] = (w[k] - w_old[k]) * weight\n",
    "\n",
    "    return w_sub\n",
    "\n",
    "def delta_rank(delta_dict):\n",
    "    cnt = 0\n",
    "    dict_a = torch.Tensor(0)\n",
    "    s = 0\n",
    "    for p in delta_dict.keys():\n",
    "        a = delta_dict[p]\n",
    "        a = a.view(-1)\n",
    "        if cnt == 0:\n",
    "            dict_a = a\n",
    "        else:\n",
    "            dict_a = torch.cat((dict_a, a), dim=0)\n",
    "\n",
    "        cnt += 1\n",
    "        # print(sim)\n",
    "    s = torch.norm(dict_a, dim=0)\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置训练参数 -- CIFAR100\n",
    "\n",
    "# 随机性\n",
    "data_random_fix = False  # 是否固定数据采样的随机性\n",
    "seed_num = 42\n",
    "\n",
    "gpu = 1  # 默认使用gpu 1 (第二个)\n",
    "verbose = False  # 调试模式，输出一些中间信息\n",
    "\n",
    "client_num = 100\n",
    "non_iid = 1.0  # Dirichlet 分布参数，数值越小数据越不均匀可根据需要调整\n",
    "size_per_client = 400  # 每个客户端的数据量（训练）\n",
    "server_percentage = 0.01  # 服务器端用于微调的数据比例\n",
    "\n",
    "\n",
    "# 模型相关\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0001  # 模型权重衰减参数，强制参数向0靠拢（和学习率衰减不一样！）这个是给我的原始代码中就是这样\n",
    "bc_size = 128\n",
    "num_classes = 20  # 分别数量，CIFAR100中是20（FedMut和CLGG都是这么采用的）\n",
    "\n",
    "# 联邦训练的超参数\n",
    "global_round = 100  # 全局训练轮数，可根据需要调整\n",
    "eta = 0.1  # 客户端端学习率，从{0.01, 0.1, 1}中调优\n",
    "gamma = 0.05  # 服务器端学习率 从{0.005， 0.05， 0.5中调有}\n",
    "K = 5  # 客户端本地训练轮数，从1，3，5中选\n",
    "E = 5  # 服务器本地训练轮数，从1，3，5中选\n",
    "M = 10  # 每一轮抽取客户端\n",
    "\n",
    "# FedMut中参数\n",
    "radius = 1  # alpha，控制mutation的幅度\n",
    "mut_acc_rate = 0.3  # 论文中的β0\n",
    "mut_bound = 50  # Tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Traning Client Total: 38984 2125 2295 2060 2198 2031 1924 1882 1957 1543 1736 1802 1926 2380 1900 1580 1811 1807 1968 1949 2110\n",
      "Client 0: 388 10 0 8 27 39 18 5 17 23 38 21 31 34 13 25 12 5 32 27 3\n",
      "Client 1: 389 36 5 8 14 3 8 32 29 7 23 23 46 12 37 5 4 4 11 17 65\n",
      "Client 2: 388 7 66 91 15 27 5 8 4 18 2 3 3 25 32 15 15 20 2 17 13\n",
      "Client 3: 392 3 57 18 13 1 15 4 0 4 6 64 23 4 9 34 12 2 43 52 28\n",
      "Client 4: 390 9 37 7 16 63 30 36 3 8 9 4 14 13 0 7 2 37 5 12 78\n",
      "Client 5: 391 12 18 2 24 14 9 13 23 64 21 10 3 57 17 14 11 3 0 10 66\n",
      "Client 6: 388 11 83 11 9 33 23 22 15 9 0 44 16 31 23 17 16 5 2 8 10\n",
      "Client 7: 391 34 14 24 32 2 23 5 37 1 27 35 17 1 6 6 47 0 27 32 21\n",
      "Client 8: 390 7 46 28 1 22 23 6 121 3 47 29 7 16 5 0 9 4 7 5 4\n",
      "Client 9: 391 1 0 20 2 90 54 13 67 4 44 1 4 10 9 6 12 11 7 6 30\n",
      "Server: 381 21 22 20 21 20 19 18 19 15 17 18 19 23 19 15 18 18 19 19 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:54<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLG-Mut 训练完成！\n",
      "各轮平均测试精度: [5.0, 5.0, 8.170000076293945, 7.860000133514404, 5.25, 5.0, 5.0, 5.0, 5.0, 5.409999847412109, 6.889999866485596, 5.0, 6.0, 5.0, 5.0, 5.019999980926514, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]\n",
      "各轮平均训练损失: [0.14807310909545335, 0.12808904112926103, 0.14995896833310976, 0.12555251702311168, 0.11756698692880603, 0.12219834175961038, 0.12784419863513666, 0.11690884630686214, 0.12576225893989657, 0.13625988774359613, 0.12149970174878834, 0.12668237003079613, 0.1215928284632695, 0.13018853477045333, 0.13574195371515307, 0.13853057234005042, 0.13997352341851504, 0.1264681775710638, 0.12892465496866345, 0.12764919233026054, 0.11687403303787307, 0.13300874703013663, 0.11971089089038696, 0.12076275534177193, 0.1211779228428774, 0.15107058102728485, 0.12827429909537105, 0.13251560688934802, 0.14188782931555916, 0.13235543780965786, 0.12730293602006776, 0.12415448525637646, 0.1396457811099433, 0.12741646642694293, 0.13515322824253084, 0.13168065930289818, 0.1366297198028856, 0.12156730739649436, 0.135122245232261, 0.1224287386457061, 0.12183791675803399, 0.1409871023863199, 0.13479990238555065, 0.12224875861522116, 0.15165561901912158, 0.13377488017344222, 0.12093664265834603, 0.11999435686129126, 0.1281925265351125, 0.12460765481702821, 0.13019614429623855, 0.12680869007040926, 0.12101828705503635, 0.1246229857753172, 0.12515924420684876, 0.1134504114021535, 0.11817434033748263, 0.13534069436282825, 0.13919999874866693, 0.12203339677331768, 0.137908633632358, 0.11608573018776261, 0.12362043742522016, 0.13319109024532236, 0.12754163992447834, 0.12201885093281213, 0.12151581547129443, 0.12054375032930599, 0.13705371457380586, 0.13578519591610735, 0.11761055408181113, 0.13179290530720314, 0.1382644034559173, 0.13731175125353523, 0.12024058398144134, 0.11992798223853818, 0.11594666984622805, 0.13225913268749268, 0.1406050901557976, 0.14654253919656143, 0.12305185881415742, 0.13146106704307708, 0.11628513236065188, 0.12138477716419475, 0.1253874682843427, 0.11580510442596235, 0.14567604562044142, 0.12393877939365126, 0.14326582221994863, 0.115872525503538, 0.1196661012040717, 0.12274491106326897, 0.12090349956102252, 0.14009501429565174, 0.11640280470461363, 0.13275657502613414, 0.14311713366665882, 0.13086094876637125, 0.12329886108666206, 0.12210975336842723]\n",
      "最终测试精度: 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:03<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server only 训练完成！\n",
      "各轮平均测试精度: [5.0, 5.769999980926514, 5.420000076293945, 7.059999942779541, 9.319999694824219, 6.130000114440918, 9.550000190734863, 9.550000190734863, 11.140000343322754, 11.350000381469727, 9.0, 11.5, 13.359999656677246, 13.140000343322754, 13.239999771118164, 15.069999694824219, 12.699999809265137, 15.100000381469727, 14.050000190734863, 14.270000457763672, 15.220000267028809, 14.270000457763672, 13.34000015258789, 13.5600004196167, 13.1899995803833, 14.75, 14.380000114440918, 13.489999771118164, 14.100000381469727, 14.1899995803833, 14.260000228881836, 14.270000457763672, 14.210000038146973, 14.239999771118164, 14.180000305175781, 14.220000267028809, 14.1899995803833, 14.180000305175781, 14.149999618530273, 14.210000038146973, 14.239999771118164, 14.25, 14.199999809265137, 14.229999542236328, 14.180000305175781, 14.210000038146973, 14.199999809265137, 14.1899995803833, 14.260000228881836, 14.210000038146973, 14.180000305175781, 14.199999809265137, 14.210000038146973, 14.210000038146973, 14.220000267028809, 14.229999542236328, 14.199999809265137, 14.1899995803833, 14.199999809265137, 14.180000305175781, 14.170000076293945, 14.149999618530273, 14.170000076293945, 14.149999618530273, 14.149999618530273, 14.130000114440918, 14.140000343322754, 14.15999984741211, 14.140000343322754, 14.130000114440918, 14.130000114440918, 14.140000343322754, 14.149999618530273, 14.15999984741211, 14.149999618530273, 14.1899995803833, 14.170000076293945, 14.180000305175781, 14.170000076293945, 14.1899995803833, 14.180000305175781, 14.180000305175781, 14.180000305175781, 14.199999809265137, 14.199999809265137, 14.1899995803833, 14.1899995803833, 14.199999809265137, 14.180000305175781, 14.170000076293945, 14.149999618530273, 14.140000343322754, 14.130000114440918, 14.140000343322754, 14.149999618530273, 14.149999618530273, 14.140000343322754, 14.119999885559082, 14.140000343322754, 14.130000114440918]\n",
      "各轮平均训练损失: [0.023606049492955205, 0.0235735327253739, 0.023549997459848725, 0.023527578833699228, 0.023491861633459725, 0.023398176964124044, 0.02303213758369287, 0.022540799125035604, 0.0220695926596721, 0.021697409664591154, 0.02152917844057083, 0.02102992843588193, 0.019677063769102096, 0.019565138081709545, 0.019443937747677168, 0.018147299506266912, 0.016917522401114306, 0.014191954078773656, 0.013017838136355084, 0.010319251839816568, 0.0073893111767868195, 0.005037697225809097, 0.0036833850068350633, 0.0036299064623812835, 0.004754891978949309, 0.002590567013186713, 0.0005392083496126967, 0.00023753179872292095, 5.11535149513899e-05, 2.1098881626191238e-05, 1.5764541692866866e-05, 1.2624088856197583e-05, 1.054390526211743e-05, 9.182736160073545e-06, 8.064180866495008e-06, 7.224633460767413e-06, 6.517572757972326e-06, 5.895846455678111e-06, 5.386444648562853e-06, 4.981532377375212e-06, 4.6664851181655345e-06, 4.3802713418699565e-06, 4.108389127213741e-06, 3.881667877794825e-06, 3.686494767559149e-06, 3.527873318186418e-06, 3.382716362102656e-06, 3.2340571338863814e-06, 3.1171683320280862e-06, 3.006069541212734e-06, 2.902654249434515e-06, 2.801495780659025e-06, 2.7172373981860194e-06, 2.6299362730545305e-06, 2.5439096181798957e-06, 2.4887433198576523e-06, 2.4216107231647283e-06, 2.3529867913869867e-06, 2.2978324441889224e-06, 2.2442600486101582e-06, 2.190538579695082e-06, 2.141146779710349e-06, 2.1012295093896683e-06, 2.053426560996741e-06, 2.0171059367688332e-06, 1.9805421233589492e-06, 1.9473256944669026e-06, 1.9165952909437083e-06, 1.8798140315387474e-06, 1.8548205303886788e-06, 1.8243194453437657e-06, 1.8021831906177493e-06, 1.7759235982642468e-06, 1.7511724773309348e-06, 1.7225705515253743e-06, 1.7043487119735802e-06, 1.6798609187996289e-06, 1.6627358620705007e-06, 1.6392074458053688e-06, 1.6256849914498162e-06, 1.6067965233257079e-06, 1.587832781721469e-06, 1.5720324005693937e-06, 1.554835061324411e-06, 1.5436529881602231e-06, 1.5260591969005568e-06, 1.5151343767380847e-06, 1.5053908298189827e-06, 1.487623431906589e-06, 1.474320412307861e-06, 1.463047310365558e-06, 1.453849479245643e-06, 1.4461343373113777e-06, 1.4331502631951782e-06, 1.4251127964598707e-06, 1.4128550717941834e-06, 1.4069695964280981e-06, 1.39727807224214e-06, 1.3861291433083049e-06, 1.3802531313558575e-06]\n",
      "最终测试精度: 14.130000114440918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:48<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fedavg训练完成！\n",
      "各轮平均测试精度: [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.690000057220459, 5.0, 5.440000057220459, 5.0, 7.78000020980835, 5.0, 5.0, 5.0, 4.989999771118164, 3.6700000762939453, 5.0, 5.28000020980835, 5.019999980926514, 5.0, 5.0, 5.690000057220459, 6.28000020980835, 5.730000019073486, 7.099999904632568, 7.230000019073486, 7.730000019073486, 9.109999656677246, 6.480000019073486, 7.159999847412109, 9.369999885559082, 8.779999732971191, 7.440000057220459, 11.050000190734863, 9.130000114440918, 11.529999732971191, 8.800000190734863, 14.979999542236328, 7.019999980926514, 11.640000343322754, 11.529999732971191, 14.029999732971191, 13.369999885559082, 10.149999618530273, 13.130000114440918, 12.949999809265137, 7.880000114440918, 11.920000076293945, 13.0600004196167, 12.850000381469727, 15.260000228881836, 13.8100004196167, 14.880000114440918, 14.550000190734863, 13.0, 14.0, 16.350000381469727, 16.09000015258789, 14.220000267028809, 14.569999694824219, 14.670000076293945, 14.539999961853027, 17.079999923706055, 17.059999465942383, 17.010000228881836, 15.149999618530273, 15.710000038146973, 15.770000457763672, 14.600000381469727, 17.610000610351562, 18.389999389648438, 18.790000915527344, 19.979999542236328, 17.809999465942383, 18.1299991607666, 18.3700008392334, 17.219999313354492, 17.540000915527344, 17.020000457763672, 16.389999389648438, 18.360000610351562, 18.239999771118164, 18.739999771118164, 19.09000015258789, 18.799999237060547, 17.639999389648438, 19.309999465942383, 17.8700008392334, 14.100000381469727, 19.43000030517578, 17.440000534057617, 17.639999389648438, 16.920000076293945, 20.209999084472656, 20.18000030517578, 19.450000762939453, 18.110000610351562, 19.030000686645508, 18.719999313354492, 18.979999542236328]\n",
      "各轮平均训练损失: [0.15400110807580253, 0.1532610591322716, 0.13451628968376844, 0.13904731928406372, 0.12513683933134942, 0.1424252091904304, 0.1373927179859685, 0.14024736762268558, 0.13269164074862763, 0.15114270326805612, 0.15855307502281807, 0.12770510486335981, 0.1376172482923028, 0.14932482620993898, 0.14310717027456984, 0.13161797620843918, 0.13445717746702332, 0.14825176723283667, 0.13606671319458458, 0.13089480526603403, 0.13403444807577344, 0.1477603464014828, 0.13770471434933798, 0.13430953776180038, 0.13732507140709768, 0.12318877887184776, 0.13425579456417333, 0.13225917326446093, 0.1252706863320477, 0.14027564699273737, 0.1248636867746356, 0.11529784357589153, 0.1351212076562501, 0.1192822349230271, 0.12291964034780505, 0.13420093028628755, 0.12894408303330696, 0.1301544195733787, 0.137528065417228, 0.12549723867424542, 0.1473918013142954, 0.11748983966958311, 0.13112171295977065, 0.12166002429028351, 0.11771454108719315, 0.13461609026124433, 0.12790984348331888, 0.12787022153101862, 0.10946363373655117, 0.13913982866651245, 0.12586972571102278, 0.11640385459265894, 0.12371031511251238, 0.11419884647819259, 0.13266226866459915, 0.13657463786884078, 0.11077318417343, 0.09795742442634785, 0.11163977803915208, 0.11981493147338429, 0.10527586996444457, 0.12729862598325345, 0.13794781662266525, 0.11673414704261258, 0.11422015616146938, 0.10169364556883062, 0.13949954091912758, 0.12133193362759986, 0.10886236515481555, 0.11078113710694015, 0.1176746390059679, 0.11788545870887382, 0.10727577429468789, 0.11947458419248107, 0.11295204660673405, 0.11289132946951405, 0.1140172760779156, 0.10415964924047391, 0.11739168993436863, 0.12269857527386574, 0.11021501191669987, 0.11059802612884058, 0.10120419501220541, 0.10655863846958216, 0.10980313557905279, 0.10798875281228018, 0.10438846316410318, 0.11496211194424402, 0.12814841050734477, 0.11666173986771275, 0.0971068842168454, 0.12014443773962558, 0.1217568830955064, 0.12778176189981222, 0.09189678979370862, 0.0946674089594079, 0.11938854566668824, 0.10600741241542652, 0.12763618212379518, 0.10771249537688814]\n",
      "最终测试精度: 18.979999542236328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:53<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLG_SGD 训练完成！\n",
      "各轮平均测试精度: [5.0, 5.0, 5.0, 5.0, 5.059999942779541, 6.429999828338623, 7.059999942779541, 9.260000228881836, 8.09000015258789, 9.319999694824219, 8.739999771118164, 9.449999809265137, 9.880000114440918, 10.3100004196167, 10.5, 9.289999961853027, 11.5, 11.630000114440918, 10.539999961853027, 12.8100004196167, 13.470000267028809, 12.880000114440918, 14.670000076293945, 14.25, 15.710000038146973, 15.239999771118164, 16.479999542236328, 16.540000915527344, 16.260000228881836, 14.109999656677246, 16.40999984741211, 14.989999771118164, 15.8100004196167, 15.960000038146973, 16.65999984741211, 15.979999542236328, 16.420000076293945, 16.6200008392334, 15.6899995803833, 16.6200008392334, 17.18000030517578, 17.18000030517578, 16.959999084472656, 16.31999969482422, 16.6200008392334, 16.06999969482422, 16.520000457763672, 16.6200008392334, 15.880000114440918, 16.850000381469727, 16.90999984741211, 16.479999542236328, 16.260000228881836, 16.670000076293945, 16.43000030517578, 16.670000076293945, 15.789999961853027, 16.40999984741211, 16.8700008392334, 16.190000534057617, 16.15999984741211, 17.020000457763672, 16.969999313354492, 16.309999465942383, 16.579999923706055, 16.850000381469727, 16.690000534057617, 17.56999969482422, 17.040000915527344, 16.709999084472656, 16.809999465942383, 16.829999923706055, 15.710000038146973, 16.079999923706055, 15.369999885559082, 16.450000762939453, 15.90999984741211, 15.699999809265137, 15.850000381469727, 15.829999923706055, 13.960000038146973, 16.170000076293945, 15.800000190734863, 16.010000228881836, 15.920000076293945, 16.56999969482422, 15.40999984741211, 15.779999732971191, 15.869999885559082, 15.600000381469727, 15.420000076293945, 16.260000228881836, 15.640000343322754, 15.970000267028809, 16.540000915527344, 15.649999618530273, 15.850000381469727, 15.0600004196167, 16.1200008392334, 15.539999961853027]\n",
      "各轮平均训练损失: [0.12205188513893907, 0.12862634915268267, 0.1416535084194435, 0.12893802724326459, 0.1234893334484881, 0.13639040556060414, 0.11637278268438206, 0.12465023333722831, 0.12570475023482075, 0.1410939729991413, 0.12898242201884783, 0.1183573875279686, 0.12435231645298533, 0.13035282888275077, 0.12257982722039928, 0.13026459417121233, 0.11154907280937869, 0.11324496907890809, 0.11945212760243516, 0.12462234380031045, 0.10831664240101908, 0.12162979587221363, 0.11741672821657255, 0.11761078539427805, 0.11130266358749498, 0.11034992734974337, 0.12077357882889574, 0.09606086641704237, 0.10740184536210191, 0.1132984545123461, 0.1109792905729651, 0.11338857605338615, 0.12304624172990165, 0.12007223041151929, 0.1181464793787729, 0.10994066858557877, 0.11983323456954807, 0.11267881466004014, 0.12488177672173038, 0.10894617206473146, 0.11267837998274376, 0.10903606443825414, 0.11163341670092762, 0.1264749305062883, 0.11338462074910699, 0.10533612914477591, 0.09857835951016097, 0.10437233305283988, 0.10977795352070911, 0.10718525028039592, 0.09636179131871629, 0.10992209188341934, 0.10636955280224612, 0.10995797401414578, 0.10534738010961836, 0.10784801557862171, 0.09810747743450525, 0.0962786734939702, 0.10870036075966696, 0.10044353674191225, 0.11094631787916164, 0.0990470401113861, 0.09626600198888019, 0.10313465796498282, 0.12115745342866684, 0.11209341529037366, 0.09115180975459357, 0.10045867821020903, 0.11836285018786294, 0.10658493732199567, 0.1121061010414334, 0.1188441813080471, 0.10488828387304784, 0.11346984867977855, 0.12591734485203054, 0.10294050636437291, 0.10807916919778048, 0.12054036463304742, 0.10447223101252957, 0.10556999693579358, 0.11949289575282213, 0.09619221915587364, 0.10958430937402014, 0.109001277972324, 0.11185434342123632, 0.1043042296867979, 0.11611470774720983, 0.09735141965254206, 0.10886588004540604, 0.12871757582380752, 0.09598253293398976, 0.122239619758873, 0.1245676022786792, 0.09532768280475654, 0.1137636802229724, 0.10602637781754008, 0.10031907665856926, 0.10291267756297055, 0.12407736884381348, 0.11849118431075212]\n",
      "最终测试精度: 15.539999961853027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 准备数据集\n",
    "# 这部分是我加的\n",
    "\n",
    "cifar, test_dataset = CIFAR100()\n",
    "prob = get_prob(non_iid, client_num, class_num=20)\n",
    "# client_data, client_test_data = create_data(prob, size_per_client, cifar, N=20)\n",
    "client_data = create_data_all_train(prob, size_per_client, cifar, N=20)   # 这里改为全部构建训练集\n",
    "\n",
    "# 将测试标签转换为粗类别\n",
    "test_dataset.targets = sparse2coarse(test_dataset.targets)\n",
    "\n",
    "# 如果需要确保测试标签为整数类型\n",
    "test_dataset.targets = test_dataset.targets.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "all_images = []\n",
    "all_labels = []\n",
    "for data in client_data:\n",
    "    all_images.extend(data[0])\n",
    "    all_labels.extend(data[1])\n",
    "comb_client_data = [np.array(all_images), np.array(all_labels)]\n",
    "\n",
    "# 输出comb_client_data情况\n",
    "imgs, lbls = comb_client_data\n",
    "lbls = np.array(lbls)\n",
    "total_count = len(lbls)\n",
    "unique_classes, counts = np.unique(lbls, return_counts=True)\n",
    "\n",
    "# 创建一个长度为20的数组记录各类别计数，默认0\n",
    "class_counts = [0]*20\n",
    "for cls, cnt in zip(unique_classes, counts):\n",
    "    class_counts[cls] = cnt\n",
    "\n",
    "# 打印格式：Total: 总数 类别0计数 类别1计数 ... 类别19计数\n",
    "print(\"Traning Client Total: {}\".format(\" \".join([str(total_count)] + [str(c) for c in class_counts])))\n",
    "\n",
    "\n",
    "# 打印每个客户端训练数据情况（只输出前10个）\n",
    "for i, (imgs, lbls) in enumerate(client_data[:10]):\n",
    "    lbls = np.array(lbls)\n",
    "    total_count = len(lbls)\n",
    "    unique_classes, counts = np.unique(lbls, return_counts=True)\n",
    "    # 创建一个长度为20的数组记录各类别计数，默认0\n",
    "    class_counts = [0]*20\n",
    "    for cls, cnt in zip(unique_classes, counts):\n",
    "        class_counts[cls] = cnt\n",
    "    # 打印格式：Client i: 总数 类别0计数 类别1计数 ... 类别19计数\n",
    "    print(\"Client {}: {}\".format(i, \" \".join([str(total_count)] + [str(c) for c in class_counts])))\n",
    "    \n",
    "\n",
    "# 提前生成固定的服务器数据\n",
    "# Modify: 这是我后来修改的\n",
    "server_data = select_subset(comb_client_data, server_percentage)\n",
    "\n",
    "s_imgs, s_lbls = server_data\n",
    "s_lbls = np.array(s_lbls)\n",
    "total_count = len(s_lbls)\n",
    "unique_classes, counts = np.unique(s_lbls, return_counts=True)\n",
    "class_counts = [0]*20\n",
    "for cls, cnt in zip(unique_classes, counts):\n",
    "    class_counts[cls] = cnt\n",
    "\n",
    "# 输出格式: Server: 总数 类别0计数 类别1计数 ... 类别19计数\n",
    "print(\"Server: {}\".format(\" \".join([str(total_count)] + [str(c) for c in class_counts])))\n",
    "# print(\"  前5个标签: \", lbls[:5])\n",
    "# print(\"  前5个数据形状: \", [server_data[0][j].shape for j in range(min(5, len(server_data[0])))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 用了FedMut中定义的CNN网络\n",
    "\n",
    "init_model = cnncifar().to(device)\n",
    "initial_w = copy.deepcopy(init_model.state_dict())\n",
    "\n",
    "\n",
    "# CLG_Mut训练，这里不同在于直接传初始化后的模型\n",
    "test_acc, train_loss = CLG_Mut(init_model, global_round, eta, gamma, K, E, M)\n",
    "\n",
    "# 打印训练过程中的结果\n",
    "print(\"CLG-Mut 训练完成！\")\n",
    "print(\"各轮平均测试精度:\", test_acc)\n",
    "print(\"各轮平均训练损失:\", train_loss)\n",
    "print(\"最终测试精度:\", test_acc[-1] if len(test_acc) > 0 else \"无数据\")\n",
    "\n",
    "\n",
    "# Server-only训练\n",
    "test_acc, train_loss = server_only(initial_w, global_round, gamma, E)\n",
    "\n",
    "# 打印训练过程中的结果\n",
    "print(\"Server only 训练完成！\")\n",
    "print(\"各轮平均测试精度:\", test_acc)\n",
    "print(\"各轮平均训练损失:\", train_loss)\n",
    "print(\"最终测试精度:\", test_acc[-1] if len(test_acc) > 0 else \"无数据\")\n",
    "\n",
    "\n",
    "# fedavg训练\n",
    "test_acc, train_loss = fedavg(initial_w, global_round, eta, K, M)\n",
    "\n",
    "# 打印训练过程中的结果\n",
    "print(\"fedavg训练完成！\")\n",
    "print(\"各轮平均测试精度:\", test_acc)\n",
    "print(\"各轮平均训练损失:\", train_loss)\n",
    "print(\"最终测试精度:\", test_acc[-1] if len(test_acc) > 0 else \"无数据\")\n",
    "\n",
    "\n",
    "\n",
    "# # hybridfl训练\n",
    "# test_acc, train_loss = hybridFL(initial_w, global_round, eta, K, M)\n",
    "\n",
    "# # 打印训练过程中的结果\n",
    "# print(\"hrbridFL训练完成！\")\n",
    "# print(\"各轮平均测试精度:\", test_acc)\n",
    "# print(\"各轮平均训练损失:\", train_loss)\n",
    "# print(\"最终测试精度:\", test_acc[-1] if len(test_acc) > 0 else \"无数据\")\n",
    "\n",
    "# CLG_SGD训练\n",
    "test_acc, train_loss = CLG_SGD(initial_w, global_round, eta, gamma, K, E, M)\n",
    "\n",
    "# 打印训练过程中的结果\n",
    "print(\"CLG_SGD 训练完成！\")\n",
    "print(\"各轮平均测试精度:\", test_acc)\n",
    "print(\"各轮平均训练损失:\", train_loss)\n",
    "print(\"最终测试精度:\", test_acc[-1] if len(test_acc) > 0 else \"无数据\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
