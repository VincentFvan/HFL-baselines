{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "id": "gSK1TSekTVeu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.functional as func\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "id": "9-WEXWakTwf8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # 解决由于多次加载 OpenMP 相关动态库而引起的冲突"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置训练参数 -- CIFAR100\n",
    "\n",
    "# 随机性\n",
    "data_random_fix = False  # 是否固定数据采样的随机性\n",
    "seed_num = 42\n",
    "\n",
    "gpu = 1  # 默认使用gpu 1 (第二个)\n",
    "verbose = False  # 调试模式，输出一些中间信息\n",
    "\n",
    "client_num = 200\n",
    "non_iid = 0.5  # Dirichlet 分布参数，数值越小数据越不均匀可根据需要调整\n",
    "size_per_client = 200  # 每个客户端的数据量（训练）\n",
    "server_percentage = 0.01  # 服务器端用于微调的数据比例\n",
    "\n",
    "\n",
    "# 模型相关\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0001  # 模型权重衰减参数，强制参数向0靠拢（和学习率衰减不一样！）这个是给我的原始代码中就是这样\n",
    "bc_size = 128\n",
    "num_classes = 20  # 分别数量，CIFAR100中是20（FedMut和CLGG都是这么采用的）\n",
    "\n",
    "# 联邦训练的超参数\n",
    "global_round = 100  # 全局训练轮数，可根据需要调整\n",
    "eta = 0.1  # 客户端端学习率，从{0.01, 0.1, 1}中调优\n",
    "gamma = 0.05  # 服务器端学习率 从{0.005， 0.05， 0.5中调有}\n",
    "K = 5  # 客户端本地训练轮数，从1，3，5中选\n",
    "E = 5  # 服务器本地训练轮数，从1，3，5中选\n",
    "M = 10  # 每一轮抽取客户端\n",
    "\n",
    "# FedMut中参数\n",
    "radius = 4.0  # alpha，控制mutation的幅度\n",
    "mut_acc_rate = 0.3  # 论文中的β0\n",
    "mut_bound = 50  # Tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sns3blEITybc",
    "outputId": "120095fb-5597-4ada-8c12-b4470d8ad28e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "Fri Jan  3 11:09:35 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:21:00.0 Off |                  Off |\n",
      "| 30%   31C    P2              42W / 350W |    555MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off | 00000000:E1:00.0 Off |                  Off |\n",
      "| 31%   29C    P8              16W / 350W |     14MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      4802      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A    175434      C   /home/anaconda/envs/env8/bin/python         538MiB |\n",
      "|    1   N/A  N/A      4802      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "if torch.cuda.is_available() and gpu >= 0 and gpu < torch.cuda.device_count():\n",
    "    device = torch.device(f'cuda:{gpu}')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "id": "96gqMCQneWho"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LinearBottleNeck(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride, t=6, class_num=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * t, 1),\n",
    "            nn.BatchNorm2d(in_channels * t),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels * t, in_channels * t, 3, stride=stride, padding=1, groups=in_channels * t),\n",
    "            nn.BatchNorm2d(in_channels * t),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels * t, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.stride = stride\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = self.residual(x)\n",
    "\n",
    "        if self.stride == 1 and self.in_channels == self.out_channels:\n",
    "            residual += x\n",
    "\n",
    "        return residual\n",
    "\n",
    "# MobileNetV2（比lenet更复杂的CNN网络）网络中的线性瓶颈结构，原文中用于CIFAR-100任务\n",
    "class MobileNetV2(nn.Module):\n",
    "\n",
    "    def __init__(self, class_num=20):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pre = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.stage1 = LinearBottleNeck(32, 16, 1, 1)\n",
    "        self.stage2 = self._make_stage(2, 16, 24, 2, 6)\n",
    "        self.stage3 = self._make_stage(3, 24, 32, 2, 6)\n",
    "        self.stage4 = self._make_stage(4, 32, 64, 2, 6)\n",
    "        self.stage5 = self._make_stage(3, 64, 96, 1, 6)\n",
    "        self.stage6 = self._make_stage(3, 96, 160, 1, 6)\n",
    "        self.stage7 = LinearBottleNeck(160, 320, 1, 6)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(320, 1280, 1),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(1280, class_num, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "        x = self.stage6(x)\n",
    "        x = self.stage7(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _make_stage(self, repeat, in_channels, out_channels, stride, t):\n",
    "\n",
    "        layers = []\n",
    "        layers.append(LinearBottleNeck(in_channels, out_channels, stride, t))\n",
    "\n",
    "        while repeat - 1:\n",
    "            layers.append(LinearBottleNeck(out_channels, out_channels, 1, t))\n",
    "            repeat -= 1\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "def mobilenetv2():\n",
    "    return MobileNetV2()\n",
    "\n",
    "\n",
    "# FedMut中采用的cnn模型\n",
    "class CNNCifar(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNCifar, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x, start_layer_idx=0, logit=False):\n",
    "        if start_layer_idx < 0:  #\n",
    "            return self.mapping(x, start_layer_idx=start_layer_idx, logit=logit)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        result = {'activation' : x}\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        result['hint'] = x\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        result['representation'] = x\n",
    "        x = self.fc3(x)\n",
    "        result['output'] = x\n",
    "        return result\n",
    "\n",
    "    def mapping(self, z_input, start_layer_idx=-1, logit=True):\n",
    "        z = z_input\n",
    "        z = self.fc3(z)\n",
    "\n",
    "        result = {'output': z}\n",
    "        if logit:\n",
    "            result['logit'] = z\n",
    "        return result\n",
    "    \n",
    "def cnncifar():\n",
    "    return CNNCifar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "id": "QkvtGtMuUDmr"
   },
   "outputs": [],
   "source": [
    "# def test_inference(model, test):\n",
    "#     \"\"\" Returns the test accuracy and loss.\n",
    "#     \"\"\"\n",
    "#     tensor_x = torch.Tensor(test[0]).to(device)\n",
    "#     tensor_y = torch.Tensor(test[1]).to(device)\n",
    "#     test_dataset = TensorDataset(tensor_x, tensor_y)\n",
    "\n",
    "#     model.eval()\n",
    "#     loss, total, correct = 0.0, 0.0, 0.0\n",
    "\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     testloader = DataLoader(test_dataset, batch_size=bc_size,\n",
    "#                             shuffle=True)\n",
    "\n",
    "#     for batch_idx, (images, labels) in enumerate(testloader):\n",
    "#         with torch.no_grad():  # 在测试过程中不需要计算梯度，节省内存和加速计算\n",
    "#         # Inference\n",
    "#             outputs = model(images)\n",
    "#             batch_loss = criterion(outputs, labels.long())\n",
    "#             loss += batch_loss.item() * labels.size(0) # 计算损失值，更好反映模型输出概率分布与真实标签的差距\n",
    "\n",
    "#         # Prediction\n",
    "#             _, pred_labels = torch.max(outputs, 1)\n",
    "#             pred_labels = pred_labels.view(-1)\n",
    "#             correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "#             total += len(labels)\n",
    "#     #print(correct,\"/\",total)\n",
    "#     accuracy = correct/total\n",
    "    \n",
    "#     print(\"Testing accuracy: {:.2f}\", accuracy)\n",
    "#     return accuracy, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新的测试：针对整个测试数据集的测试\n",
    "def test_inference(net_glob, dataset_test):\n",
    "    # testing\n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test)\n",
    "\n",
    "    print(\"Testing accuracy: {:.2f}\".format(acc_test))\n",
    "\n",
    "    return acc_test.item()\n",
    "\n",
    "def test_img(net_g, datatest):\n",
    "    net_g.eval()\n",
    "    # testing\n",
    "    # test loss代表在测试集上的平均损失（对测试数据的预测输出与真实标签的差距）\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    data_loader = DataLoader(datatest, batch_size=bc_size)\n",
    "    l = len(data_loader)\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, target) in enumerate(data_loader):\n",
    "            if gpu != -1:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            log_probs = net_g(data)['output']\n",
    "            # sum up batch loss\n",
    "            test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n",
    "            # get the index of the max log-probability\n",
    "            y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "            correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.00 * correct / len(data_loader.dataset)\n",
    "    if verbose:\n",
    "        print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(data_loader.dataset), accuracy))\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "id": "jURskA9VUJOF"
   },
   "outputs": [],
   "source": [
    "# 将CIFAR-100的100个类别转为20个类别（粒度更粗，降低任务复杂度）\n",
    "def sparse2coarse(targets):\n",
    "    \"\"\"Convert Pytorch CIFAR100 sparse targets to coarse targets.\n",
    "\n",
    "    Usage:\n",
    "        trainset = torchvision.datasets.CIFAR100(path)\n",
    "        trainset.targets = sparse2coarse(trainset.targets)\n",
    "    \"\"\"\n",
    "    coarse_labels = np.array([ 4,  1, 14,  8,  0,  6,  7,  7, 18,  3,\n",
    "                               3, 14,  9, 18,  7, 11,  3,  9,  7, 11,\n",
    "                               6, 11,  5, 10,  7,  6, 13, 15,  3, 15,\n",
    "                               0, 11,  1, 10, 12, 14, 16,  9, 11,  5,\n",
    "                               5, 19,  8,  8, 15, 13, 14, 17, 18, 10,\n",
    "                               16, 4, 17,  4,  2,  0, 17,  4, 18, 17,\n",
    "                               10, 3,  2, 12, 12, 16, 12,  1,  9, 19,\n",
    "                               2, 10,  0,  1, 16, 12,  9, 13, 15, 13,\n",
    "                              16, 19,  2,  4,  6, 19,  5,  5,  8, 19,\n",
    "                              18,  1,  2, 15,  6,  0, 17,  8, 14, 13])\n",
    "    return coarse_labels[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "id": "jcZ7eQGET_YJ"
   },
   "outputs": [],
   "source": [
    "# 共有6w个图像，其中5w训练，1w测试\n",
    "def CIFAR100():\n",
    "    '''Return Cifar100\n",
    "    '''\n",
    "    train_dataset = torchvision.datasets.CIFAR100(root='../data/CIFAR-100',\n",
    "                                            train=True,\n",
    "                                            transform=transforms.ToTensor(),\n",
    "                                            download=True)\n",
    "    test_dataset = torchvision.datasets.CIFAR100(root='../data/CIFAR-100',\n",
    "                                            train=False,\n",
    "                                            transform=transforms.ToTensor(),\n",
    "                                            download=True)\n",
    "    total_img,total_label = [],[]\n",
    "    for imgs,labels in train_dataset:\n",
    "        total_img.append(imgs.numpy())\n",
    "        total_label.append(labels)\n",
    "    # for imgs,labels in test_dataset:\n",
    "    #     total_img.append(imgs.numpy())\n",
    "    #     total_label.append(labels) \n",
    "    total_img = np.array(total_img)\n",
    "    total_label = np.array(sparse2coarse(total_label))\n",
    "\n",
    "    cifar = [total_img, total_label]\n",
    "    return cifar, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "id": "f1eQhNtPUMOF"
   },
   "outputs": [],
   "source": [
    "# 基于 Dirichlet 分布 来模拟non-IID。返回一个形状为 (client_num, class_num) 的概率矩阵，每一行代表一个客户端对各类别的概率分布。\n",
    "def get_prob(non_iid, client_num, class_num = 20):\n",
    "    # Modify：我之后加上的\n",
    "    if data_random_fix:\n",
    "        np.random.seed(seed_num)  # 固定种子，确保数据抽样一致\n",
    "    \n",
    "    return np.random.dirichlet(np.repeat(non_iid, class_num), client_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "id": "npT3idE-UaGm"
   },
   "outputs": [],
   "source": [
    "def create_data(prob, size_per_client, dataset, N=20):\n",
    "    total_each_class = size_per_client * np.sum(prob, 0)\n",
    "    data, label = dataset\n",
    "\n",
    "    # Modify：我之后加上的\n",
    "    if data_random_fix:\n",
    "        np.random.seed(seed_num)  # 固定种子，确保数据抽样一致\n",
    "        random.seed(seed_num)\n",
    "\n",
    "    # 为每个类别随机采样数据\n",
    "    all_class_set = []\n",
    "    for i in range(N):\n",
    "        size = total_each_class[i]\n",
    "        sub_data = data[label == i]\n",
    "        sub_label = label[label == i]\n",
    "\n",
    "        rand_indx = np.random.choice(len(sub_data), size=int(size), replace=False).astype(int)\n",
    "        sub2_data, sub2_label = sub_data[rand_indx], sub_label[rand_indx]\n",
    "        all_class_set.append((sub2_data, sub2_label))\n",
    "\n",
    "    index = [0] * N\n",
    "    clients, test = [], []\n",
    "\n",
    "    for m in range(prob.shape[0]):  # 遍历客户端\n",
    "        labels, images = [], []  # 训练数据\n",
    "        tlabels, timages = [], [] # 测试数据\n",
    "\n",
    "        # TODO_241216：这里每个client的测试集和它的训练集分布相同，并且最后测试时，也是计算所有client中的准确率的平均值\n",
    "        # TODO_241216：别的FL方法也是这样做的吗？我也要这样做吗？\n",
    "        for n in range(N):\n",
    "            # 80%用于训练，20%用于测试\n",
    "            # 这里的int向下取整，会导致实际的数据量比计算略小\n",
    "            start, end = index[n], index[n] + int(prob[m][n] * size_per_client * 0.8)\n",
    "            test_start, test_end = end, index[n] + int(prob[m][n] * size_per_client)\n",
    "\n",
    "            image, label = all_class_set[n][0][start:end], all_class_set[n][1][start:end]\n",
    "            test_image, test_label = all_class_set[n][0][test_start:test_end], all_class_set[n][1][test_start:test_end]\n",
    "\n",
    "            # 记录当前类别的数据分配进度\n",
    "            index[n] += int(prob[m][n] * size_per_client)\n",
    "\n",
    "            labels.extend(label)\n",
    "            images.extend(image)\n",
    "\n",
    "            tlabels.extend(test_label)\n",
    "            timages.extend(test_image)\n",
    "\n",
    "        clients.append((np.array(images), np.array(labels)))\n",
    "        test.append((np.array(timages), np.array(tlabels)))\n",
    "\n",
    "    return clients, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "id": "DCsR6_QqUzJ8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 合并所有客户端的测试数据 （上面讲测试数据分成了不同的客户端）\n",
    "# 但并没有使用，用途不明\n",
    "def comb_client_test_func(client_test_data):\n",
    "    comb_client_test_image = []\n",
    "    comb_client_test_label = []\n",
    "    for i in range(client_num):\n",
    "        comb_client_test_image.extend(list(client_test_data[i][0]))\n",
    "        comb_client_test_label.extend(list(client_test_data[i][1]))\n",
    "    \n",
    "    # 将测试图片和标签合并为 numpy 数组\n",
    "    comb_client_test_image = np.array(comb_client_test_image)\n",
    "    comb_client_test_label = np.array(comb_client_test_label)\n",
    "    \n",
    "    label_count = Counter(comb_client_test_label)\n",
    "    print(\"测试集类别分布：\")\n",
    "    for label, count in sorted(label_count.items()):\n",
    "        print(f\"类别 {label}: {count} 个样本\")\n",
    "    \n",
    "    return [comb_client_test_image, comb_client_test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "id": "JEKzDM0yW3DW"
   },
   "outputs": [],
   "source": [
    "# 从数据集中按类别均匀抽取子集，并按照指定的比例 percentage 进行缩减，同时对数据进行随机打乱\n",
    "def select_subset(whole_set, percentage):\n",
    "    \n",
    "    # Modify：我之后加上的\n",
    "    if data_random_fix:\n",
    "        np.random.seed(seed_num)  # 固定种子，确保数据抽样一致\n",
    "        random.seed(seed_num)\n",
    "    \n",
    "    a = whole_set[0]\n",
    "    b = whole_set[1]\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(\"Both arrays should have the same length.\")\n",
    "\n",
    "    if not 0 <= percentage <= 1:\n",
    "        raise ValueError(\"Percentage must be between 0 and 1.\")\n",
    "\n",
    "    unique_classes = np.unique(b)\n",
    "\n",
    "    a_prime = []\n",
    "    b_prime = []\n",
    "\n",
    "    for cls in unique_classes:\n",
    "        indices = np.where(b == cls)[0]\n",
    "        subset_size = int(len(indices) * percentage)\n",
    "\n",
    "        selected_indices = np.random.choice(indices, subset_size, replace=False)\n",
    "\n",
    "        a_prime.extend(a[selected_indices])\n",
    "        b_prime.extend(b[selected_indices])\n",
    "\n",
    "    a_prime, b_prime = np.array(a_prime), np.array(b_prime)\n",
    "\n",
    "    # Shuffle arrays to randomize the order of elements\n",
    "    shuffle_indices = np.random.permutation(len(a_prime))\n",
    "    a_prime, b_prime = a_prime[shuffle_indices], b_prime[shuffle_indices]\n",
    "\n",
    "    return [a_prime, b_prime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[312], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m cifar, test_dataset \u001b[38;5;241m=\u001b[39m CIFAR100()\n\u001b[1;32m      5\u001b[0m prob \u001b[38;5;241m=\u001b[39m get_prob(non_iid, client_num, class_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m client_data, client_test_data \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_per_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcifar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 将测试标签转换为粗类别\u001b[39;00m\n\u001b[1;32m      9\u001b[0m test_dataset\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;241m=\u001b[39m sparse2coarse(test_dataset\u001b[38;5;241m.\u001b[39mtargets)\n",
      "Cell \u001b[0;32mIn[309], line 17\u001b[0m, in \u001b[0;36mcreate_data\u001b[0;34m(prob, size_per_client, dataset, N)\u001b[0m\n\u001b[1;32m     14\u001b[0m sub_data \u001b[38;5;241m=\u001b[39m data[label \u001b[38;5;241m==\u001b[39m i]\n\u001b[1;32m     15\u001b[0m sub_label \u001b[38;5;241m=\u001b[39m label[label \u001b[38;5;241m==\u001b[39m i]\n\u001b[0;32m---> 17\u001b[0m rand_indx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msub_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     18\u001b[0m sub2_data, sub2_label \u001b[38;5;241m=\u001b[39m sub_data[rand_indx], sub_label[rand_indx]\n\u001b[1;32m     19\u001b[0m all_class_set\u001b[38;5;241m.\u001b[39mappend((sub2_data, sub2_label))\n",
      "File \u001b[0;32mmtrand.pyx:984\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "# 准备数据集\n",
    "# 这部分是我加的\n",
    "\n",
    "cifar, test_dataset = CIFAR100()\n",
    "prob = get_prob(non_iid, client_num, class_num=20)\n",
    "client_data, client_test_data = create_data(prob, size_per_client, cifar, N=20)\n",
    "\n",
    "# 将测试标签转换为粗类别\n",
    "test_dataset.targets = sparse2coarse(test_dataset.targets)\n",
    "\n",
    "# 如果需要确保测试标签为整数类型\n",
    "test_dataset.targets = test_dataset.targets.astype(int)\n",
    "\n",
    "# 打印训练标签的唯一值\n",
    "print(\"训练标签的唯一值：\", np.unique(cifar[1]))\n",
    "\n",
    "# 打印测试标签的唯一值\n",
    "print(\"测试标签的唯一值：\", np.unique(test_dataset.targets))\n",
    "\n",
    "\n",
    "all_images = []\n",
    "all_labels = []\n",
    "for data in client_data:\n",
    "    all_images.extend(data[0])\n",
    "    all_labels.extend(data[1])\n",
    "comb_client_data = [np.array(all_images), np.array(all_labels)]\n",
    "\n",
    "# 输出comb_client_data情况\n",
    "imgs, lbls = comb_client_data\n",
    "lbls = np.array(lbls)\n",
    "total_count = len(lbls)\n",
    "unique_classes, counts = np.unique(lbls, return_counts=True)\n",
    "\n",
    "# 创建一个长度为20的数组记录各类别计数，默认0\n",
    "class_counts = [0]*20\n",
    "for cls, cnt in zip(unique_classes, counts):\n",
    "    class_counts[cls] = cnt\n",
    "\n",
    "# 打印格式：Total: 总数 类别0计数 类别1计数 ... 类别19计数\n",
    "print(\"Traning Client Total: {}\".format(\" \".join([str(total_count)] + [str(c) for c in class_counts])))\n",
    "\n",
    "\n",
    "# 打印每个客户端训练数据情况（只输出前10个）\n",
    "for i, (imgs, lbls) in enumerate(client_data[:10]):\n",
    "    lbls = np.array(lbls)\n",
    "    total_count = len(lbls)\n",
    "    unique_classes, counts = np.unique(lbls, return_counts=True)\n",
    "    # 创建一个长度为20的数组记录各类别计数，默认0\n",
    "    class_counts = [0]*20\n",
    "    for cls, cnt in zip(unique_classes, counts):\n",
    "        class_counts[cls] = cnt\n",
    "    # 打印格式：Client i: 总数 类别0计数 类别1计数 ... 类别19计数\n",
    "    print(\"Client {}: {}\".format(i, \" \".join([str(total_count)] + [str(c) for c in class_counts])))\n",
    "    # 打印前5个数据和标签\n",
    "    # print(\"  前5个标签: \", lbls[:5])\n",
    "    # print(\"  前5个数据形状: \", [imgs[j].shape for j in range(min(5, len(imgs)))])\n",
    "    # print()\n",
    "    \n",
    "\n",
    "# 打印每个客户端测试数据情况（只输出前10个）\n",
    "for i, (imgs, lbls) in enumerate(client_test_data[:10]):\n",
    "    lbls = np.array(lbls)\n",
    "    total_count = len(lbls)\n",
    "    unique_classes, counts = np.unique(lbls, return_counts=True)\n",
    "    class_counts = [0]*20\n",
    "    for cls, cnt in zip(unique_classes, counts):\n",
    "        class_counts[cls] = cnt\n",
    "    # 打印格式：Client i Test: 总数 类别0计数 类别1计数 ... 类别19计数\n",
    "    print(\"Client {} Test: {}\".format(i, \" \".join([str(total_count)] + [str(c) for c in class_counts])))\n",
    "    # 打印前5个数据和标签\n",
    "    # print(\"  前5个标签: \", lbls[:5])\n",
    "    # print(\"  前5个数据形状: \", [imgs[j].shape for j in range(min(5, len(imgs)))])\n",
    "    # print()\n",
    "\n",
    "# 提前生成固定的服务器数据\n",
    "# Modify: 这是我后来修改的\n",
    "server_data = select_subset(comb_client_data, server_percentage)\n",
    "\n",
    "s_imgs, s_lbls = server_data\n",
    "s_lbls = np.array(s_lbls)\n",
    "total_count = len(s_lbls)\n",
    "unique_classes, counts = np.unique(s_lbls, return_counts=True)\n",
    "class_counts = [0]*20\n",
    "for cls, cnt in zip(unique_classes, counts):\n",
    "    class_counts[cls] = cnt\n",
    "\n",
    "# 输出格式: Server: 总数 类别0计数 类别1计数 ... 类别19计数\n",
    "print(\"Server: {}\".format(\" \".join([str(total_count)] + [str(c) for c in class_counts])))\n",
    "# print(\"  前5个标签: \", lbls[:5])\n",
    "# print(\"  前5个数据形状: \", [server_data[0][j].shape for j in range(min(5, len(server_data[0])))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "id": "XvuicdZfeDvL"
   },
   "outputs": [],
   "source": [
    "# 本地训练并更新权重，返回更新后的模型权重、平均训练损失以及第一个迭代的梯度信息\n",
    "def update_weights(model_weight, dataset, learning_rate, local_epoch):\n",
    "    model = cnncifar().to(device)\n",
    "    model.load_state_dict(model_weight)\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    Tensor_set = TensorDataset(torch.Tensor(dataset[0]).to(device), torch.Tensor(dataset[1]).to(device))\n",
    "    data_loader = DataLoader(Tensor_set, batch_size=bc_size, shuffle=True)\n",
    "\n",
    "    first_iter_gradient = None  # 初始化变量来保存第一个iter的梯度\n",
    "\n",
    "    for iter in range(local_epoch):\n",
    "        batch_loss = []\n",
    "        for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs['output'], labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss.append(loss.item()/images.shape[0])\n",
    "\n",
    "            # 保存第一个iter的梯度\n",
    "            if iter == 0 and batch_idx == 0:\n",
    "                first_iter_gradient = {}\n",
    "                for name, param in model.named_parameters():\n",
    "                    first_iter_gradient[name] = param.grad.clone()\n",
    "                # 保存 BatchNorm 层的 running mean 和 running variance\n",
    "                for name, module in model.named_modules():\n",
    "                    if isinstance(module, nn.BatchNorm2d):\n",
    "                        first_iter_gradient[name + '.running_mean'] = module.running_mean.clone()\n",
    "                        first_iter_gradient[name + '.running_var'] = module.running_var.clone()\n",
    "\n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "\n",
    "    return model.state_dict(), sum(epoch_loss) / len(epoch_loss), first_iter_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "id": "qsPZGD4Iem5w"
   },
   "outputs": [],
   "source": [
    "# 计算模型权重的差异，并根据学习率 lr 对权重差异进行缩放\n",
    "def weight_differences(n_w, p_w, lr):\n",
    "    w_diff = copy.deepcopy(n_w)\n",
    "    for key in w_diff.keys():\n",
    "        if 'num_batches_tracked' in key:\n",
    "            continue\n",
    "        w_diff[key] = (p_w[key] - n_w[key]) * lr\n",
    "    return w_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "id": "040d862vbG9M"
   },
   "outputs": [],
   "source": [
    "# 也是本地训练，不过引入了Fed-C的权重修正机制\n",
    "def update_weights_correction(model_weight, dataset, learning_rate, local_epoch, c_i, c_s):\n",
    "    model = cnncifar().to(device)\n",
    "    model.load_state_dict(model_weight)\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    Tensor_set = TensorDataset(torch.Tensor(dataset[0]).to(device), torch.Tensor(dataset[1]).to(device))\n",
    "    data_loader = DataLoader(Tensor_set, batch_size=bc_size, shuffle=True)\n",
    "\n",
    "    for iter in range(local_epoch):\n",
    "        batch_loss = []\n",
    "        for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs['output'], labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss.append(loss.sum().item()/images.shape[0])\n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "        corrected_graident = weight_differences(c_i, c_s, learning_rate)\n",
    "        orginal_model_weight = model.state_dict()\n",
    "        corrected_model_weight = weight_differences(corrected_graident, orginal_model_weight, 1)  # 这里缩放权重为1\n",
    "        model.load_state_dict(corrected_model_weight)\n",
    "\n",
    "    return model.state_dict(),  sum(epoch_loss) / len(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "id": "qeFHXRuEo5Du"
   },
   "outputs": [],
   "source": [
    "def average_weights(w):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        if 'num_batches_tracked' in key:\n",
    "            continue\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], len(w))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "id": "pLG9RrFffbl8"
   },
   "outputs": [],
   "source": [
    "# baseline: server-only\n",
    "def server_only(initial_w, global_round, gamma, E):\n",
    "    test_model = cnncifar().to(device)\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        # Server side local training\n",
    "        \n",
    "        \n",
    "        # 从comb中（论文中说明为全部训练数据）选择固定比率的server数据（并且是保证类别均衡的）\n",
    "        # server_data = select_subset(comb_client_data, server_percentage)\n",
    "                \n",
    "        update_server_w, round_loss, _ = update_weights(train_w, server_data, gamma, E)\n",
    "        train_w = update_server_w\n",
    "        test_model.load_state_dict(train_w)\n",
    "        train_loss.append(round_loss)\n",
    "        \n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "    \n",
    "        # Test Accuracy\n",
    "        # test_a = 0\n",
    "        # for i in client_test_data:\n",
    "        #     ac = test_inference(test_model,i)[0]\n",
    "        #     test_a = test_a + ac\n",
    "        # test_a = test_a/len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "        # print(test_a)\n",
    "    return test_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "id": "nGtAn28aok2c"
   },
   "outputs": [],
   "source": [
    "def fedavg(initial_w, global_round, eta, K, M):\n",
    "    test_model = cnncifar().to(device)\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    for round in tqdm(range(global_round)):\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "        for i in sampled_client:\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        train_w = average_weights(local_weights)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "        \n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "            \n",
    "        # test_a = 0\n",
    "        # for i in client_test_data:\n",
    "        #     ac = test_inference(test_model,i)[0]\n",
    "        #     test_a = test_a + ac\n",
    "        # test_a = test_a/len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "#         print(test_a)\n",
    "    return test_acc, train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybridFL(initial_w, global_round, eta, K, M):\n",
    "    \"\"\"\n",
    "    HybridFL算法：FedAvg改进，服务器也作为一个普通客户端参与训练。\n",
    "    \n",
    "    参数:\n",
    "    - initial_w: 初始模型权重\n",
    "    - global_round: 全局训练轮数\n",
    "    - eta: 学习率\n",
    "    - K: 本地训练轮数\n",
    "    - M: 每轮采样的客户端数量\n",
    "    \"\"\"\n",
    "    test_model = cnncifar().to(device)  # 初始化测试模型\n",
    "    train_w = copy.deepcopy(initial_w)     # 当前全局权重\n",
    "    test_acc = []                          # 保存每轮测试精度\n",
    "    train_loss = []                        # 保存每轮训练损失\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        local_weights, local_loss = [], []  # 存储每个客户端/服务器的权重和损失\n",
    "\n",
    "        # 随机采样 M 个客户端\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "\n",
    "        # 客户端本地训练\n",
    "        for i in sampled_client:\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        # 服务器参与训练\n",
    "        update_server_w, server_round_loss, _ = update_weights(train_w, server_data, eta, K)\n",
    "        local_weights.append(update_server_w)   # 将服务器权重加入列表\n",
    "        local_loss.append(server_round_loss)    # 将服务器损失加入列表\n",
    "\n",
    "        # 权重聚合\n",
    "        train_w = average_weights(local_weights)\n",
    "\n",
    "        # 评估模型性能\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss) / len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "    \n",
    "        \n",
    "        # test_a = 0\n",
    "        # for i in client_test_data:  # 遍历所有客户端测试数据\n",
    "        #     ac = test_inference(test_model, i)[0]\n",
    "        #     test_a += ac\n",
    "        # test_a = test_a / len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "        \n",
    "        # # 打印每轮的结果\n",
    "        # print(f\"Round {round + 1}: Test Accuracy = {test_a:.4f}, Train Loss = {loss_avg:.4f}\")\n",
    "    \n",
    "    return test_acc, train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "id": "uzxW0sUxRGth"
   },
   "outputs": [],
   "source": [
    "def CLG_SGD(initial_w, global_round, eta, gamma, K, E, M):\n",
    "    test_model = cnncifar().to(device)\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # 学习率衰减，这里默认注释掉了\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # 从总共client_num客户端中选择M个训练\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "        for i in sampled_client:\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "        train_w = average_weights(local_weights)\n",
    "        # Server side local training\n",
    "        \n",
    "        \n",
    "        # 从comb中（论文中说明为全部训练数据）选择固定比率的server数据（并且是保证类别均衡的）\n",
    "        # TODO_241216:这里是每一轮都重新选择数据（但保证类别比例是一样的，都是按照comb中的比例），我的场景中可以这样吗？\n",
    "        # server_data = select_subset(comb_client_data, server_percentage)\n",
    "        \n",
    "        update_server_w, round_loss, _ = update_weights(train_w, server_data, gamma, E)\n",
    "        train_w = update_server_w\n",
    "        local_loss.append(round_loss)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)   # 计算所有客户端和服务器一起的平均损失\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "    \n",
    "        # test_a = 0\n",
    "        # # 遍历客户端测试数据，计算平均准确率\n",
    "        # for i in client_test_data:\n",
    "        #     ac = test_inference(test_model,i)[0]\n",
    "        #     test_a = test_a + ac\n",
    "        # test_a = test_a/len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "#         print(test_a)\n",
    "    return test_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "id": "iJTODAzxYJgA"
   },
   "outputs": [],
   "source": [
    "def Fed_C(initial_w, global_round, eta, gamma, K, E, M):\n",
    "    test_model = cnncifar().to(device)\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        g_i_list = []\n",
    "        # server_data = select_subset(comb_client_data, server_percentage)\n",
    "        \n",
    "        \n",
    "        # 计算Server gradient\n",
    "        _, _, g_s = update_weights(train_w, server_data, gamma, 1)\n",
    "\n",
    "        # 计算Client gradient\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "        for i in sampled_client:\n",
    "            _, _, g_i = update_weights(train_w, client_data[i], eta, 1)\n",
    "            g_i_list.append(g_i)\n",
    "\n",
    "\n",
    "        # Client side local training\n",
    "        for i in range(len(sampled_client)):\n",
    "            update_client_w, client_round_loss = update_weights_correction(train_w, client_data[sampled_client[i]], eta, K, g_i_list[i], g_s)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "        train_w = average_weights(local_weights)\n",
    "        # Server side local training\n",
    "        update_server_w, round_loss, _ = update_weights(train_w, server_data, gamma, E)\n",
    "        train_w = update_server_w\n",
    "        local_loss.append(round_loss)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "    \n",
    "        # test_a = 0\n",
    "        # for i in client_test_data:\n",
    "        #     ac = test_inference(test_model,i)[0]\n",
    "        #     test_a = test_a + ac\n",
    "        # test_a = test_a/len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "    return test_acc, train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "id": "tXqVbWZK7gLn"
   },
   "outputs": [],
   "source": [
    "def Fed_S(initial_w, global_round, eta, gamma, K, E, M):\n",
    "    test_model = cnncifar().to(device)\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        g_i_list = []\n",
    "        # Server gradient\n",
    "        # server_data = select_subset(comb_client_data, server_percentage)\n",
    "        _, _, g_s = update_weights(train_w, server_data, gamma, 1)\n",
    "\n",
    "        # Client gradient\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "        for i in sampled_client:\n",
    "            _, _, g_i = update_weights(train_w, client_data[i], eta, 1)\n",
    "            g_i_list.append(g_i)\n",
    "\n",
    "\n",
    "        # Client side local training\n",
    "        for i in range(len(sampled_client)):\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[sampled_client[i]], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "        train_w = average_weights(local_weights)\n",
    "\n",
    "        # Server aggregation correction\n",
    "        g_i_average = average_weights(g_i_list)\n",
    "        correction_g = weight_differences(g_i_average, g_s, K*eta)\n",
    "        train_w = weight_differences(correction_g, copy.deepcopy(train_w), 1)\n",
    "\n",
    "\n",
    "        # Server side local training\n",
    "        update_server_w, round_loss, _ = update_weights(train_w, server_data, gamma, E)\n",
    "        train_w = update_server_w\n",
    "        local_loss.append(round_loss)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "        \n",
    "        # test_a = 0\n",
    "        # for i in client_test_data:\n",
    "        #     ac = test_inference(test_model,i)[0]\n",
    "        #     test_a = test_a + ac\n",
    "        # test_a = test_a/len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "    return test_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def CLG_Mut(net_glob, global_round, eta, gamma, K, E, M):\n",
    "    \n",
    "    net_glob.train()\n",
    "    \n",
    "    test_model = cnncifar().to(device)\n",
    "    train_w = copy.deepcopy(net_glob.state_dict())\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    w_locals = []\n",
    "    for i in range(M):\n",
    "        w_locals.append(copy.deepcopy(net_glob.state_dict()))\n",
    "    \n",
    "    delta_list = []\n",
    "    max_rank = 0\n",
    "    w_old = copy.deepcopy(net_glob.state_dict())\n",
    "    w_old_s1 = copy.deepcopy(net_glob.state_dict())\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # 学习率衰减，这里默认注释掉了\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # 从总共client_num客户端中选择M个训练\n",
    "        idxs_users = np.random.choice(range(client_num), M, replace=False)\n",
    "        for i, idx in enumerate(idxs_users):\n",
    "            net_glob.load_state_dict(w_locals[i])\n",
    "            \n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            w_locals[i] = copy.deepcopy(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        # Global Model Generation\n",
    "        w_agg = Aggregation(w_locals, None)  \n",
    "        \n",
    "        # Server side local training\n",
    "        update_server_w, round_loss, _ = update_weights(w_agg, server_data, gamma, E)\n",
    "        local_loss.append(round_loss)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(update_server_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)   # 计算所有客户端和服务器一起的平均损失\n",
    "\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "\n",
    "        # 老测试：针对每个client计算测试\n",
    "        # test_a = 0\n",
    "        # # 遍历客户端测试数据，计算平均准确率\n",
    "        # for i in client_test_data:\n",
    "        #     ac = test_inference(test_model,i)[0]\n",
    "        #     test_a = test_a + ac\n",
    "        # test_a = test_a/len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "#         print(test_a)\n",
    "\n",
    "        # 按照server训练的方向，进行mutation\n",
    "        w_delta = FedSub(update_server_w, w_agg, 1.0)\n",
    "        # 计算模型更新w_delta的L2范数（平方和），衡量模型更新程度的大小\n",
    "        rank = delta_rank(w_delta)\n",
    "        # print(rank)\n",
    "        if rank > max_rank:\n",
    "            max_rank = rank\n",
    "        alpha = radius  # 论文中的alpha，衡量Mutation的幅度\n",
    "        # alpha = min(max(args.radius, max_rank/rank),(10.0-args.radius) * (1 - iter/args.epochs) + args.radius)\n",
    "        w_locals = mutation_spread(\n",
    "            round, update_server_w, M, w_delta, alpha\n",
    "        )\n",
    "\n",
    "    return test_acc, train_loss\n",
    "\n",
    "\n",
    "def mutation_spread(iter, w_glob, m, w_delta, alpha):\n",
    "    # w_delta = FedSub(w_glob,w_old,(args.radius - args.min_radius) * (1.0 - iter/args.epochs) + args.min_radius)\n",
    "    # if iter/args.epochs > 0.5:\n",
    "    #     w_delta = FedSub(w_glob,w_old,(args.radius - args.min_radius) * (1.0 - iter/args.epochs)*2 + args.min_radius)\n",
    "    # else:\n",
    "    # w_delta = FedSub(w_glob,w_old,(args.radius - args.min_radius) * (iter/args.epochs)*2 + args.min_radius)\n",
    "    # w_delta = FedSub(w_glob, w_old, args.radius)\n",
    "\n",
    "    w_locals_new = []\n",
    "    ctrl_cmd_list = []\n",
    "    ctrl_rate = mut_acc_rate * (\n",
    "        1.0 - min(iter * 1.0 / mut_bound, 1.0)\n",
    "    )  # 论文中的βt，随着iter逐渐从β0减小到0\n",
    "\n",
    "    # k代表模型中的参数数量，对每个参数按照client数量分配v（论文中是按照每一层分配）\n",
    "    for k in w_glob.keys():\n",
    "        ctrl_list = []\n",
    "        for i in range(0, int(m / 2)):\n",
    "            ctrl = random.random()  # 随机数，范围：[0,1)\n",
    "            # 这里分ctrl感觉没什么必要，shuffle后都会随机掉\n",
    "            if ctrl > 0.5:\n",
    "                ctrl_list.append(1.0)\n",
    "                ctrl_list.append(1.0 * (-1.0 + ctrl_rate))\n",
    "            else:\n",
    "                ctrl_list.append(1.0 * (-1.0 + ctrl_rate))\n",
    "                ctrl_list.append(1.0)\n",
    "        random.shuffle(ctrl_list)  # 打乱列表\n",
    "        ctrl_cmd_list.append(ctrl_list)\n",
    "    cnt = 0\n",
    "    for j in range(m):\n",
    "        w_sub = copy.deepcopy(w_glob)\n",
    "        if not (cnt == m - 1 and m % 2 == 1):\n",
    "            ind = 0\n",
    "            for k in w_sub.keys():\n",
    "                w_sub[k] = w_sub[k] + w_delta[k] * ctrl_cmd_list[ind][j] * alpha\n",
    "                ind += 1\n",
    "        cnt += 1\n",
    "        w_locals_new.append(w_sub)\n",
    "\n",
    "    return w_locals_new\n",
    "\n",
    "\n",
    "\n",
    "# 加权平均聚合，lens代表了权重，如果没有定义就是普通平均（FedMut就每定义）\n",
    "def Aggregation(w, lens):\n",
    "    w_avg = None\n",
    "    if lens == None:\n",
    "        total_count = len(w)\n",
    "        lens = []\n",
    "        for i in range(len(w)):\n",
    "            lens.append(1.0)\n",
    "    else:\n",
    "        total_count = sum(lens)\n",
    "\n",
    "    for i in range(0, len(w)):\n",
    "        if i == 0:\n",
    "            w_avg = copy.deepcopy(w[0])\n",
    "            for k in w_avg.keys():\n",
    "                w_avg[k] = w[i][k] * lens[i]\n",
    "        else:\n",
    "            for k in w_avg.keys():\n",
    "                w_avg[k] += w[i][k] * lens[i]\n",
    "\n",
    "    for k in w_avg.keys():\n",
    "        w_avg[k] = torch.div(w_avg[k], total_count)\n",
    "\n",
    "    return w_avg\n",
    "\n",
    "\n",
    "\n",
    "def FedSub(w, w_old, weight):\n",
    "    w_sub = copy.deepcopy(w)\n",
    "    for k in w_sub.keys():\n",
    "        w_sub[k] = (w[k] - w_old[k]) * weight\n",
    "\n",
    "    return w_sub\n",
    "\n",
    "def delta_rank(delta_dict):\n",
    "    cnt = 0\n",
    "    dict_a = torch.Tensor(0)\n",
    "    s = 0\n",
    "    for p in delta_dict.keys():\n",
    "        a = delta_dict[p]\n",
    "        a = a.view(-1)\n",
    "        if cnt == 0:\n",
    "            dict_a = a\n",
    "        else:\n",
    "            dict_a = torch.cat((dict_a, a), dim=0)\n",
    "\n",
    "        cnt += 1\n",
    "        # print(sim)\n",
    "    s = torch.norm(dict_a, dim=0)\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[295], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 初始化模型与参数\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 这部分是我补充的\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 用了FedMut中定义的CNN网络\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m init_model \u001b[38;5;241m=\u001b[39m \u001b[43mcnncifar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m initial_w \u001b[38;5;241m=\u001b[39m init_model\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# CLG_Mut训练，这里不同在于直接传初始化后的模型\u001b[39;00m\n",
      "File \u001b[0;32m/home/anaconda/envs/env8/lib/python3.8/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/anaconda/envs/env8/lib/python3.8/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/home/anaconda/envs/env8/lib/python3.8/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/home/anaconda/envs/env8/lib/python3.8/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型与参数\n",
    "# 这部分是我补充的\n",
    "\n",
    "# 用了FedMut中定义的CNN网络\n",
    "\n",
    "init_model = cnncifar().to(device)\n",
    "initial_w = init_model.state_dict()\n",
    "\n",
    "# CLG_Mut训练，这里不同在于直接传初始化后的模型\n",
    "test_acc, train_loss = CLG_Mut(init_model, global_round, eta, gamma, K, E, M)\n",
    "\n",
    "# 打印训练过程中的结果\n",
    "print(\"CLG-Mut 训练完成！\")\n",
    "print(\"各轮平均测试精度:\", test_acc)\n",
    "print(\"各轮平均训练损失:\", train_loss)\n",
    "print(\"最终测试精度:\", test_acc[-1] if len(test_acc) > 0 else \"无数据\")\n",
    "\n",
    "\n",
    "\n",
    "# # Servfer-only训练\n",
    "# test_acc, train_loss = server_only(initial_w, global_round, gamma, E)\n",
    "\n",
    "# # 打印训练过程中的结果\n",
    "# print(\"Server only 训练完成！\")\n",
    "# print(\"各轮平均测试精度:\", test_acc)\n",
    "# print(\"各轮平均训练损失:\", train_loss)\n",
    "# print(\"最终测试精度:\", test_acc[-1] if len(test_acc) > 0 else \"无数据\")\n",
    "\n",
    "\n",
    "# # fedavg训练\n",
    "# test_acc, train_loss = fedavg(initial_w, global_round, eta, K, M)\n",
    "\n",
    "# # 打印训练过程中的结果\n",
    "# print(\"fedavg训练完成！\")\n",
    "# print(\"各轮平均测试精度:\", test_acc)\n",
    "# print(\"各轮平均训练损失:\", train_loss)\n",
    "# print(\"最终测试精度:\", test_acc[-1] if len(test_acc) > 0 else \"无数据\")\n",
    "\n",
    "\n",
    "\n",
    "# # hybridfl训练\n",
    "# test_acc, train_loss = hybridFL(initial_w, global_round, eta, K, M)\n",
    "\n",
    "# # 打印训练过程中的结果\n",
    "# print(\"hrbridFL训练完成！\")\n",
    "# print(\"各轮平均测试精度:\", test_acc)\n",
    "# print(\"各轮平均训练损失:\", train_loss)\n",
    "# print(\"最终测试精度:\", test_acc[-1] if len(test_acc) > 0 else \"无数据\")\n",
    "\n",
    "\n",
    "\n",
    "# # CLG_SGD训练\n",
    "# test_acc, train_loss = CLG_SGD(initial_w, global_round, eta, gamma, K, E, M)\n",
    "\n",
    "# # 打印训练过程中的结果\n",
    "# print(\"CLG_SGD 训练完成！\")\n",
    "# print(\"各轮平均测试精度:\", test_acc)\n",
    "# print(\"各轮平均训练损失:\", train_loss)\n",
    "# print(\"最终测试精度:\", test_acc[-1] if len(test_acc) > 0 else \"无数据\")\n",
    "\n",
    "\n",
    "\n",
    "# # Fed_C训练\n",
    "# test_acc, train_loss = Fed_C(initial_w, global_round, eta, gamma, K, E, M)\n",
    "\n",
    "# # 打印训练过程中的结果\n",
    "# print(\"Fed_C 训练完成！\")\n",
    "# print(\"各轮平均测试精度:\", test_acc)\n",
    "# print(\"各轮平均训练损失:\", train_loss)\n",
    "# print(\"最终测试精度:\", test_acc[-1] if len(test_acc) > 0 else \"无数据\")\n",
    "\n",
    "\n",
    "\n",
    "# # Fed_S训练\n",
    "# test_acc, train_loss = Fed_S(initial_w, global_round, eta, gamma, K, E, M)\n",
    "\n",
    "# # 打印训练过程中的结果\n",
    "# print(\"Fed_S 训练完成！\")\n",
    "# print(\"各轮平均测试精度:\", test_acc)\n",
    "# print(\"各轮平均训练损失:\", train_loss)\n",
    "# print(\"最终测试精度:\", test_acc[-1] if len(test_acc) > 0 else \"无数据\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
