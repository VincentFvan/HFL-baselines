{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSK1TSekTVeu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.functional as func\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-WEXWakTwf8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sns3blEITybc",
    "outputId": "120095fb-5597-4ada-8c12-b4470d8ad28e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 25 13:31:32 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   33C    P0    25W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96gqMCQneWho"
   },
   "outputs": [],
   "source": [
    "# MobileNetV2（比lenet更复杂的CNN网络）网络中的线性瓶颈结构，原文中用于CIFAR-100任务\n",
    "class LinearBottleNeck(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride, t=6, class_num=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * t, 1),\n",
    "            nn.BatchNorm2d(in_channels * t),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels * t, in_channels * t, 3, stride=stride, padding=1, groups=in_channels * t),\n",
    "            nn.BatchNorm2d(in_channels * t),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels * t, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.stride = stride\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = self.residual(x)\n",
    "\n",
    "        if self.stride == 1 and self.in_channels == self.out_channels:\n",
    "            residual += x\n",
    "\n",
    "        return residual\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "\n",
    "    def __init__(self, class_num=20):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pre = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.stage1 = LinearBottleNeck(32, 16, 1, 1)\n",
    "        self.stage2 = self._make_stage(2, 16, 24, 2, 6)\n",
    "        self.stage3 = self._make_stage(3, 24, 32, 2, 6)\n",
    "        self.stage4 = self._make_stage(4, 32, 64, 2, 6)\n",
    "        self.stage5 = self._make_stage(3, 64, 96, 1, 6)\n",
    "        self.stage6 = self._make_stage(3, 96, 160, 1, 6)\n",
    "        self.stage7 = LinearBottleNeck(160, 320, 1, 6)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(320, 1280, 1),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(1280, class_num, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "        x = self.stage6(x)\n",
    "        x = self.stage7(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _make_stage(self, repeat, in_channels, out_channels, stride, t):\n",
    "\n",
    "        layers = []\n",
    "        layers.append(LinearBottleNeck(in_channels, out_channels, stride, t))\n",
    "\n",
    "        while repeat - 1:\n",
    "            layers.append(LinearBottleNeck(out_channels, out_channels, 1, t))\n",
    "            repeat -= 1\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "def mobilenetv2():\n",
    "    return MobileNetV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkvtGtMuUDmr"
   },
   "outputs": [],
   "source": [
    "def test_inference(model, test):\n",
    "    \"\"\" Returns the test accuracy and loss.\n",
    "    \"\"\"\n",
    "    tensor_x = torch.Tensor(test[0]).to(device)\n",
    "    tensor_y = torch.Tensor(test[1]).to(device)\n",
    "    test_dataset = TensorDataset(tensor_x, tensor_y)\n",
    "\n",
    "    model.eval()\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    testloader = DataLoader(test_dataset, batch_size=128,\n",
    "                            shuffle=True)\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(testloader):\n",
    "        with torch.no_grad():  # 在测试过程中不需要计算梯度，节省内存和加速计算\n",
    "        # Inference\n",
    "            outputs = model(images)\n",
    "            batch_loss = criterion(outputs, labels.long())\n",
    "            loss += batch_loss.item() * labels.size(0) # 计算损失值，更好反映模型输出概率分布与真实标签的差距\n",
    "\n",
    "        # Prediction\n",
    "            _, pred_labels = torch.max(outputs, 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "            total += len(labels)\n",
    "    #print(correct,\"/\",total)\n",
    "    accuracy = correct/total\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jURskA9VUJOF"
   },
   "outputs": [],
   "source": [
    "# 将CIFAR-100的100个类别转为20个类别（粒度更粗，降低任务复杂度）\n",
    "def sparse2coarse(targets):\n",
    "    \"\"\"Convert Pytorch CIFAR100 sparse targets to coarse targets.\n",
    "\n",
    "    Usage:\n",
    "        trainset = torchvision.datasets.CIFAR100(path)\n",
    "        trainset.targets = sparse2coarse(trainset.targets)\n",
    "    \"\"\"\n",
    "    coarse_labels = np.array([ 4,  1, 14,  8,  0,  6,  7,  7, 18,  3,\n",
    "                               3, 14,  9, 18,  7, 11,  3,  9,  7, 11,\n",
    "                               6, 11,  5, 10,  7,  6, 13, 15,  3, 15,\n",
    "                               0, 11,  1, 10, 12, 14, 16,  9, 11,  5,\n",
    "                               5, 19,  8,  8, 15, 13, 14, 17, 18, 10,\n",
    "                               16, 4, 17,  4,  2,  0, 17,  4, 18, 17,\n",
    "                               10, 3,  2, 12, 12, 16, 12,  1,  9, 19,\n",
    "                               2, 10,  0,  1, 16, 12,  9, 13, 15, 13,\n",
    "                              16, 19,  2,  4,  6, 19,  5,  5,  8, 19,\n",
    "                              18,  1,  2, 15,  6,  0, 17,  8, 14, 13])\n",
    "    return coarse_labels[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcZ7eQGET_YJ"
   },
   "outputs": [],
   "source": [
    "def CIFAR100():\n",
    "    '''Return Cifar100\n",
    "    '''\n",
    "    train_dataset = torchvision.datasets.CIFAR100(root='../data/CIFAR-100',\n",
    "                                            train=True,\n",
    "                                            transform=transforms.ToTensor(),\n",
    "                                            download=True)\n",
    "    test_dataset = torchvision.datasets.CIFAR100(root='../data/CIFAR-100',\n",
    "                                            train=False,\n",
    "                                            transform=transforms.ToTensor(),\n",
    "                                            download=True)\n",
    "    total_img,total_label = [],[]\n",
    "    for imgs,labels in train_dataset:\n",
    "        total_img.append(imgs.numpy())\n",
    "        total_label.append(labels)\n",
    "    for imgs,labels in test_dataset:\n",
    "        total_img.append(imgs.numpy())\n",
    "        total_label.append(labels) \n",
    "    total_img = np.array(total_img)\n",
    "    total_label = np.array(sparse2coarse(total_label))\n",
    "\n",
    "    cifar = [total_img, total_label]\n",
    "    return cifar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1eQhNtPUMOF"
   },
   "outputs": [],
   "source": [
    "# 基于 Dirichlet 分布 来模拟non-IID。返回一个形状为 (client_num, class_num) 的概率矩阵，每一行代表一个客户端对各类别的概率分布。\n",
    "def get_prob(non_iid, client_num, class_num = 20):\n",
    "    return np.random.dirichlet(np.repeat(non_iid, class_num), client_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npT3idE-UaGm"
   },
   "outputs": [],
   "source": [
    "def create_data(prob, size_per_client, dataset, N=20):\n",
    "    total_each_class = size_per_client * np.sum(prob, 0)\n",
    "    data, label = dataset\n",
    "\n",
    "\n",
    "    # 为每个类别随机采样数据\n",
    "    all_class_set = []\n",
    "    for i in range(N):\n",
    "        size = total_each_class[i]\n",
    "        sub_data = data[label == i]\n",
    "        sub_label = label[label == i]\n",
    "\n",
    "        rand_indx = np.random.choice(len(sub_data), size=int(size), replace=False).astype(int)\n",
    "        sub2_data, sub2_label = sub_data[rand_indx], sub_label[rand_indx]\n",
    "        all_class_set.append((sub2_data, sub2_label))\n",
    "\n",
    "    index = [0] * N\n",
    "    clients, test = [], []\n",
    "\n",
    "    for m in range(prob.shape[0]):  # 遍历客户端\n",
    "        labels, images = [], []  # 训练数据\n",
    "        tlabels, timages = [], [] # 测试数据\n",
    "\n",
    "        for n in range(N):\n",
    "            # 80%用于训练，20%用于测试\n",
    "            start, end = index[n], index[n] + int(prob[m][n] * size_per_client * 0.8)\n",
    "            test_start, test_end = end, index[n] + int(prob[m][n] * size_per_client)\n",
    "\n",
    "            image, label = all_class_set[n][0][start:end], all_class_set[n][1][start:end]\n",
    "            test_image, test_label = all_class_set[n][0][test_start:test_end], all_class_set[n][1][test_start:test_end]\n",
    "\n",
    "            # 记录当前类别的数据分配进度\n",
    "            index[n] += int(prob[m][n] * size_per_client)\n",
    "\n",
    "            labels.extend(label)\n",
    "            images.extend(image)\n",
    "\n",
    "            tlabels.extend(test_label)\n",
    "            timages.extend(test_image)\n",
    "\n",
    "        clients.append((np.array(images), np.array(labels)))\n",
    "        test.append((np.array(timages), np.array(tlabels)))\n",
    "\n",
    "    return clients, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCsR6_QqUzJ8"
   },
   "outputs": [],
   "source": [
    "# 合并所有客户端的测试数据 （上面讲测试数据分成了不同的客户端）\n",
    "def comb_client_test_func(client_test_data):\n",
    "    comb_client_test_image = []\n",
    "    comb_client_test_label = []\n",
    "    for i in range(client_num):\n",
    "        comb_client_test_image.extend(list(client_test_data[i][0]))\n",
    "        comb_client_test_label.extend(list(client_test_data[i][1]))\n",
    "    \n",
    "    # 将测试图片和标签合并为 numpy 数组\n",
    "    comb_client_test_image = np.array(comb_client_test_image)\n",
    "    comb_client_test_label = np.array(comb_client_test_label)\n",
    "    \n",
    "    # TODO: 计算测试集类别分布，与训练数据比较一下\n",
    "    label_count = Counter(comb_client_test_label)\n",
    "    print(\"测试集类别分布：\")\n",
    "    for label, count in sorted(label_count.items()):\n",
    "        print(f\"类别 {label}: {count} 个样本\")\n",
    "    \n",
    "    return [comb_client_test_image, comb_client_test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEKzDM0yW3DW"
   },
   "outputs": [],
   "source": [
    "# 从数据集中按类别均匀抽取子集，并按照指定的比例 percentage 进行缩减，同时对数据进行随机打乱\n",
    "def select_subset(whole_set, percentage):\n",
    "    a = whole_set[0]\n",
    "    b = whole_set[1]\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(\"Both arrays should have the same length.\")\n",
    "\n",
    "    if not 0 <= percentage <= 1:\n",
    "        raise ValueError(\"Percentage must be between 0 and 1.\")\n",
    "\n",
    "    unique_classes = np.unique(b)\n",
    "\n",
    "    a_prime = []\n",
    "    b_prime = []\n",
    "\n",
    "    for cls in unique_classes:\n",
    "        indices = np.where(b == cls)[0]\n",
    "        subset_size = int(len(indices) * percentage)\n",
    "\n",
    "        selected_indices = np.random.choice(indices, subset_size, replace=False)\n",
    "\n",
    "        a_prime.extend(a[selected_indices])\n",
    "        b_prime.extend(b[selected_indices])\n",
    "\n",
    "    a_prime, b_prime = np.array(a_prime), np.array(b_prime)\n",
    "\n",
    "    # Shuffle arrays to randomize the order of elements\n",
    "    shuffle_indices = np.random.permutation(len(a_prime))\n",
    "    a_prime, b_prime = a_prime[shuffle_indices], b_prime[shuffle_indices]\n",
    "\n",
    "    return [a_prime, b_prime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvuicdZfeDvL"
   },
   "outputs": [],
   "source": [
    "# 本地训练并更新权重，返回更新后的模型权重、平均训练损失以及第一个迭代的梯度信息\n",
    "def update_weights(model_weight, dataset, learning_rate, local_epoch):\n",
    "    model = mobilenetv2().to(device)\n",
    "    model.load_state_dict(model_weight)\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    Tensor_set = TensorDataset(torch.Tensor(dataset[0]).to(device), torch.Tensor(dataset[1]).to(device))\n",
    "    data_loader = DataLoader(Tensor_set, batch_size=128, shuffle=True)\n",
    "\n",
    "    first_iter_gradient = None  # 初始化变量来保存第一个iter的梯度\n",
    "\n",
    "    for iter in range(local_epoch):\n",
    "        batch_loss = []\n",
    "        for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss.append(loss.item()/images.shape[0])\n",
    "\n",
    "            # 保存第一个iter的梯度\n",
    "            if iter == 0 and batch_idx == 0:\n",
    "                first_iter_gradient = {}\n",
    "                for name, param in model.named_parameters():\n",
    "                    first_iter_gradient[name] = param.grad.clone()\n",
    "                # 保存 BatchNorm 层的 running mean 和 running variance\n",
    "                for name, module in model.named_modules():\n",
    "                    if isinstance(module, nn.BatchNorm2d):\n",
    "                        first_iter_gradient[name + '.running_mean'] = module.running_mean.clone()\n",
    "                        first_iter_gradient[name + '.running_var'] = module.running_var.clone()\n",
    "\n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "\n",
    "    return model.state_dict(), sum(epoch_loss) / len(epoch_loss), first_iter_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qsPZGD4Iem5w"
   },
   "outputs": [],
   "source": [
    "# 计算模型权重的差异，并根据学习率 lr 对权重差异进行缩放\n",
    "def weight_differences(n_w, p_w, lr):\n",
    "    w_diff = copy.deepcopy(n_w)\n",
    "    for key in w_diff.keys():\n",
    "        if 'num_batches_tracked' in key:\n",
    "            continue\n",
    "        w_diff[key] = (p_w[key] - n_w[key]) * lr\n",
    "    return w_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "040d862vbG9M"
   },
   "outputs": [],
   "source": [
    "# 也是本地训练，不过引入了本文的权重修正机制\n",
    "def update_weights_correction(model_weight, dataset, learning_rate, local_epoch, c_i, c_s):\n",
    "    model = mobilenetv2().to(device)\n",
    "    model.load_state_dict(model_weight)\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    Tensor_set = TensorDataset(torch.Tensor(dataset[0]).to(device), torch.Tensor(dataset[1]).to(device))\n",
    "    data_loader = DataLoader(Tensor_set, batch_size=200, shuffle=True)\n",
    "\n",
    "    for iter in range(local_epoch):\n",
    "        batch_loss = []\n",
    "        for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss.append(loss.sum().item()/images.shape[0])\n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "        corrected_graident = weight_differences(c_i, c_s, learning_rate)\n",
    "        orginal_model_weight = model.state_dict()\n",
    "        corrected_model_weight = weight_differences(corrected_graident, orginal_model_weight, 1)\n",
    "        model.load_state_dict(corrected_model_weight)\n",
    "\n",
    "    return model.state_dict(),  sum(epoch_loss) / len(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeFHXRuEo5Du"
   },
   "outputs": [],
   "source": [
    "def average_weights(w):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        if 'num_batches_tracked' in key:\n",
    "            continue\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], len(w))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLG9RrFffbl8"
   },
   "outputs": [],
   "source": [
    "#  Baseline:  仅在服务器端进行模型训练和评估，不依赖于客户端进行更新\n",
    "    test_model = mobilenetv2().to(device)\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    for round in tqdm(range(global_round)):\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        # Server side local training\n",
    "        server_data = select_subset(comb_client_data, server_percentage)\n",
    "        update_server_w, round_loss, _ = update_weights(train_w, server_data, gamma, E)\n",
    "        train_w = update_server_w\n",
    "        test_model.load_state_dict(train_w)\n",
    "        train_loss.append(round_loss)\n",
    "        # Test Accuracy\n",
    "        test_a = 0\n",
    "        for i in client_test_data:\n",
    "            ac = test_inference(test_model,i)[0]\n",
    "            test_a = test_a + ac\n",
    "        test_a = test_a/len(client_test_data)\n",
    "        test_acc.append(test_a)\n",
    "        # print(test_a)\n",
    "    return test_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGtAn28aok2c"
   },
   "outputs": [],
   "source": [
    "def fedavg(initial_w, global_round, eta, K, M):\n",
    "    test_model = mobilenetv2().to(device)\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    for round in tqdm(range(global_round)):\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "        for i in sampled_client:\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        train_w = average_weights(local_weights)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "        test_a = 0\n",
    "        for i in client_test_data:\n",
    "            ac = test_inference(test_model,i)[0]\n",
    "            test_a = test_a + ac\n",
    "        test_a = test_a/len(client_test_data)\n",
    "        test_acc.append(test_a)\n",
    "#         print(test_a)\n",
    "    return test_acc, train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzxW0sUxRGth"
   },
   "outputs": [],
   "source": [
    "def CLG_SGD(initial_w, global_round, eta, gamma, K, E, M):\n",
    "    test_model = mobilenetv2().to(device)\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    for round in tqdm(range(global_round)):\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "        for i in sampled_client:\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "        train_w = average_weights(local_weights)\n",
    "        # Server side local training\n",
    "        server_data = select_subset(comb_client_data, server_percentage)\n",
    "        update_server_w, round_loss, _ = update_weights(train_w, server_data, gamma, E)\n",
    "        train_w = update_server_w\n",
    "        local_loss.append(round_loss)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        test_a = 0\n",
    "        for i in client_test_data:\n",
    "            ac = test_inference(test_model,i)[0]\n",
    "            test_a = test_a + ac\n",
    "        test_a = test_a/len(client_test_data)\n",
    "        test_acc.append(test_a)\n",
    "#         print(test_a)\n",
    "    return test_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJTODAzxYJgA"
   },
   "outputs": [],
   "source": [
    "def Fed_C(initial_w, global_round, eta, gamma, K, E, M):\n",
    "    test_model = mobilenetv2().to(device)\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    for round in tqdm(range(global_round)):\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        g_i_list = []\n",
    "        server_data = select_subset(comb_client_data, server_percentage)\n",
    "        # Server gradient\n",
    "        _, _, g_s = update_weights(train_w, server_data, gamma, 1)\n",
    "\n",
    "        # Client gradient\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "        for i in sampled_client:\n",
    "            _, _, g_i = update_weights(train_w, client_data[i], eta, 1)\n",
    "            g_i_list.append(g_i)\n",
    "\n",
    "\n",
    "        # Client side local training\n",
    "        for i in range(len(sampled_client)):\n",
    "            update_client_w, client_round_loss = update_weights_correction(train_w, client_data[sampled_client[i]], eta, K, g_i_list[i], g_s)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "        train_w = average_weights(local_weights)\n",
    "        # Server side local training\n",
    "        update_server_w, round_loss, _ = update_weights(train_w, server_data, gamma, E)\n",
    "        train_w = update_server_w\n",
    "        local_loss.append(round_loss)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        test_a = 0\n",
    "        for i in client_test_data:\n",
    "            ac = test_inference(test_model,i)[0]\n",
    "            test_a = test_a + ac\n",
    "        test_a = test_a/len(client_test_data)\n",
    "        test_acc.append(test_a)\n",
    "    return test_acc, train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXqVbWZK7gLn"
   },
   "outputs": [],
   "source": [
    "def Fed_S(initial_w, global_round, eta, gamma, K, E, M):\n",
    "    test_model = mobilenetv2().to(device)\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    for round in tqdm(range(global_round)):\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        g_i_list = []\n",
    "        # Server gradient\n",
    "        server_data = select_subset(comb_client_data, server_percentage)\n",
    "        _, _, g_s = update_weights(train_w, server_data, gamma, 1)\n",
    "\n",
    "        # Client gradient\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "        for i in sampled_client:\n",
    "            _, _, g_i = update_weights(train_w, client_data[i], eta, 1)\n",
    "            g_i_list.append(g_i)\n",
    "\n",
    "\n",
    "        # Client side local training\n",
    "        for i in range(len(sampled_client)):\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[sampled_client[i]], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "        train_w = average_weights(local_weights)\n",
    "\n",
    "        # Server aggregation correction\n",
    "        g_i_average = average_weights(g_i_list)\n",
    "        correction_g = weight_differences(g_i_average, g_s, K*eta)\n",
    "        train_w = weight_differences(correction_g, copy.deepcopy(train_w), 1)\n",
    "\n",
    "\n",
    "        # Server side local training\n",
    "        update_server_w, round_loss, _ = update_weights(train_w, server_data, gamma, E)\n",
    "        train_w = update_server_w\n",
    "        local_loss.append(round_loss)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        test_a = 0\n",
    "        for i in client_test_data:\n",
    "            ac = test_inference(test_model,i)[0]\n",
    "            test_a = test_a + ac\n",
    "        test_a = test_a/len(client_test_data)\n",
    "        test_acc.append(test_a)\n",
    "    return test_acc, train_loss"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
