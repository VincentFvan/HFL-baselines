{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "gSK1TSekTVeu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.functional as func\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "9-WEXWakTwf8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # 解决由于多次加载 OpenMP 相关动态库而引起的冲突"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sns3blEITybc",
    "outputId": "120095fb-5597-4ada-8c12-b4470d8ad28e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 17 15:45:06 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:21:00.0 Off |                  Off |\n",
      "| 30%   25C    P8              14W / 350W |   3343MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off | 00000000:E1:00.0 Off |                  Off |\n",
      "| 30%   30C    P8              15W / 350W |     14MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      4557      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A    717162      C   /home/anaconda/envs/env8/bin/python        3326MiB |\n",
      "|    1   N/A  N/A      4557      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "96gqMCQneWho"
   },
   "outputs": [],
   "source": [
    "# MobileNetV2（比lenet更复杂的CNN网络）网络中的线性瓶颈结构，原文中用于CIFAR-100任务\n",
    "class LinearBottleNeck(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride, t=6, class_num=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * t, 1),\n",
    "            nn.BatchNorm2d(in_channels * t),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels * t, in_channels * t, 3, stride=stride, padding=1, groups=in_channels * t),\n",
    "            nn.BatchNorm2d(in_channels * t),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels * t, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.stride = stride\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = self.residual(x)\n",
    "\n",
    "        if self.stride == 1 and self.in_channels == self.out_channels:\n",
    "            residual += x\n",
    "\n",
    "        return residual\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "\n",
    "    def __init__(self, class_num=20):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pre = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.stage1 = LinearBottleNeck(32, 16, 1, 1)\n",
    "        self.stage2 = self._make_stage(2, 16, 24, 2, 6)\n",
    "        self.stage3 = self._make_stage(3, 24, 32, 2, 6)\n",
    "        self.stage4 = self._make_stage(4, 32, 64, 2, 6)\n",
    "        self.stage5 = self._make_stage(3, 64, 96, 1, 6)\n",
    "        self.stage6 = self._make_stage(3, 96, 160, 1, 6)\n",
    "        self.stage7 = LinearBottleNeck(160, 320, 1, 6)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(320, 1280, 1),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(1280, class_num, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "        x = self.stage6(x)\n",
    "        x = self.stage7(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _make_stage(self, repeat, in_channels, out_channels, stride, t):\n",
    "\n",
    "        layers = []\n",
    "        layers.append(LinearBottleNeck(in_channels, out_channels, stride, t))\n",
    "\n",
    "        while repeat - 1:\n",
    "            layers.append(LinearBottleNeck(out_channels, out_channels, 1, t))\n",
    "            repeat -= 1\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "def mobilenetv2():\n",
    "    return MobileNetV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "QkvtGtMuUDmr"
   },
   "outputs": [],
   "source": [
    "def test_inference(model, test):\n",
    "    \"\"\" Returns the test accuracy and loss.\n",
    "    \"\"\"\n",
    "    tensor_x = torch.Tensor(test[0]).to(device)\n",
    "    tensor_y = torch.Tensor(test[1]).to(device)\n",
    "    test_dataset = TensorDataset(tensor_x, tensor_y)\n",
    "\n",
    "    model.eval()\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    testloader = DataLoader(test_dataset, batch_size=128,\n",
    "                            shuffle=True)\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(testloader):\n",
    "        with torch.no_grad():  # 在测试过程中不需要计算梯度，节省内存和加速计算\n",
    "        # Inference\n",
    "            outputs = model(images)\n",
    "            batch_loss = criterion(outputs, labels.long())\n",
    "            loss += batch_loss.item() * labels.size(0) # 计算损失值，更好反映模型输出概率分布与真实标签的差距\n",
    "\n",
    "        # Prediction\n",
    "            _, pred_labels = torch.max(outputs, 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "            total += len(labels)\n",
    "    #print(correct,\"/\",total)\n",
    "    accuracy = correct/total\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "jURskA9VUJOF"
   },
   "outputs": [],
   "source": [
    "# 将CIFAR-100的100个类别转为20个类别（粒度更粗，降低任务复杂度）\n",
    "def sparse2coarse(targets):\n",
    "    \"\"\"Convert Pytorch CIFAR100 sparse targets to coarse targets.\n",
    "\n",
    "    Usage:\n",
    "        trainset = torchvision.datasets.CIFAR100(path)\n",
    "        trainset.targets = sparse2coarse(trainset.targets)\n",
    "    \"\"\"\n",
    "    coarse_labels = np.array([ 4,  1, 14,  8,  0,  6,  7,  7, 18,  3,\n",
    "                               3, 14,  9, 18,  7, 11,  3,  9,  7, 11,\n",
    "                               6, 11,  5, 10,  7,  6, 13, 15,  3, 15,\n",
    "                               0, 11,  1, 10, 12, 14, 16,  9, 11,  5,\n",
    "                               5, 19,  8,  8, 15, 13, 14, 17, 18, 10,\n",
    "                               16, 4, 17,  4,  2,  0, 17,  4, 18, 17,\n",
    "                               10, 3,  2, 12, 12, 16, 12,  1,  9, 19,\n",
    "                               2, 10,  0,  1, 16, 12,  9, 13, 15, 13,\n",
    "                              16, 19,  2,  4,  6, 19,  5,  5,  8, 19,\n",
    "                              18,  1,  2, 15,  6,  0, 17,  8, 14, 13])\n",
    "    return coarse_labels[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "jcZ7eQGET_YJ"
   },
   "outputs": [],
   "source": [
    "# 共有6w个图像，其中5w训练，1w测试\n",
    "def CIFAR100():\n",
    "    '''Return Cifar100\n",
    "    '''\n",
    "    train_dataset = torchvision.datasets.CIFAR100(root='../data/CIFAR-100',\n",
    "                                            train=True,\n",
    "                                            transform=transforms.ToTensor(),\n",
    "                                            download=True)\n",
    "    test_dataset = torchvision.datasets.CIFAR100(root='../data/CIFAR-100',\n",
    "                                            train=False,\n",
    "                                            transform=transforms.ToTensor(),\n",
    "                                            download=True)\n",
    "    total_img,total_label = [],[]\n",
    "    for imgs,labels in train_dataset:\n",
    "        total_img.append(imgs.numpy())\n",
    "        total_label.append(labels)\n",
    "    for imgs,labels in test_dataset:\n",
    "        total_img.append(imgs.numpy())\n",
    "        total_label.append(labels) \n",
    "    total_img = np.array(total_img)\n",
    "    total_label = np.array(sparse2coarse(total_label))\n",
    "\n",
    "    cifar = [total_img, total_label]\n",
    "    return cifar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "f1eQhNtPUMOF"
   },
   "outputs": [],
   "source": [
    "# 基于 Dirichlet 分布 来模拟non-IID。返回一个形状为 (client_num, class_num) 的概率矩阵，每一行代表一个客户端对各类别的概率分布。\n",
    "def get_prob(non_iid, client_num, class_num = 20):\n",
    "    return np.random.dirichlet(np.repeat(non_iid, class_num), client_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "npT3idE-UaGm"
   },
   "outputs": [],
   "source": [
    "def create_data(prob, size_per_client, dataset, N=20):\n",
    "    total_each_class = size_per_client * np.sum(prob, 0)\n",
    "    data, label = dataset\n",
    "\n",
    "\n",
    "    # 为每个类别随机采样数据\n",
    "    all_class_set = []\n",
    "    for i in range(N):\n",
    "        size = total_each_class[i]\n",
    "        sub_data = data[label == i]\n",
    "        sub_label = label[label == i]\n",
    "\n",
    "        rand_indx = np.random.choice(len(sub_data), size=int(size), replace=False).astype(int)\n",
    "        sub2_data, sub2_label = sub_data[rand_indx], sub_label[rand_indx]\n",
    "        all_class_set.append((sub2_data, sub2_label))\n",
    "\n",
    "    index = [0] * N\n",
    "    clients, test = [], []\n",
    "\n",
    "    for m in range(prob.shape[0]):  # 遍历客户端\n",
    "        labels, images = [], []  # 训练数据\n",
    "        tlabels, timages = [], [] # 测试数据\n",
    "\n",
    "        # TODO_241216：这里每个client的测试集和它的训练集分布相同，并且最后测试时，也是计算所有client中的准确率的平均值\n",
    "        # TODO_241216：别的FL方法也是这样做的吗？我也要这样做吗？\n",
    "        for n in range(N):\n",
    "            # 80%用于训练，20%用于测试\n",
    "            # 这里的int向下取整，会导致实际的数据量比计算略小\n",
    "            start, end = index[n], index[n] + int(prob[m][n] * size_per_client * 0.8)\n",
    "            test_start, test_end = end, index[n] + int(prob[m][n] * size_per_client)\n",
    "\n",
    "            image, label = all_class_set[n][0][start:end], all_class_set[n][1][start:end]\n",
    "            test_image, test_label = all_class_set[n][0][test_start:test_end], all_class_set[n][1][test_start:test_end]\n",
    "\n",
    "            # 记录当前类别的数据分配进度\n",
    "            index[n] += int(prob[m][n] * size_per_client)\n",
    "\n",
    "            labels.extend(label)\n",
    "            images.extend(image)\n",
    "\n",
    "            tlabels.extend(test_label)\n",
    "            timages.extend(test_image)\n",
    "\n",
    "        clients.append((np.array(images), np.array(labels)))\n",
    "        test.append((np.array(timages), np.array(tlabels)))\n",
    "\n",
    "    return clients, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "DCsR6_QqUzJ8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 合并所有客户端的测试数据 （上面讲测试数据分成了不同的客户端）\n",
    "# 但并没有使用，用途不明\n",
    "def comb_client_test_func(client_test_data):\n",
    "    comb_client_test_image = []\n",
    "    comb_client_test_label = []\n",
    "    for i in range(client_num):\n",
    "        comb_client_test_image.extend(list(client_test_data[i][0]))\n",
    "        comb_client_test_label.extend(list(client_test_data[i][1]))\n",
    "    \n",
    "    # 将测试图片和标签合并为 numpy 数组\n",
    "    comb_client_test_image = np.array(comb_client_test_image)\n",
    "    comb_client_test_label = np.array(comb_client_test_label)\n",
    "    \n",
    "    label_count = Counter(comb_client_test_label)\n",
    "    print(\"测试集类别分布：\")\n",
    "    for label, count in sorted(label_count.items()):\n",
    "        print(f\"类别 {label}: {count} 个样本\")\n",
    "    \n",
    "    return [comb_client_test_image, comb_client_test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "JEKzDM0yW3DW"
   },
   "outputs": [],
   "source": [
    "# 从数据集中按类别均匀抽取子集，并按照指定的比例 percentage 进行缩减，同时对数据进行随机打乱\n",
    "def select_subset(whole_set, percentage):\n",
    "    a = whole_set[0]\n",
    "    b = whole_set[1]\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(\"Both arrays should have the same length.\")\n",
    "\n",
    "    if not 0 <= percentage <= 1:\n",
    "        raise ValueError(\"Percentage must be between 0 and 1.\")\n",
    "\n",
    "    unique_classes = np.unique(b)\n",
    "\n",
    "    a_prime = []\n",
    "    b_prime = []\n",
    "\n",
    "    for cls in unique_classes:\n",
    "        indices = np.where(b == cls)[0]\n",
    "        subset_size = int(len(indices) * percentage)\n",
    "\n",
    "        selected_indices = np.random.choice(indices, subset_size, replace=False)\n",
    "\n",
    "        a_prime.extend(a[selected_indices])\n",
    "        b_prime.extend(b[selected_indices])\n",
    "\n",
    "    a_prime, b_prime = np.array(a_prime), np.array(b_prime)\n",
    "\n",
    "    # Shuffle arrays to randomize the order of elements\n",
    "    shuffle_indices = np.random.permutation(len(a_prime))\n",
    "    a_prime, b_prime = a_prime[shuffle_indices], b_prime[shuffle_indices]\n",
    "\n",
    "    return [a_prime, b_prime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Traning Client Total: 30209 1629 1556 1799 1697 1466 1692 1576 1488 1456 1427 1605 1323 1511 1483 1501 1266 1205 1689 1386 1454\n",
      "Client 0: 150 6 2 0 8 2 58 0 1 2 1 0 13 0 0 25 0 2 20 5 5\n",
      "Client 1: 149 12 31 10 2 7 0 0 4 2 16 5 9 5 18 1 17 3 4 3 0\n",
      "Client 2: 152 7 2 2 5 0 0 26 7 2 0 34 7 27 3 10 2 0 18 0 0\n",
      "Client 3: 151 10 2 0 0 2 8 2 9 2 18 3 0 27 0 0 3 0 5 19 41\n",
      "Client 4: 148 1 15 3 16 1 9 6 0 5 0 26 0 0 8 2 2 35 10 3 6\n",
      "Client 5: 152 12 0 0 5 18 17 77 8 0 0 1 0 0 1 0 2 0 5 5 1\n",
      "Client 6: 150 21 3 17 0 0 0 5 30 22 0 8 4 3 23 0 4 2 0 7 1\n",
      "Client 7: 151 2 12 17 26 10 0 0 6 12 0 1 46 1 7 0 8 0 0 3 0\n",
      "Client 8: 151 1 0 25 0 0 1 37 9 6 0 0 22 4 1 10 3 4 15 7 6\n",
      "Client 9: 151 0 12 2 10 3 15 9 16 25 0 0 4 0 2 0 13 14 5 20 1\n",
      "Client 10: 150 22 1 3 10 7 5 24 6 2 17 1 0 0 23 3 0 8 1 1 16\n",
      "Client 11: 151 47 0 0 0 20 0 4 0 1 0 20 10 0 23 9 2 4 1 9 1\n",
      "Client 12: 151 11 2 18 0 8 27 2 0 0 6 0 3 11 1 2 0 31 17 12 0\n",
      "Client 13: 152 0 1 0 20 1 3 11 21 10 29 2 0 30 9 8 1 0 0 5 1\n",
      "Client 14: 151 3 7 3 0 6 0 29 37 14 1 7 4 1 4 4 7 4 1 1 18\n",
      "Client 15: 149 0 19 27 0 17 2 19 7 0 0 6 6 7 0 1 17 5 2 14 0\n",
      "Client 16: 150 1 39 2 51 5 0 6 0 0 0 0 0 6 1 6 6 0 4 23 0\n",
      "Client 17: 152 17 6 28 0 0 0 1 2 0 0 37 0 3 13 0 13 6 19 7 0\n",
      "Client 18: 149 7 9 15 0 8 0 4 0 0 0 3 14 0 56 14 6 11 0 2 0\n",
      "Client 19: 152 0 0 1 17 12 0 0 0 19 0 38 0 0 2 47 0 1 1 11 3\n",
      "Client 20: 150 40 1 19 0 0 13 5 3 2 14 1 6 13 9 0 2 9 7 0 6\n",
      "Client 21: 151 7 7 5 13 1 21 5 2 4 0 0 9 0 7 7 20 0 43 0 0\n",
      "Client 22: 151 0 0 46 2 8 5 3 21 5 8 2 17 16 1 0 0 0 15 1 1\n",
      "Client 23: 151 28 0 15 7 6 18 15 7 12 0 0 2 14 1 3 6 5 0 10 2\n",
      "Client 24: 150 0 1 5 26 1 22 0 0 15 0 19 5 1 33 1 0 4 6 4 7\n",
      "Client 25: 153 2 2 0 5 2 32 10 0 22 0 49 0 7 2 1 3 8 8 0 0\n",
      "Client 26: 153 5 10 31 0 3 17 0 1 9 10 2 12 23 7 17 6 0 0 0 0\n",
      "Client 27: 150 14 1 0 0 1 14 21 38 6 6 2 1 23 1 18 1 2 1 0 0\n",
      "Client 28: 150 12 10 24 3 6 10 8 0 1 10 33 2 5 1 11 6 0 1 7 0\n",
      "Client 29: 149 6 3 10 20 8 1 1 0 10 6 0 0 3 15 36 0 16 3 9 2\n",
      "Client 30: 150 0 7 20 3 5 4 6 2 2 7 9 0 20 11 1 2 0 6 27 18\n",
      "Client 31: 151 9 1 7 7 4 9 31 0 1 4 0 0 0 1 0 17 5 1 46 8\n",
      "Client 32: 150 10 11 1 0 24 0 0 1 23 6 20 3 0 1 1 22 2 1 16 8\n",
      "Client 33: 150 1 23 2 17 0 40 1 1 0 3 2 0 0 0 28 14 0 6 9 3\n",
      "Client 34: 152 1 7 28 0 5 3 4 18 28 14 0 2 11 6 9 11 0 5 0 0\n",
      "Client 35: 153 19 0 14 0 1 2 0 21 20 24 3 3 11 4 2 10 0 19 0 0\n",
      "Client 36: 151 0 19 1 0 0 2 3 30 10 0 41 1 4 8 6 10 0 5 11 0\n",
      "Client 37: 153 3 14 6 11 2 19 9 20 16 6 1 0 0 4 0 0 0 0 14 28\n",
      "Client 38: 150 24 1 16 0 0 3 6 0 5 3 15 0 0 13 0 11 0 15 9 29\n",
      "Client 39: 150 0 0 16 0 6 30 6 7 9 0 1 41 0 0 15 0 9 3 1 6\n",
      "Client 40: 151 7 34 5 4 20 0 10 5 4 4 11 1 9 0 3 29 3 1 0 1\n",
      "Client 41: 151 12 0 21 12 1 4 0 1 1 18 14 0 24 17 4 11 1 0 8 2\n",
      "Client 42: 149 5 0 2 13 2 1 0 4 0 33 21 0 0 13 6 5 13 20 9 2\n",
      "Client 43: 150 29 0 0 0 7 2 1 2 0 6 0 3 31 8 12 37 11 0 1 0\n",
      "Client 44: 150 5 0 50 1 10 4 0 8 0 2 5 0 0 0 1 0 3 29 3 29\n",
      "Client 45: 149 15 0 13 0 2 2 4 25 8 20 0 1 0 0 1 18 30 4 2 4\n",
      "Client 46: 153 12 1 1 0 4 2 9 1 20 0 2 2 11 11 46 15 0 16 0 0\n",
      "Client 47: 152 2 19 2 0 12 19 2 0 0 9 6 0 0 5 0 4 3 49 4 16\n",
      "Client 48: 149 3 0 0 0 9 32 1 0 6 22 0 10 0 27 22 7 0 2 0 8\n",
      "Client 49: 150 1 30 39 0 4 0 1 2 3 5 4 5 1 11 7 0 1 35 1 0\n",
      "Client 50: 149 0 1 16 5 7 0 1 0 2 13 7 1 0 16 51 9 9 10 1 0\n",
      "Client 51: 154 63 2 0 8 0 10 2 0 0 4 35 1 0 14 0 0 0 1 10 4\n",
      "Client 52: 152 22 0 0 7 1 5 1 1 7 2 1 38 0 30 0 7 16 14 0 0\n",
      "Client 53: 150 7 0 8 35 0 3 0 14 0 6 0 4 13 1 0 5 0 3 30 21\n",
      "Client 54: 151 6 0 1 9 5 0 0 21 0 53 3 2 0 20 1 12 6 11 0 1\n",
      "Client 55: 151 1 51 7 6 2 7 1 11 1 0 7 0 45 2 1 2 1 3 3 0\n",
      "Client 56: 148 1 18 0 2 1 7 9 21 0 0 25 0 12 0 22 0 0 5 4 21\n",
      "Client 57: 154 3 7 72 0 1 3 1 0 2 14 6 1 16 0 7 0 14 1 0 6\n",
      "Client 58: 152 10 1 0 1 1 0 0 16 32 0 10 6 1 4 17 15 1 0 37 0\n",
      "Client 59: 151 1 15 0 7 10 8 1 9 1 22 1 0 4 14 1 7 0 4 33 13\n",
      "Client 60: 150 14 4 0 26 26 0 17 0 4 3 0 0 1 6 4 16 13 9 2 5\n",
      "Client 61: 149 5 1 0 6 5 2 5 59 6 0 2 3 4 2 20 7 16 1 1 4\n",
      "Client 62: 154 0 6 0 23 7 28 0 0 0 23 20 8 7 1 11 5 0 7 8 0\n",
      "Client 63: 152 3 1 18 2 5 12 0 0 30 3 2 3 0 0 8 6 13 0 13 33\n",
      "Client 64: 153 14 15 4 16 1 4 10 7 6 0 2 14 2 9 9 1 39 0 0 0\n",
      "Client 65: 150 1 0 2 20 11 0 4 4 7 7 0 6 0 12 18 23 24 4 2 5\n",
      "Client 66: 151 19 3 0 6 32 0 17 2 10 23 0 3 5 3 1 2 10 4 8 3\n",
      "Client 67: 150 1 20 6 2 14 0 17 23 3 0 41 1 10 4 2 3 1 1 0 1\n",
      "Client 68: 151 2 7 2 12 24 20 3 0 17 1 4 0 8 0 27 0 11 2 6 5\n",
      "Client 69: 154 0 0 4 3 34 2 4 2 16 22 12 0 0 2 17 0 12 7 2 15\n",
      "Client 70: 150 8 15 0 6 12 17 11 15 13 17 1 0 0 2 1 4 1 0 22 5\n",
      "Client 71: 151 7 0 3 12 2 0 3 0 5 16 3 1 2 27 15 0 4 7 41 3\n",
      "Client 72: 152 11 14 0 0 68 5 13 4 3 0 1 0 0 3 0 13 2 11 3 1\n",
      "Client 73: 151 23 0 0 0 13 0 9 0 4 0 21 4 17 28 4 0 0 6 22 0\n",
      "Client 74: 148 2 2 24 10 1 26 0 4 7 2 5 13 6 6 15 3 4 1 3 14\n",
      "Client 75: 150 12 18 47 9 4 0 11 2 1 40 0 0 2 0 0 3 0 0 0 1\n",
      "Client 76: 155 3 0 21 0 11 0 2 14 0 1 8 0 12 13 0 7 24 39 0 0\n",
      "Client 77: 151 1 17 14 34 1 3 0 0 0 0 15 0 8 2 3 13 0 29 11 0\n",
      "Client 78: 152 29 11 0 43 0 0 5 8 1 0 18 22 6 0 0 6 0 2 0 1\n",
      "Client 79: 153 0 7 0 13 14 0 0 0 0 21 1 53 22 0 0 1 13 0 7 1\n",
      "Client 80: 154 24 5 1 4 0 24 5 1 1 23 2 0 17 15 1 5 8 11 1 6\n",
      "Client 81: 152 3 6 11 3 0 0 0 8 4 0 16 0 10 2 0 7 14 23 1 44\n",
      "Client 82: 151 6 10 0 0 11 0 1 1 9 10 0 7 1 1 9 73 5 1 6 0\n",
      "Client 83: 151 7 8 2 3 40 14 20 2 2 2 1 10 9 1 9 0 0 12 0 9\n",
      "Client 84: 152 3 1 1 1 0 31 1 0 12 0 2 2 21 2 4 10 12 0 1 48\n",
      "Client 85: 152 34 6 8 0 19 0 2 9 2 10 0 0 9 3 13 0 4 16 15 2\n",
      "Client 86: 149 6 1 63 10 1 7 7 1 2 14 3 1 1 16 5 7 0 4 0 0\n",
      "Client 87: 153 8 0 9 0 24 5 0 15 14 14 16 0 2 0 3 0 0 3 27 13\n",
      "Client 88: 149 7 3 1 15 15 4 0 11 7 2 15 2 25 4 2 3 2 14 1 16\n",
      "Client 89: 153 25 9 1 19 2 22 0 5 0 4 2 0 0 0 34 10 0 1 0 19\n",
      "Client 90: 151 22 1 1 3 1 16 2 2 3 0 2 4 3 10 19 0 6 49 0 7\n",
      "Client 91: 149 0 11 4 1 3 28 4 0 16 2 0 9 4 1 0 1 10 5 16 34\n",
      "Client 92: 151 21 9 13 0 13 1 11 19 7 0 36 3 1 3 1 5 4 0 3 1\n",
      "Client 93: 152 26 12 2 0 0 12 6 14 1 0 8 0 1 2 15 4 2 0 12 35\n",
      "Client 94: 150 1 14 0 14 0 14 2 1 4 0 2 14 2 10 9 8 28 20 0 7\n",
      "Client 95: 152 0 1 0 0 26 10 4 15 0 4 2 0 2 5 59 4 7 4 6 3\n",
      "Client 96: 153 2 7 14 13 1 21 0 21 1 6 0 7 1 12 3 1 3 11 0 29\n",
      "Client 97: 151 3 4 26 10 5 0 0 13 21 17 5 0 6 26 0 3 5 2 0 5\n",
      "Client 98: 150 16 0 0 8 7 17 9 0 0 9 0 1 3 18 0 8 0 48 6 0\n",
      "Client 99: 152 33 0 0 0 5 5 0 3 20 4 7 1 10 1 7 0 0 0 0 56\n",
      "Client 100: 150 3 0 1 5 0 6 0 4 2 0 2 36 2 4 0 0 0 33 2 50\n",
      "Client 101: 153 4 15 22 0 0 11 10 6 1 14 0 23 2 0 0 6 1 28 1 9\n",
      "Client 102: 148 0 7 40 13 0 5 3 1 0 41 5 3 0 8 2 11 1 4 0 4\n",
      "Client 103: 152 0 4 2 32 9 1 0 24 7 10 7 0 14 5 4 24 0 5 4 0\n",
      "Client 104: 152 0 14 13 12 2 0 7 0 16 4 26 14 3 0 3 1 26 8 0 3\n",
      "Client 105: 155 4 10 3 2 0 1 0 3 45 0 13 6 3 0 24 9 24 5 0 3\n",
      "Client 106: 152 5 1 2 4 0 29 3 3 30 28 0 0 0 3 0 0 0 19 0 25\n",
      "Client 107: 153 5 21 0 72 0 0 9 1 3 2 0 5 11 3 8 9 4 0 0 0\n",
      "Client 108: 151 1 4 5 5 0 0 4 33 0 7 2 23 1 16 13 0 17 7 13 0\n",
      "Client 109: 152 0 0 3 0 0 12 3 18 4 7 9 13 6 2 5 46 4 15 0 5\n",
      "Client 110: 152 0 50 0 2 0 5 7 19 0 10 16 0 5 8 9 0 0 14 7 0\n",
      "Client 111: 150 2 0 7 0 1 1 3 7 4 0 10 38 7 9 1 1 2 0 51 6\n",
      "Client 112: 149 1 8 1 3 7 7 0 28 28 0 18 3 8 1 0 1 2 9 8 16\n",
      "Client 113: 153 32 4 0 20 8 3 25 10 0 0 0 6 1 10 6 1 3 17 3 4\n",
      "Client 114: 153 0 14 26 22 0 0 21 1 22 4 2 0 10 0 0 0 10 0 18 3\n",
      "Client 115: 153 1 1 46 2 5 14 33 15 1 6 0 9 10 0 0 0 6 3 0 1\n",
      "Client 116: 151 0 7 9 14 0 6 0 15 2 2 32 0 4 14 0 1 17 21 2 5\n",
      "Client 117: 151 8 36 0 10 0 0 2 2 34 0 0 0 0 13 2 12 18 8 3 3\n",
      "Client 118: 150 3 0 20 23 5 0 0 0 0 17 2 5 20 1 5 2 16 23 8 0\n",
      "Client 119: 150 0 0 0 7 2 25 0 4 9 15 0 2 15 3 0 16 20 3 0 29\n",
      "Client 120: 152 59 3 17 8 16 4 0 10 16 11 1 1 0 5 0 0 1 0 0 0\n",
      "Client 121: 153 0 0 38 0 0 22 17 1 0 20 2 17 1 7 21 1 6 0 0 0\n",
      "Client 122: 150 0 1 0 15 1 19 9 1 5 0 1 22 36 0 2 17 20 0 0 1\n",
      "Client 123: 152 19 4 0 0 4 2 5 5 9 15 26 2 0 0 8 0 23 14 13 3\n",
      "Client 124: 151 0 0 2 52 9 6 13 3 0 3 0 3 19 1 17 0 6 1 0 16\n",
      "Client 125: 151 7 5 0 7 0 2 7 3 2 17 1 32 0 0 11 6 1 46 3 1\n",
      "Client 126: 151 26 0 8 7 0 9 0 0 8 0 1 6 47 10 6 0 6 0 0 17\n",
      "Client 127: 153 2 8 23 22 7 0 7 15 4 8 27 0 3 0 0 15 9 0 3 0\n",
      "Client 128: 152 1 8 42 7 0 10 0 2 7 12 25 0 0 0 5 0 5 0 11 17\n",
      "Client 129: 151 0 8 5 38 52 0 1 1 8 0 1 0 0 2 7 5 0 1 13 9\n",
      "Client 130: 151 21 16 5 0 10 5 10 2 0 2 0 23 24 14 0 5 5 4 3 2\n",
      "Client 131: 151 4 3 26 1 5 1 23 6 0 0 8 4 0 2 8 1 17 12 20 10\n",
      "Client 132: 152 2 0 0 0 4 0 3 1 12 14 0 2 46 22 0 5 0 18 23 0\n",
      "Client 133: 152 1 1 16 65 6 7 0 0 0 3 25 0 0 0 1 21 4 0 2 0\n",
      "Client 134: 154 0 0 16 0 14 2 42 15 3 0 18 2 0 0 11 0 7 18 1 5\n",
      "Client 135: 153 24 0 8 2 21 15 2 1 0 6 3 4 3 8 0 6 0 1 19 30\n",
      "Client 136: 150 3 7 14 19 12 0 5 6 2 6 2 3 30 1 13 8 4 10 0 5\n",
      "Client 137: 150 0 6 13 2 1 4 24 5 11 7 7 8 8 8 15 4 0 12 3 12\n",
      "Client 138: 151 0 23 1 5 0 1 16 0 4 1 4 0 9 8 43 8 1 6 6 15\n",
      "Client 139: 149 3 8 1 1 3 0 53 3 1 1 0 1 0 25 0 25 0 24 0 0\n",
      "Client 140: 151 1 4 4 11 2 32 0 0 9 1 11 0 1 6 0 37 1 12 3 16\n",
      "Client 141: 152 4 30 8 1 8 4 4 4 2 2 7 12 0 12 0 4 0 2 9 39\n",
      "Client 142: 152 20 17 7 6 0 3 3 3 4 5 13 45 0 2 10 0 13 1 0 0\n",
      "Client 143: 150 0 6 2 1 0 0 20 8 32 0 0 16 3 26 6 4 4 19 1 2\n",
      "Client 144: 149 11 28 1 11 1 13 4 0 15 2 15 4 0 3 0 7 5 25 0 4\n",
      "Client 145: 152 1 6 0 8 0 0 0 20 11 1 27 9 30 0 19 0 0 19 1 0\n",
      "Client 146: 150 0 2 46 0 0 0 29 0 5 22 13 15 0 0 2 5 0 10 1 0\n",
      "Client 147: 153 1 15 14 0 0 18 36 12 2 39 1 3 0 0 0 7 0 5 0 0\n",
      "Client 148: 151 19 0 1 19 24 1 0 3 10 0 1 4 0 8 6 0 46 1 6 2\n",
      "Client 149: 149 2 22 0 10 0 1 1 4 1 7 0 1 17 3 38 3 1 15 20 3\n",
      "Client 150: 150 5 0 4 7 8 19 3 4 3 11 1 0 16 1 7 4 22 13 15 7\n",
      "Client 151: 151 0 0 28 9 10 11 4 18 5 0 9 20 20 0 3 13 0 0 0 1\n",
      "Client 152: 150 0 3 9 6 45 4 0 14 6 3 0 17 10 1 16 0 4 7 0 5\n",
      "Client 153: 152 33 10 0 17 13 0 34 16 0 3 1 9 2 3 0 0 2 0 3 6\n",
      "Client 154: 147 6 8 0 0 2 7 4 12 2 6 35 3 15 19 3 4 2 0 3 16\n",
      "Client 155: 151 1 7 0 5 5 1 1 4 0 10 1 24 50 20 0 6 0 1 13 2\n",
      "Client 156: 151 11 0 5 18 25 2 22 4 0 0 14 32 3 2 0 2 6 5 0 0\n",
      "Client 157: 149 5 20 0 1 0 0 3 25 6 11 27 0 1 15 9 0 0 0 17 9\n",
      "Client 158: 149 1 37 1 39 0 1 5 1 1 2 8 3 0 0 0 7 24 0 10 9\n",
      "Client 159: 152 17 13 1 0 1 7 5 23 0 7 0 14 9 33 0 0 4 13 5 0\n",
      "Client 160: 150 0 50 10 1 8 7 20 21 5 3 0 0 0 1 2 0 2 1 1 18\n",
      "Client 161: 151 31 8 0 0 3 11 14 18 4 3 8 0 13 1 18 0 2 1 0 16\n",
      "Client 162: 151 14 0 30 10 1 0 0 3 0 0 5 1 5 3 0 13 2 33 8 23\n",
      "Client 163: 152 1 4 0 0 22 2 0 2 31 3 9 0 36 2 27 11 0 1 1 0\n",
      "Client 164: 153 6 19 6 0 0 26 0 5 46 1 0 0 5 5 1 16 3 5 9 0\n",
      "Client 165: 151 7 5 8 12 21 1 13 5 0 2 0 0 33 2 0 2 29 4 5 2\n",
      "Client 166: 152 1 68 0 9 1 34 0 0 4 0 1 2 1 0 2 0 7 0 22 0\n",
      "Client 167: 148 0 18 4 2 4 4 3 3 3 2 21 12 10 12 2 0 17 1 29 1\n",
      "Client 168: 148 28 0 2 1 16 3 4 0 20 0 8 2 19 5 23 0 0 0 16 1\n",
      "Client 169: 151 13 1 9 1 0 5 9 3 5 0 9 3 0 10 31 1 12 9 21 9\n",
      "Client 170: 151 1 1 51 0 1 1 9 15 12 2 0 2 13 0 14 6 4 1 0 18\n",
      "Client 171: 150 0 0 3 1 4 50 0 0 9 5 9 11 1 26 3 0 0 16 10 2\n",
      "Client 172: 151 9 28 0 2 0 2 13 5 5 1 3 0 43 0 15 12 7 1 3 2\n",
      "Client 173: 153 9 3 0 60 1 1 8 0 6 0 1 9 7 21 12 4 0 2 9 0\n",
      "Client 174: 150 6 0 0 23 0 10 4 4 0 6 19 1 9 15 0 1 0 0 36 16\n",
      "Client 175: 149 16 17 8 4 1 7 2 3 5 11 0 3 4 0 0 1 10 48 7 2\n",
      "Client 176: 150 19 4 3 0 0 8 5 4 36 1 7 43 0 1 0 0 1 18 0 0\n",
      "Client 177: 151 7 0 0 0 35 0 27 0 0 22 17 0 0 0 0 1 1 26 7 8\n",
      "Client 178: 152 4 9 11 2 53 1 19 21 11 0 1 2 11 0 1 4 2 0 0 0\n",
      "Client 179: 152 1 1 1 4 0 4 15 2 27 5 0 0 21 1 11 2 1 48 3 5\n",
      "Client 180: 152 8 0 0 18 5 10 6 8 0 0 11 21 1 21 0 0 0 34 0 9\n",
      "Client 181: 151 12 0 3 0 39 13 8 0 13 18 0 2 3 0 27 2 0 7 4 0\n",
      "Client 182: 151 0 2 0 5 5 0 16 5 35 1 16 15 9 10 1 0 2 1 6 22\n",
      "Client 183: 151 10 5 1 6 0 3 35 1 5 6 14 1 5 3 0 19 33 0 3 1\n",
      "Client 184: 148 18 17 9 3 1 4 5 1 22 3 8 3 4 44 0 0 0 0 2 4\n",
      "Client 185: 151 1 2 12 0 9 1 30 1 4 0 15 0 0 3 7 27 5 2 32 0\n",
      "Client 186: 149 0 5 6 30 3 35 3 3 5 5 5 8 0 0 25 2 9 3 0 2\n",
      "Client 187: 152 0 2 1 14 26 9 19 22 16 0 1 27 0 2 1 0 2 0 10 0\n",
      "Client 188: 153 0 7 11 5 12 29 32 3 4 23 3 0 2 1 0 3 1 17 0 0\n",
      "Client 189: 151 1 0 7 9 41 26 13 3 0 5 8 0 5 14 0 3 0 1 14 1\n",
      "Client 190: 152 0 9 2 1 0 3 23 1 1 28 20 0 0 27 10 1 5 0 2 19\n",
      "Client 191: 152 11 0 0 28 0 1 12 0 3 0 30 1 0 1 23 14 4 9 4 11\n",
      "Client 192: 152 1 3 0 13 1 25 3 0 4 14 2 3 6 43 4 17 0 0 10 3\n",
      "Client 193: 153 18 0 10 0 0 14 2 3 6 48 3 0 8 13 0 6 0 8 14 0\n",
      "Client 194: 151 12 1 16 8 24 11 1 17 1 0 2 33 4 3 0 5 6 0 0 7\n",
      "Client 195: 152 0 0 7 7 8 8 6 33 3 7 0 9 18 24 0 20 0 0 2 0\n",
      "Client 196: 150 13 0 8 6 0 8 0 7 0 28 20 0 0 25 0 6 8 11 2 8\n",
      "Client 197: 149 2 0 15 1 0 3 0 41 0 2 8 11 6 4 13 0 9 7 23 4\n",
      "Client 198: 151 2 8 1 2 3 49 2 11 16 5 5 14 0 3 8 5 2 2 7 6\n",
      "Client 199: 152 0 25 0 0 3 35 5 4 5 0 11 24 1 5 3 0 0 0 4 27\n",
      "Client 0 Test: 42 2 1 0 3 1 15 0 0 0 0 1 3 0 0 7 0 0 5 2 2\n",
      "Client 1 Test: 42 3 8 3 1 2 0 0 2 0 4 2 2 1 5 0 4 1 2 1 1\n",
      "Client 2 Test: 40 2 0 1 2 0 0 7 2 1 0 9 1 7 1 2 0 1 4 0 0\n",
      "Client 3 Test: 41 2 0 0 1 1 3 0 2 1 4 1 0 7 0 0 1 0 2 5 11\n",
      "Client 4 Test: 41 0 4 0 4 1 3 2 0 2 0 7 0 0 3 0 1 9 2 1 2\n",
      "Client 5 Test: 41 3 0 1 1 4 4 20 3 0 0 0 0 0 0 0 1 1 1 2 0\n",
      "Client 6 Test: 40 6 1 4 0 0 1 1 7 6 0 2 1 1 6 0 1 1 0 2 0\n",
      "Client 7 Test: 41 1 3 4 7 3 0 0 1 4 0 1 12 0 2 0 3 0 0 0 0\n",
      "Client 8 Test: 40 0 1 6 0 0 0 10 2 2 1 0 6 1 0 2 1 1 3 2 2\n",
      "Client 9 Test: 40 0 3 1 2 1 4 2 5 7 0 0 1 0 1 0 3 4 1 5 0\n",
      "Client 10 Test: 40 6 0 1 3 2 1 7 1 0 4 1 0 0 5 1 1 2 0 1 4\n",
      "Client 11 Test: 40 11 1 1 0 5 0 1 0 0 0 5 3 1 6 2 0 1 0 2 1\n",
      "Client 12 Test: 40 3 1 5 0 2 7 1 0 0 1 0 1 3 0 0 0 8 4 3 1\n",
      "Client 13 Test: 38 0 0 0 5 0 0 2 5 2 8 1 0 7 3 2 1 0 0 2 0\n",
      "Client 14 Test: 40 1 2 1 0 2 0 8 10 4 0 1 1 0 1 1 1 1 1 0 5\n",
      "Client 15 Test: 41 0 5 7 0 5 1 5 2 0 0 1 2 2 0 0 5 2 1 3 0\n",
      "Client 16 Test: 42 0 10 0 13 2 0 2 0 0 1 0 0 2 0 2 2 0 2 6 0\n",
      "Client 17 Test: 38 5 1 7 0 0 0 0 0 0 0 9 1 0 3 0 3 2 5 2 0\n",
      "Client 18 Test: 41 2 3 4 0 3 0 1 0 0 0 1 3 0 15 4 2 3 0 0 0\n",
      "Client 19 Test: 42 0 0 0 4 4 0 0 1 4 1 10 0 0 1 12 0 0 1 3 1\n",
      "Client 20 Test: 42 11 0 5 0 0 3 1 1 1 4 0 2 3 3 1 1 2 2 0 2\n",
      "Client 21 Test: 40 2 2 1 4 1 5 2 0 1 0 0 2 0 2 2 5 0 11 0 0\n",
      "Client 22 Test: 40 0 0 12 0 2 2 1 5 2 2 1 4 4 1 0 0 0 4 0 0\n",
      "Client 23 Test: 41 7 0 3 2 2 5 4 2 4 0 0 0 4 1 1 1 2 0 2 1\n",
      "Client 24 Test: 41 0 0 1 7 0 5 0 0 4 0 5 1 1 9 1 0 2 2 1 2\n",
      "Client 25 Test: 39 0 0 0 2 1 8 2 0 6 0 12 0 2 1 0 1 2 2 0 0\n",
      "Client 26 Test: 39 1 2 7 0 1 4 0 0 3 3 1 3 6 1 5 2 0 0 0 0\n",
      "Client 27 Test: 40 4 0 0 0 1 4 5 10 2 1 0 0 6 1 4 1 0 0 1 0\n",
      "Client 28 Test: 40 3 3 6 1 1 3 2 0 1 2 8 1 2 1 3 1 0 0 2 0\n",
      "Client 29 Test: 43 1 1 3 5 3 0 0 1 3 2 1 0 1 4 9 0 5 1 2 1\n",
      "Client 30 Test: 43 0 2 6 1 2 1 1 1 1 2 2 1 5 3 0 1 0 2 7 5\n",
      "Client 31 Test: 39 3 0 1 1 1 2 8 0 1 2 0 0 0 1 0 4 1 0 12 2\n",
      "Client 32 Test: 40 2 3 1 0 6 1 0 1 5 2 5 0 0 1 0 5 0 1 4 3\n",
      "Client 33 Test: 41 0 6 1 4 1 10 0 1 0 1 1 1 0 0 7 3 0 2 2 1\n",
      "Client 34 Test: 40 0 2 7 0 2 1 1 4 7 3 0 1 3 2 2 3 0 2 0 0\n",
      "Client 35 Test: 39 5 0 3 0 0 1 0 5 5 6 1 1 3 1 1 3 0 4 0 0\n",
      "Client 36 Test: 39 0 5 0 0 1 0 0 8 3 0 10 0 1 2 2 2 0 2 3 0\n",
      "Client 37 Test: 38 1 4 2 2 1 5 2 5 4 1 0 0 0 1 0 0 0 0 3 7\n",
      "Client 38 Test: 41 7 1 4 1 0 1 2 0 2 1 4 0 0 3 0 3 0 3 2 7\n",
      "Client 39 Test: 41 0 1 4 1 1 8 2 2 2 0 0 11 0 1 3 0 2 1 1 1\n",
      "Client 40 Test: 40 2 9 1 2 5 0 3 2 1 1 2 0 3 0 1 7 1 0 0 0\n",
      "Client 41 Test: 41 3 0 5 3 1 1 0 1 0 5 3 0 7 5 1 3 0 1 2 0\n",
      "Client 42 Test: 43 2 0 0 3 1 1 0 1 0 9 6 0 0 3 2 2 4 5 3 1\n",
      "Client 43 Test: 38 7 0 0 0 1 0 0 0 0 1 0 1 8 3 3 10 3 0 1 0\n",
      "Client 44 Test: 43 2 0 13 1 2 2 0 2 1 1 1 0 0 0 1 0 0 8 1 8\n",
      "Client 45 Test: 40 4 0 4 0 0 0 1 7 2 5 0 1 0 0 1 5 7 2 0 1\n",
      "Client 46 Test: 39 3 0 1 0 1 0 2 0 5 0 1 1 3 2 12 4 0 4 0 0\n",
      "Client 47 Test: 41 0 5 0 1 3 4 1 0 0 3 2 0 0 1 0 2 1 13 1 4\n",
      "Client 48 Test: 41 1 0 0 0 3 8 1 1 2 6 0 2 0 6 6 2 0 1 0 2\n",
      "Client 49 Test: 41 1 7 10 0 2 1 0 0 0 2 2 2 0 2 2 0 0 9 0 1\n",
      "Client 50 Test: 40 0 0 4 1 2 0 0 0 1 4 2 0 0 4 13 2 3 3 1 0\n",
      "Client 51 Test: 38 16 0 1 2 0 2 1 0 0 1 8 0 0 4 0 0 0 0 2 1\n",
      "Client 52 Test: 39 6 0 0 1 1 2 0 1 1 1 1 9 0 7 0 1 4 4 0 0\n",
      "Client 53 Test: 41 2 0 2 8 1 1 0 3 0 1 0 1 4 1 0 2 0 1 8 6\n",
      "Client 54 Test: 38 1 0 0 2 1 0 0 6 0 14 1 0 0 5 1 3 1 3 0 0\n",
      "Client 55 Test: 39 1 12 2 2 1 2 0 2 0 0 2 0 12 0 0 0 1 1 1 0\n",
      "Client 56 Test: 43 1 5 1 1 0 2 3 5 0 0 7 0 3 0 6 0 0 2 2 5\n",
      "Client 57 Test: 35 0 1 18 0 0 0 0 0 0 4 1 0 4 0 1 0 4 0 0 2\n",
      "Client 58 Test: 40 2 0 0 1 0 0 0 4 8 0 3 2 0 1 5 4 0 0 10 0\n",
      "Client 59 Test: 40 1 4 0 2 2 2 1 2 1 6 0 0 2 3 1 1 0 1 8 3\n",
      "Client 60 Test: 42 4 2 0 7 7 0 4 0 1 1 1 0 0 2 1 4 3 3 1 1\n",
      "Client 61 Test: 41 1 0 0 2 2 1 2 15 2 0 0 1 1 1 6 1 4 1 0 1\n",
      "Client 62 Test: 39 0 2 0 5 1 7 0 0 0 6 5 2 2 1 2 2 0 2 2 0\n",
      "Client 63 Test: 41 1 0 4 1 2 3 1 0 8 1 0 1 0 0 2 2 4 0 3 8\n",
      "Client 64 Test: 39 3 4 1 4 1 2 2 1 1 0 1 3 0 3 2 1 10 0 0 0\n",
      "Client 65 Test: 42 0 0 0 6 3 0 2 2 2 1 0 2 0 4 5 6 7 1 0 1\n",
      "Client 66 Test: 39 4 1 0 2 8 0 4 1 3 6 0 1 1 0 0 1 3 1 2 1\n",
      "Client 67 Test: 39 0 5 2 0 3 0 5 6 1 0 11 0 3 1 0 1 1 0 0 0\n",
      "Client 68 Test: 39 0 2 1 3 6 5 1 1 4 0 1 0 2 0 7 0 3 0 1 2\n",
      "Client 69 Test: 37 0 0 1 1 9 0 1 0 4 6 3 0 0 1 4 0 3 1 0 3\n",
      "Client 70 Test: 39 2 4 1 1 3 4 2 4 4 4 0 0 0 1 0 2 0 0 6 1\n",
      "Client 71 Test: 39 2 1 1 3 1 1 1 0 1 4 0 0 1 7 3 0 2 1 10 0\n",
      "Client 72 Test: 38 2 3 0 0 17 2 3 1 1 0 0 0 0 1 0 3 1 3 1 0\n",
      "Client 73 Test: 39 5 0 0 0 3 0 2 0 1 0 6 1 5 7 1 1 0 2 5 0\n",
      "Client 74 Test: 45 1 1 6 2 1 7 0 2 2 1 2 4 2 2 4 1 1 1 1 4\n",
      "Client 75 Test: 41 4 5 12 2 1 1 3 1 0 10 0 0 1 0 0 1 0 0 0 0\n",
      "Client 76 Test: 36 1 0 5 0 2 0 0 3 0 0 2 0 3 3 0 1 6 10 0 0\n",
      "Client 77 Test: 39 0 4 4 9 1 1 0 0 0 0 4 0 3 0 0 3 0 7 3 0\n",
      "Client 78 Test: 39 7 2 0 10 1 0 2 2 0 0 4 6 2 0 0 2 0 1 0 0\n",
      "Client 79 Test: 38 0 1 0 3 4 0 0 0 0 6 0 14 5 0 0 0 3 0 2 0\n",
      "Client 80 Test: 38 6 2 0 1 0 6 1 0 0 5 1 0 4 4 0 1 2 3 0 2\n",
      "Client 81 Test: 39 1 2 2 1 0 0 0 2 2 0 4 0 3 0 0 2 3 6 0 11\n",
      "Client 82 Test: 40 2 3 0 0 2 0 1 0 2 3 0 2 1 1 2 18 2 0 1 0\n",
      "Client 83 Test: 39 1 2 1 1 10 4 5 0 0 0 0 3 2 1 2 0 0 4 0 3\n",
      "Client 84 Test: 42 1 0 1 1 0 8 1 0 4 0 0 0 6 0 2 3 3 0 0 12\n",
      "Client 85 Test: 39 9 2 2 0 5 0 0 2 0 2 0 0 2 1 3 1 2 4 4 0\n",
      "Client 86 Test: 42 2 0 16 3 1 2 2 1 1 3 1 1 1 4 1 2 0 1 0 0\n",
      "Client 87 Test: 37 2 0 2 0 6 1 0 4 4 4 4 0 0 0 0 0 0 0 7 3\n",
      "Client 88 Test: 41 2 1 1 4 4 1 0 3 2 1 4 1 7 1 0 0 0 4 1 4\n",
      "Client 89 Test: 38 6 3 0 5 1 5 0 1 0 1 1 0 0 0 9 2 0 0 0 4\n",
      "Client 90 Test: 39 6 0 1 1 0 5 1 1 0 0 0 2 0 2 5 0 1 12 0 2\n",
      "Client 91 Test: 40 0 3 2 1 1 7 1 0 4 1 0 2 1 0 0 0 2 1 5 9\n",
      "Client 92 Test: 40 5 3 3 0 4 1 2 5 2 0 9 1 1 0 0 2 1 0 1 0\n",
      "Client 93 Test: 39 6 3 0 0 0 4 2 3 1 0 2 0 0 1 4 1 0 0 3 9\n",
      "Client 94 Test: 42 1 3 0 4 0 4 1 1 1 0 0 4 0 3 3 3 7 5 1 1\n",
      "Client 95 Test: 39 0 0 0 0 7 3 1 4 0 1 1 0 0 1 14 1 2 1 2 1\n",
      "Client 96 Test: 37 0 2 3 3 0 5 0 5 1 1 0 1 1 3 0 0 1 3 0 8\n",
      "Client 97 Test: 39 1 1 6 3 2 0 0 4 5 4 1 0 2 6 0 1 1 1 0 1\n",
      "Client 98 Test: 41 4 0 0 2 1 4 2 0 1 3 0 1 1 5 1 2 0 12 2 0\n",
      "Client 99 Test: 40 9 0 0 1 2 2 0 0 5 1 2 0 2 0 2 0 0 0 0 14\n",
      "Client 100 Test: 42 1 1 1 1 0 2 0 1 0 0 0 9 1 2 0 0 0 9 1 13\n",
      "Client 101 Test: 39 1 3 6 0 0 2 3 1 1 4 0 6 1 0 0 2 0 7 0 2\n",
      "Client 102 Test: 40 0 2 10 3 0 2 0 1 0 10 2 1 1 2 1 3 0 1 0 1\n",
      "Client 103 Test: 40 0 1 1 8 2 0 0 7 2 2 2 0 4 1 1 6 0 2 1 0\n",
      "Client 104 Test: 38 1 3 4 4 0 0 2 0 4 1 6 3 0 0 1 0 7 2 0 0\n",
      "Client 105 Test: 35 1 2 1 0 0 0 0 0 11 0 3 1 0 0 6 2 6 1 0 1\n",
      "Client 106 Test: 41 2 0 1 1 0 7 1 1 8 7 0 1 0 1 0 0 0 5 0 6\n",
      "Client 107 Test: 38 1 6 0 18 0 0 2 0 0 1 1 1 2 1 2 2 1 0 0 0\n",
      "Client 108 Test: 40 0 1 2 2 0 0 1 9 0 2 1 5 0 4 3 0 5 1 3 1\n",
      "Client 109 Test: 39 0 0 0 0 0 4 0 5 1 2 2 3 2 1 1 12 1 4 0 1\n",
      "Client 110 Test: 39 1 12 0 1 0 1 2 5 1 2 4 0 1 2 2 0 0 4 1 0\n",
      "Client 111 Test: 38 1 0 1 0 0 0 1 1 1 0 2 9 2 2 1 1 1 0 13 2\n",
      "Client 112 Test: 41 0 3 0 1 2 2 0 8 7 0 5 0 2 1 0 0 0 3 2 5\n",
      "Client 113 Test: 40 8 2 0 5 2 1 6 3 0 0 0 1 0 2 2 1 1 4 1 1\n",
      "Client 114 Test: 39 0 4 7 6 0 0 5 0 6 1 0 0 2 1 0 0 2 0 4 1\n",
      "Client 115 Test: 39 1 1 11 1 1 4 8 3 0 1 0 3 3 0 0 0 1 1 0 0\n",
      "Client 116 Test: 39 0 1 3 3 1 2 0 4 0 0 8 0 1 4 0 0 5 5 0 2\n",
      "Client 117 Test: 41 2 9 0 2 0 0 1 0 9 0 0 0 0 4 1 4 5 2 1 1\n",
      "Client 118 Test: 39 0 0 5 6 2 0 0 0 0 4 0 1 5 1 2 1 4 6 2 0\n",
      "Client 119 Test: 41 0 0 0 1 1 6 1 1 3 4 0 1 4 1 0 4 5 1 0 8\n",
      "Client 120 Test: 39 14 1 4 2 5 2 0 2 4 3 0 0 0 1 0 0 1 0 0 0\n",
      "Client 121 Test: 38 0 0 9 0 0 6 4 0 0 5 0 4 1 1 5 1 2 0 0 0\n",
      "Client 122 Test: 40 0 0 0 4 1 4 3 0 2 0 0 6 9 0 0 5 5 0 1 0\n",
      "Client 123 Test: 39 5 2 0 0 1 0 1 1 2 4 7 0 0 0 2 0 5 4 4 1\n",
      "Client 124 Test: 40 0 0 1 13 3 1 3 1 0 1 0 1 4 0 4 1 2 0 0 5\n",
      "Client 125 Test: 37 2 1 0 2 0 0 1 0 0 5 0 9 0 0 2 2 0 12 1 0\n",
      "Client 126 Test: 40 6 0 2 2 0 3 0 0 2 0 1 2 12 2 2 0 2 0 0 4\n",
      "Client 127 Test: 37 0 2 6 5 1 0 2 4 1 2 7 0 0 0 0 3 3 0 1 0\n",
      "Client 128 Test: 40 0 2 11 2 0 2 0 1 2 3 6 0 1 0 1 0 2 0 3 4\n",
      "Client 129 Test: 41 0 2 1 10 14 1 0 1 2 0 0 0 0 0 2 2 0 1 3 2\n",
      "Client 130 Test: 41 5 4 2 1 3 1 3 1 0 1 0 6 6 3 0 1 2 1 0 1\n",
      "Client 131 Test: 39 1 0 6 1 2 1 6 2 0 0 2 1 0 0 2 0 4 3 5 3\n",
      "Client 132 Test: 40 1 0 1 0 1 0 1 0 3 3 0 1 12 6 0 1 0 5 5 0\n",
      "Client 133 Test: 41 0 1 5 17 1 2 0 0 0 1 6 0 0 0 1 6 1 0 0 0\n",
      "Client 134 Test: 37 0 0 4 0 4 1 10 4 0 0 4 0 0 0 3 0 2 4 0 1\n",
      "Client 135 Test: 39 6 0 2 0 6 4 0 1 0 2 1 1 1 2 0 1 0 0 5 7\n",
      "Client 136 Test: 40 1 2 4 5 3 0 1 1 0 1 1 1 8 0 3 3 1 3 0 2\n",
      "Client 137 Test: 41 0 1 4 1 0 1 6 2 3 2 2 2 2 3 4 2 0 3 0 3\n",
      "Client 138 Test: 40 0 6 1 1 0 0 4 0 1 0 1 0 2 3 11 2 0 2 2 4\n",
      "Client 139 Test: 41 1 2 0 0 1 1 14 1 0 1 0 0 0 7 0 6 0 6 1 0\n",
      "Client 140 Test: 39 0 1 1 3 1 9 0 0 2 0 3 0 0 2 0 9 0 3 1 4\n",
      "Client 141 Test: 40 1 8 2 1 2 1 1 1 0 0 2 3 0 4 0 1 0 1 2 10\n",
      "Client 142 Test: 42 6 4 2 2 0 1 1 1 1 2 3 11 0 1 3 0 4 0 0 0\n",
      "Client 143 Test: 41 0 2 0 1 0 0 5 2 8 0 1 4 1 7 1 1 2 5 1 0\n",
      "Client 144 Test: 43 3 7 1 3 0 4 2 0 4 0 4 1 0 1 0 2 2 7 0 2\n",
      "Client 145 Test: 38 0 1 0 2 0 0 0 6 2 1 7 2 7 0 5 0 0 5 0 0\n",
      "Client 146 Test: 41 0 1 11 0 0 0 8 0 2 6 3 4 0 0 1 2 0 2 1 0\n",
      "Client 147 Test: 39 1 4 4 0 0 5 9 4 0 9 0 1 0 0 0 1 0 1 0 0\n",
      "Client 148 Test: 39 5 0 1 5 6 1 1 0 2 0 0 1 0 2 2 0 12 0 1 0\n",
      "Client 149 Test: 42 1 6 0 3 0 0 1 1 0 2 0 0 5 1 10 1 1 4 5 1\n",
      "Client 150 Test: 40 2 0 1 1 2 5 0 1 1 3 0 0 4 0 2 2 6 4 4 2\n",
      "Client 151 Test: 40 1 0 7 3 3 2 1 5 1 0 2 5 5 0 1 4 0 0 0 0\n",
      "Client 152 Test: 41 0 1 2 2 12 1 0 4 2 1 0 4 3 0 4 0 2 2 0 1\n",
      "Client 153 Test: 40 9 3 0 4 4 0 8 4 0 1 0 3 0 1 0 0 0 0 1 2\n",
      "Client 154 Test: 44 2 3 1 0 1 2 2 4 1 2 9 1 3 5 1 1 1 0 1 4\n",
      "Client 155 Test: 41 1 2 0 2 2 0 0 1 0 2 0 6 12 6 0 2 0 1 3 1\n",
      "Client 156 Test: 37 3 0 1 4 7 0 6 1 0 0 3 9 0 0 0 0 1 1 0 1\n",
      "Client 157 Test: 44 1 6 0 1 0 0 1 6 2 3 7 0 1 4 3 1 1 0 4 3\n",
      "Client 158 Test: 41 1 10 0 9 0 1 1 1 0 1 2 1 0 1 0 2 6 0 3 2\n",
      "Client 159 Test: 38 5 3 0 0 0 2 1 6 0 1 0 3 2 8 0 1 1 4 1 0\n",
      "Client 160 Test: 42 0 12 3 1 2 2 5 6 2 1 0 0 0 0 1 0 1 0 1 5\n",
      "Client 161 Test: 40 8 3 0 0 1 3 3 5 1 1 2 0 3 0 5 0 0 1 0 4\n",
      "Client 162 Test: 39 4 0 7 3 0 1 0 1 0 0 1 0 1 1 0 3 1 8 2 6\n",
      "Client 163 Test: 41 1 1 0 1 5 1 0 1 8 1 2 0 9 1 7 2 0 1 0 0\n",
      "Client 164 Test: 38 2 5 1 0 0 6 0 1 11 0 0 0 2 1 0 5 0 2 2 0\n",
      "Client 165 Test: 40 2 1 3 3 5 0 3 1 0 0 0 1 8 1 0 1 7 1 2 1\n",
      "Client 166 Test: 40 0 17 0 3 1 9 0 0 1 0 1 1 0 0 1 0 1 0 5 0\n",
      "Client 167 Test: 42 0 4 2 1 1 1 1 0 1 1 6 4 3 3 1 1 5 0 7 0\n",
      "Client 168 Test: 44 8 0 1 0 4 1 2 1 6 0 2 1 4 2 6 1 0 0 5 0\n",
      "Client 169 Test: 39 3 1 3 0 0 1 2 1 1 0 2 1 0 3 8 1 3 2 5 2\n",
      "Client 170 Test: 38 0 0 12 0 1 0 3 4 3 0 0 0 3 0 4 2 1 0 0 5\n",
      "Client 171 Test: 40 1 0 0 0 1 12 0 0 2 1 3 3 0 7 1 1 0 4 3 1\n",
      "Client 172 Test: 39 2 7 0 1 0 1 3 1 1 1 1 0 10 1 4 3 2 1 0 0\n",
      "Client 173 Test: 38 2 1 0 15 1 0 2 0 1 0 0 2 2 5 3 1 0 1 2 0\n",
      "Client 174 Test: 39 2 0 0 6 0 3 1 1 0 2 5 0 2 4 0 0 0 0 9 4\n",
      "Client 175 Test: 39 4 5 3 1 0 1 0 1 2 3 0 1 1 0 0 0 2 12 2 1\n",
      "Client 176 Test: 41 5 2 1 0 0 2 1 1 9 0 2 11 0 1 0 0 0 5 1 0\n",
      "Client 177 Test: 39 2 0 0 0 9 0 7 0 0 5 4 0 1 0 0 1 0 6 2 2\n",
      "Client 178 Test: 38 1 3 3 1 13 0 4 5 3 0 0 1 3 0 0 1 0 0 0 0\n",
      "Client 179 Test: 36 0 0 0 1 0 1 4 0 7 1 0 0 5 0 3 1 0 12 0 1\n",
      "Client 180 Test: 42 2 0 0 5 1 3 2 2 0 0 3 6 0 6 0 1 0 9 0 2\n",
      "Client 181 Test: 41 3 0 1 0 10 3 2 0 4 5 0 0 1 0 7 1 0 2 2 0\n",
      "Client 182 Test: 41 0 0 0 2 1 0 4 1 9 1 4 4 2 3 0 1 1 0 2 6\n",
      "Client 183 Test: 41 3 1 0 2 0 1 9 0 1 1 4 1 2 1 0 5 9 0 1 0\n",
      "Client 184 Test: 43 5 5 2 1 0 1 2 0 6 1 3 1 1 11 1 0 1 0 1 1\n",
      "Client 185 Test: 40 0 1 3 1 2 0 7 0 2 0 4 0 0 1 2 7 1 1 8 0\n",
      "Client 186 Test: 42 0 2 2 8 0 9 1 1 1 2 1 3 0 0 6 1 3 1 0 1\n",
      "Client 187 Test: 40 0 0 1 4 6 2 4 5 4 0 1 7 0 1 0 0 1 1 3 0\n",
      "Client 188 Test: 37 0 1 2 1 3 8 8 1 1 6 1 0 0 0 0 1 0 4 0 0\n",
      "Client 189 Test: 39 0 0 2 3 10 6 3 1 0 1 2 0 1 4 1 1 0 0 4 0\n",
      "Client 190 Test: 39 0 2 1 0 0 1 6 0 0 7 5 0 0 7 3 1 1 0 0 5\n",
      "Client 191 Test: 39 3 0 0 7 0 0 3 0 1 1 7 1 0 0 6 3 1 2 1 3\n",
      "Client 192 Test: 41 0 1 0 3 1 7 0 0 1 4 1 1 1 11 1 5 0 0 3 1\n",
      "Client 193 Test: 38 5 0 3 0 0 4 0 1 1 12 1 0 2 3 0 1 0 2 3 0\n",
      "Client 194 Test: 41 3 0 4 2 7 2 1 5 0 0 1 9 1 0 0 2 2 0 0 2\n",
      "Client 195 Test: 39 0 0 2 2 2 2 2 8 1 1 0 2 4 6 0 6 0 0 1 0\n",
      "Client 196 Test: 42 4 0 2 2 0 3 1 2 0 7 6 0 0 6 0 1 2 3 1 2\n",
      "Client 197 Test: 43 0 0 4 1 0 1 0 10 1 1 2 3 2 1 4 0 3 2 6 2\n",
      "Client 198 Test: 39 1 2 0 0 1 12 0 3 4 2 1 4 0 1 2 2 0 1 2 1\n",
      "Client 199 Test: 39 0 6 0 0 1 9 1 1 1 0 3 6 1 2 0 0 0 0 1 7\n",
      "Server <built-in function round>: 1501 81 77 89 84 73 84 78 74 72 71 80 66 75 74 75 63 60 84 69 72\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 准备数据集\n",
    "# 这部分是我加的\n",
    "cifar = CIFAR100()\n",
    "prob = get_prob(non_iid, client_num, class_num=20)\n",
    "client_data, client_test_data = create_data(prob, size_per_client, cifar, N=20)\n",
    "\n",
    "\n",
    "all_images = []\n",
    "all_labels = []\n",
    "for data in client_data:\n",
    "    all_images.extend(data[0])\n",
    "    all_labels.extend(data[1])\n",
    "comb_client_data = [np.array(all_images), np.array(all_labels)]\n",
    "\n",
    "# 输出cpmb_client_data情况\n",
    "imgs, lbls = comb_client_data\n",
    "lbls = np.array(lbls)\n",
    "total_count = len(lbls)\n",
    "unique_classes, counts = np.unique(lbls, return_counts=True)\n",
    "\n",
    "# 创建一个长度为20的数组记录各类别计数，默认0\n",
    "class_counts = [0]*20\n",
    "for cls, cnt in zip(unique_classes, counts):\n",
    "    class_counts[cls] = cnt\n",
    "\n",
    "# 打印格式：Total: 总数 类别0计数 类别1计数 ... 类别19计数\n",
    "print(\"Traning Client Total: {}\".format(\" \".join([str(total_count)] + [str(c) for c in class_counts])))\n",
    "\n",
    "\n",
    "# 打印每个客户端训练数据情况（只输出前10个）\n",
    "for i, (imgs, lbls) in enumerate(client_data[:10]):\n",
    "    lbls = np.array(lbls)\n",
    "    total_count = len(lbls)\n",
    "    unique_classes, counts = np.unique(lbls, return_counts=True)\n",
    "    # 创建一个长度为20的数组记录各类别计数，默认0\n",
    "    class_counts = [0]*20\n",
    "    for cls, cnt in zip(unique_classes, counts):\n",
    "        class_counts[cls] = cnt\n",
    "    # 打印格式：Client i: 总数 类别0计数 类别1计数 ... 类别19计数\n",
    "    print(\"Client {}: {}\".format(i, \" \".join([str(total_count)] + [str(c) for c in class_counts])))\n",
    "    \n",
    "\n",
    "# 打印每个客户端测试数据情况（只输出前10个）\n",
    "for i, (imgs, lbls) in enumerate(client_test_data[:10]):\n",
    "    lbls = np.array(lbls)\n",
    "    total_count = len(lbls)\n",
    "    unique_classes, counts = np.unique(lbls, return_counts=True)\n",
    "    class_counts = [0]*20\n",
    "    for cls, cnt in zip(unique_classes, counts):\n",
    "        class_counts[cls] = cnt\n",
    "    # 打印格式：Client i Test: 总数 类别0计数 类别1计数 ... 类别19计数\n",
    "    print(\"Client {} Test: {}\".format(i, \" \".join([str(total_count)] + [str(c) for c in class_counts])))\n",
    "\n",
    "\n",
    "# 提前生成固定的服务器数据\n",
    "# Modify: 这是我后来修改的\n",
    "server_data = select_subset(comb_client_data, server_percentage)\n",
    "\n",
    "s_imgs, s_lbls = server_data\n",
    "s_lbls = np.array(s_lbls)\n",
    "total_count = len(s_lbls)\n",
    "unique_classes, counts = np.unique(s_lbls, return_counts=True)\n",
    "class_counts = [0]*20\n",
    "for cls, cnt in zip(unique_classes, counts):\n",
    "    class_counts[cls] = cnt\n",
    "\n",
    "# 输出格式: Server round: 总数 类别0计数 类别1计数 ... 类别19计数\n",
    "print(\"Server {}: {}\".format(round, \" \".join([str(total_count)] + [str(c) for c in class_counts])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "XvuicdZfeDvL"
   },
   "outputs": [],
   "source": [
    "# 本地训练并更新权重，返回更新后的模型权重、平均训练损失以及第一个迭代的梯度信息\n",
    "def update_weights(model_weight, dataset, learning_rate, local_epoch):\n",
    "    model = mobilenetv2().to(device)\n",
    "    model.load_state_dict(model_weight)\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    Tensor_set = TensorDataset(torch.Tensor(dataset[0]).to(device), torch.Tensor(dataset[1]).to(device))\n",
    "    data_loader = DataLoader(Tensor_set, batch_size=128, shuffle=True)\n",
    "\n",
    "    first_iter_gradient = None  # 初始化变量来保存第一个iter的梯度\n",
    "\n",
    "    for iter in range(local_epoch):\n",
    "        batch_loss = []\n",
    "        for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss.append(loss.item()/images.shape[0])\n",
    "\n",
    "            # 保存第一个iter的梯度\n",
    "            if iter == 0 and batch_idx == 0:\n",
    "                first_iter_gradient = {}\n",
    "                for name, param in model.named_parameters():\n",
    "                    first_iter_gradient[name] = param.grad.clone()\n",
    "                # 保存 BatchNorm 层的 running mean 和 running variance\n",
    "                for name, module in model.named_modules():\n",
    "                    if isinstance(module, nn.BatchNorm2d):\n",
    "                        first_iter_gradient[name + '.running_mean'] = module.running_mean.clone()\n",
    "                        first_iter_gradient[name + '.running_var'] = module.running_var.clone()\n",
    "\n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "\n",
    "    return model.state_dict(), sum(epoch_loss) / len(epoch_loss), first_iter_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "qsPZGD4Iem5w"
   },
   "outputs": [],
   "source": [
    "# 计算模型权重的差异，并根据学习率 lr 对权重差异进行缩放\n",
    "def weight_differences(n_w, p_w, lr):\n",
    "    w_diff = copy.deepcopy(n_w)\n",
    "    for key in w_diff.keys():\n",
    "        if 'num_batches_tracked' in key:\n",
    "            continue\n",
    "        w_diff[key] = (p_w[key] - n_w[key]) * lr\n",
    "    return w_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "040d862vbG9M"
   },
   "outputs": [],
   "source": [
    "# 也是本地训练，不过引入了本文的权重修正机制\n",
    "def update_weights_correction(model_weight, dataset, learning_rate, local_epoch, c_i, c_s):\n",
    "    model = mobilenetv2().to(device)\n",
    "    model.load_state_dict(model_weight)\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    Tensor_set = TensorDataset(torch.Tensor(dataset[0]).to(device), torch.Tensor(dataset[1]).to(device))\n",
    "    data_loader = DataLoader(Tensor_set, batch_size=200, shuffle=True)\n",
    "\n",
    "    for iter in range(local_epoch):\n",
    "        batch_loss = []\n",
    "        for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss.append(loss.sum().item()/images.shape[0])\n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "        corrected_graident = weight_differences(c_i, c_s, learning_rate)\n",
    "        orginal_model_weight = model.state_dict()\n",
    "        corrected_model_weight = weight_differences(corrected_graident, orginal_model_weight, 1)  # 这里缩放权重为1\n",
    "        model.load_state_dict(corrected_model_weight)\n",
    "\n",
    "    return model.state_dict(),  sum(epoch_loss) / len(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "qeFHXRuEo5Du"
   },
   "outputs": [],
   "source": [
    "def average_weights(w):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        if 'num_batches_tracked' in key:\n",
    "            continue\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], len(w))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "pLG9RrFffbl8"
   },
   "outputs": [],
   "source": [
    "# baseline: server-only\n",
    "def server_only(initial_w, global_round, gamma, E):\n",
    "    test_model = mobilenetv2().to(device)\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        # Server side local training\n",
    "        \n",
    "        \n",
    "        # 从comb中（论文中说明为全部训练数据）选择固定比率的server数据（并且是保证类别均衡的）\n",
    "        # server_data = select_subset(comb_client_data, server_percentage)\n",
    "                \n",
    "        update_server_w, round_loss, _ = update_weights(train_w, server_data, gamma, E)\n",
    "        train_w = update_server_w\n",
    "        test_model.load_state_dict(train_w)\n",
    "        train_loss.append(round_loss)\n",
    "        # Test Accuracy\n",
    "        test_a = 0\n",
    "        for i in client_test_data:\n",
    "            ac = test_inference(test_model,i)[0]\n",
    "            test_a = test_a + ac\n",
    "        test_a = test_a/len(client_test_data)\n",
    "        test_acc.append(test_a)\n",
    "        # print(test_a)\n",
    "    return test_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "nGtAn28aok2c"
   },
   "outputs": [],
   "source": [
    "def fedavg(initial_w, global_round, eta, K, M):\n",
    "    test_model = mobilenetv2().to(device)\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    for round in tqdm(range(global_round)):\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "        for i in sampled_client:\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        train_w = average_weights(local_weights)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "        test_a = 0\n",
    "        for i in client_test_data:\n",
    "            ac = test_inference(test_model,i)[0]\n",
    "            test_a = test_a + ac\n",
    "        test_a = test_a/len(client_test_data)\n",
    "        test_acc.append(test_a)\n",
    "#         print(test_a)\n",
    "    return test_acc, train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybridFL(initial_w, global_round, eta, K, M):\n",
    "    \"\"\"\n",
    "    HybridFL算法：FedAvg改进，服务器也作为一个普通客户端参与训练。\n",
    "    \n",
    "    参数:\n",
    "    - initial_w: 初始模型权重\n",
    "    - global_round: 全局训练轮数\n",
    "    - eta: 学习率\n",
    "    - K: 本地训练轮数\n",
    "    - M: 每轮采样的客户端数量\n",
    "    \"\"\"\n",
    "    test_model = mobilenetv2().to(device)  # 初始化测试模型\n",
    "    train_w = copy.deepcopy(initial_w)     # 当前全局权重\n",
    "    test_acc = []                          # 保存每轮测试精度\n",
    "    train_loss = []                        # 保存每轮训练损失\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        local_weights, local_loss = [], []  # 存储每个客户端/服务器的权重和损失\n",
    "\n",
    "        # 随机采样 M 个客户端\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "\n",
    "        # 客户端本地训练\n",
    "        for i in sampled_client:\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        # 服务器参与训练\n",
    "        update_server_w, server_round_loss, _ = update_weights(train_w, server_data, eta, K)\n",
    "        local_weights.append(update_server_w)   # 将服务器权重加入列表\n",
    "        local_loss.append(server_round_loss)    # 将服务器损失加入列表\n",
    "\n",
    "        # 权重聚合\n",
    "        train_w = average_weights(local_weights)\n",
    "\n",
    "        # 评估模型性能\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss) / len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "        \n",
    "        test_a = 0\n",
    "        for i in client_test_data:  # 遍历所有客户端测试数据\n",
    "            ac = test_inference(test_model, i)[0]\n",
    "            test_a += ac\n",
    "        test_a = test_a / len(client_test_data)\n",
    "        test_acc.append(test_a)\n",
    "        \n",
    "        # # 打印每轮的结果\n",
    "        # print(f\"Round {round + 1}: Test Accuracy = {test_a:.4f}, Train Loss = {loss_avg:.4f}\")\n",
    "    \n",
    "    return test_acc, train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "uzxW0sUxRGth"
   },
   "outputs": [],
   "source": [
    "def CLG_SGD(initial_w, global_round, eta, gamma, K, E, M):\n",
    "    test_model = mobilenetv2().to(device)\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    # 提前生成固定的服务器数据\n",
    "    # Modify: 这是我后来修改的\n",
    "    server_data = select_subset(comb_client_data, server_percentage)\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # 学习率衰减，这里默认注释掉了\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # 从总共client_num客户端中选择M个训练\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "        for i in sampled_client:\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "        train_w = average_weights(local_weights)\n",
    "        # Server side local training\n",
    "        \n",
    "        \n",
    "        # 从comb中（论文中说明为全部训练数据）选择固定比率的server数据（并且是保证类别均衡的）\n",
    "        # TODO_241216:这里是每一轮都重新选择数据（但保证类别比例是一样的，都是按照comb中的比例），我的场景中可以这样吗？\n",
    "        # server_data = select_subset(comb_client_data, server_percentage)\n",
    "        \n",
    "        update_server_w, round_loss, _ = update_weights(train_w, server_data, gamma, E)\n",
    "        train_w = update_server_w\n",
    "        local_loss.append(round_loss)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)   # 计算所有客户端和服务器一起的平均损失\n",
    "\n",
    "        test_a = 0\n",
    "        # 遍历客户端测试数据，计算平均准确率\n",
    "        for i in client_test_data:\n",
    "            ac = test_inference(test_model,i)[0]\n",
    "            test_a = test_a + ac\n",
    "        test_a = test_a/len(client_test_data)\n",
    "        test_acc.append(test_a)\n",
    "#         print(test_a)\n",
    "    return test_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "iJTODAzxYJgA"
   },
   "outputs": [],
   "source": [
    "def Fed_C(initial_w, global_round, eta, gamma, K, E, M):\n",
    "    test_model = mobilenetv2().to(device)\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    # 提前生成固定的服务器数据\n",
    "    # Modify: 这是我后来修改的\n",
    "    server_data = select_subset(comb_client_data, server_percentage)\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        g_i_list = []\n",
    "        # server_data = select_subset(comb_client_data, server_percentage)\n",
    "        \n",
    "        \n",
    "        # 计算Server gradient\n",
    "        _, _, g_s = update_weights(train_w, server_data, gamma, 1)\n",
    "\n",
    "        # 计算Client gradient\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "        for i in sampled_client:\n",
    "            _, _, g_i = update_weights(train_w, client_data[i], eta, 1)\n",
    "            g_i_list.append(g_i)\n",
    "\n",
    "\n",
    "        # Client side local training\n",
    "        for i in range(len(sampled_client)):\n",
    "            update_client_w, client_round_loss = update_weights_correction(train_w, client_data[sampled_client[i]], eta, K, g_i_list[i], g_s)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "        train_w = average_weights(local_weights)\n",
    "        # Server side local training\n",
    "        update_server_w, round_loss, _ = update_weights(train_w, server_data, gamma, E)\n",
    "        train_w = update_server_w\n",
    "        local_loss.append(round_loss)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        test_a = 0\n",
    "        for i in client_test_data:\n",
    "            ac = test_inference(test_model,i)[0]\n",
    "            test_a = test_a + ac\n",
    "        test_a = test_a/len(client_test_data)\n",
    "        test_acc.append(test_a)\n",
    "    return test_acc, train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "tXqVbWZK7gLn"
   },
   "outputs": [],
   "source": [
    "def Fed_S(initial_w, global_round, eta, gamma, K, E, M):\n",
    "    test_model = mobilenetv2().to(device)\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    # 提前生成固定的server数据\n",
    "    # Modify: 这是我后来修改的\n",
    "    server_data = select_subset(comb_client_data, server_percentage)\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        g_i_list = []\n",
    "        # Server gradient\n",
    "        # server_data = select_subset(comb_client_data, server_percentage)\n",
    "        _, _, g_s = update_weights(train_w, server_data, gamma, 1)\n",
    "\n",
    "        # Client gradient\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "        for i in sampled_client:\n",
    "            _, _, g_i = update_weights(train_w, client_data[i], eta, 1)\n",
    "            g_i_list.append(g_i)\n",
    "\n",
    "\n",
    "        # Client side local training\n",
    "        for i in range(len(sampled_client)):\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[sampled_client[i]], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "        train_w = average_weights(local_weights)\n",
    "\n",
    "        # Server aggregation correction\n",
    "        g_i_average = average_weights(g_i_list)\n",
    "        correction_g = weight_differences(g_i_average, g_s, K*eta)\n",
    "        train_w = weight_differences(correction_g, copy.deepcopy(train_w), 1)\n",
    "\n",
    "\n",
    "        # Server side local training\n",
    "        update_server_w, round_loss, _ = update_weights(train_w, server_data, gamma, E)\n",
    "        train_w = update_server_w\n",
    "        local_loss.append(round_loss)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        test_a = 0\n",
    "        for i in client_test_data:\n",
    "            ac = test_inference(test_model,i)[0]\n",
    "            test_a = test_a + ac\n",
    "        test_a = test_a/len(client_test_data)\n",
    "        test_acc.append(test_a)\n",
    "    return test_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:16<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server only 训练完成！\n",
      "各轮平均测试精度: [0.10287596441104581, 0.14398139309991856, 0.18785217434418858, 0.15332232592133085, 0.18244263935723987, 0.1758266011586979, 0.18132141144894515, 0.19606317445627663, 0.18624657813666906, 0.19404969934120306, 0.20645191418580824, 0.20822378262696045, 0.21310308546850618, 0.21384980198832929, 0.21281931882743513, 0.21749593855331667, 0.21687878338543315, 0.21281561314871392, 0.22119947122900005, 0.21719179773927563, 0.2184955529584898, 0.22000441994541833, 0.22120145343921352, 0.2218838383774683, 0.2220098439812817, 0.22330934645071362, 0.22244248830570737, 0.22266935400075302, 0.22307121334723048, 0.2240268590851516, 0.2228268642004425, 0.2231056267508674, 0.22245187424746796, 0.2228217072357219, 0.2203045619932656, 0.2200781050569018, 0.2215119964262164, 0.22105446769436837, 0.22176671851942406, 0.2221537017430568, 0.2221933714186379, 0.22351907557640532, 0.22328075253345805, 0.2219697519089592, 0.22186528445081083, 0.2199071593625958, 0.22313815307124038, 0.2234762215068698, 0.2225750717159351, 0.2228792206571188, 0.22340421851764283, 0.22223863572868718, 0.22255339628457743, 0.22251558766664978, 0.22278461827666238, 0.22257056506444525, 0.22358128542696573, 0.22336985105951737, 0.22315499849435355, 0.22537174782635958, 0.2232710985305883, 0.21582240870052838, 0.21915551469748862, 0.21948421269840726, 0.21237765036811967, 0.2133544045633075, 0.21291426781070621, 0.21322561493399556, 0.21374409925776466, 0.21393970047005395, 0.21645409715371575, 0.21462306139046217, 0.2145724196381195, 0.21617019578061988, 0.21457780152308767, 0.21393128237835599, 0.21677071723257565, 0.21652933373507233, 0.21619874732187022, 0.21656348399581749, 0.2165416701777905, 0.21540006350186736, 0.21626562642673408, 0.2172365849194221, 0.21750995769054848, 0.21732885840755567, 0.2181613758907248, 0.21753783231465554, 0.21810531577004327, 0.21904598980756923, 0.21890138689597968, 0.21873110424751988, 0.21814398026740178, 0.21877176879329682, 0.219042097253131, 0.2192509008836515, 0.2178691971727952, 0.21790959751174843, 0.2197358749637349, 0.2146850501769413]\n",
      "各轮平均训练损失: [0.023416101795330803, 0.018740201113732223, 0.014045505073442256, 0.010092580247767217, 0.0068113591961769596, 0.0035901797043475054, 0.002530206358600591, 0.001570852960455298, 0.0009752950862765237, 0.0007548387495699251, 0.0003094280554664872, 8.80768304476756e-05, 2.5122134944731737e-05, 5.029859712285673e-06, 3.405390746140474e-06, 7.220692736728212e-06, 2.031128349106139e-05, 1.3547978103670632e-05, 4.62827043782529e-06, 5.613326382721617e-06, 3.2952337195072807e-06, 2.1682130293083738e-06, 1.5598575000438124e-06, 1.5245244888692014e-06, 1.3817112288597773e-06, 1.338821971300056e-06, 1.493383246231327e-06, 1.2322335232216236e-06, 1.447492335571097e-06, 1.1152836958203594e-06, 9.332912703563551e-07, 1.0795395983326898e-06, 1.5637572170471345e-06, 1.1762653239479423e-06, 9.339801365928896e-07, 9.478615864360943e-07, 9.555076924767232e-07, 1.1580965435396187e-06, 1.22808655357923e-06, 8.334070268984503e-07, 1.0374595565173216e-06, 9.206258790157915e-07, 8.387639313467938e-07, 1.5704748455109813e-06, 9.492405464366813e-07, 9.000152431173764e-07, 1.6664109708640389e-06, 9.679692557912245e-07, 9.41572932832819e-07, 9.004234087890878e-07, 8.842546761602486e-07, 1.5634756601586175e-06, 9.50060313888372e-07, 1.328175580404459e-06, 1.4680912651989913e-06, 1.0736803084988034e-06, 1.0648869627241385e-06, 1.0617371973621859e-06, 1.093720449848604e-06, 1.0928858774367879e-06, 9.241736505617083e-07, 5.6599356009591845e-06, 2.6351491866849645e-06, 1.7038448175088721e-06, 1.133686943346487e-05, 2.6294486067326353e-06, 2.195504316690496e-06, 7.619369014099492e-06, 4.912358741200631e-06, 2.2602606811692094e-06, 1.8613114123722083e-06, 1.843813762676816e-06, 1.4170162671180118e-06, 1.2971574603979106e-06, 1.459308128558382e-06, 1.4061419891174497e-06, 1.189843355896594e-06, 1.2936548513812258e-06, 1.1802550571553923e-06, 1.2850135207442746e-06, 1.1860025442867781e-06, 1.1806132903913787e-06, 1.2210187579197848e-06, 1.2179190965985415e-06, 1.3218014359749775e-06, 1.152130315806655e-06, 1.4252640014640787e-06, 1.1201560052739062e-06, 1.2335767548974118e-06, 1.1497429294581465e-06, 1.0941735267517197e-06, 1.0882389583836507e-06, 1.1922291043702414e-06, 1.2414559338742208e-06, 1.1918483423250108e-06, 1.1756905653534207e-06, 1.8011923342995826e-06, 1.6228187229502163e-06, 1.2108724619306534e-06, 1.4262470050767975e-06]\n",
      "最终测试精度: 0.2146850501769413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:32<00:00,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fedavg训练完成！\n",
      "各轮平均测试精度: [0.04909025234902549, 0.04056460147812731, 0.043613334077364424, 0.05211640637356741, 0.050342874210119744, 0.06446414010343238, 0.10243203302001098, 0.09908160080422884, 0.11629886879982776, 0.0862639521604092, 0.10791582903555977, 0.10070050606671044, 0.12557201517442726, 0.14429485832808506, 0.10927617180579746, 0.1416412639197253, 0.0910099150824588, 0.1473405917603344, 0.1614552283172195, 0.17422235679553486, 0.11458040346819925, 0.12349595601979015, 0.10593530344096798, 0.14233352339481228, 0.11999208294907153, 0.11194232680607576, 0.16152634145237565, 0.14427451565313143, 0.15231357130838036, 0.1667952550986105, 0.14761827962895954, 0.1498900449501697, 0.18335759596093715, 0.14973906480802981, 0.16316053386612303, 0.15528568421199837, 0.15429198570812358, 0.13830550802731134, 0.1637545000082278, 0.18456487916641437, 0.16394078133692488, 0.15428780770213948, 0.18352982600159212, 0.17320022653322273, 0.1900488867649733, 0.19417672279388987, 0.1697734325374337, 0.18493117578679713, 0.19552856490977813, 0.18604549328378694, 0.15914002244347483, 0.2030831480194482, 0.19086071497474008, 0.18029472238322267, 0.17645239550366487, 0.2155909388449727, 0.17735842152441386, 0.16889376954749594, 0.20709042370114625, 0.1675801334980888, 0.20171377514225733, 0.19165758841284258, 0.20731247455191393, 0.21389860845316933, 0.21444943980253103, 0.19508817381727264, 0.20624185589250743, 0.18270216223845273, 0.1963310723772929, 0.20366576679912188, 0.19532017520826958, 0.19957608793736273, 0.21297790936057406, 0.20456784033886824, 0.22260023821068078, 0.20292258358576223, 0.19764239976274645, 0.2031793517405177, 0.21072413841754747, 0.21598843246727756, 0.19783981113037072, 0.21216576768156006, 0.20947032277752553, 0.2111380729262591, 0.2131805753727641, 0.1978666541681997, 0.22685032028865407, 0.21025241359686603, 0.2408290689160431, 0.22783229156602508, 0.22676848695118978, 0.23604121621795568, 0.23542905136642622, 0.21957994469530198, 0.23260770025310118, 0.23212105306485392, 0.23460743683185614, 0.19881356136665562, 0.23785613061619112, 0.2380983829412904]\n",
      "各轮平均训练损失: [0.1194063848781172, 0.0863479965937065, 0.08287341058340604, 0.06371296940172218, 0.06148028849830802, 0.06118923528173379, 0.060541356715515304, 0.0555923189480302, 0.057969740089590084, 0.05486983071892508, 0.05261966084644189, 0.0558759777305702, 0.052195725500187064, 0.05197233530786503, 0.0528913437351287, 0.05008449017792791, 0.05284028704905027, 0.05316310624805356, 0.050491664667284955, 0.05098451635050483, 0.04794332225304872, 0.04573394765254067, 0.050755768729561654, 0.04772959471986002, 0.04659545887703873, 0.05075328775960928, 0.04673349142925554, 0.04395636964786648, 0.044074611638193935, 0.045464827855032794, 0.04384936362233121, 0.0454973635645117, 0.04288024690305994, 0.0433357284100025, 0.043110381183204004, 0.04152929653226321, 0.04268536573494723, 0.04148354638411381, 0.042831297096759334, 0.04013538808327096, 0.03920292440307448, 0.042821875082150504, 0.03910126060673047, 0.03976258897808535, 0.038122746394129846, 0.03976768819576665, 0.03899169499598375, 0.04015358159315408, 0.04002786810016735, 0.03466140684580926, 0.03926837130001844, 0.036242299820618724, 0.034896103588904416, 0.038414585350936045, 0.032625582693850595, 0.035911171954709234, 0.03888257126665008, 0.036494451687792376, 0.03359370558338328, 0.03806067747731036, 0.032508136743557646, 0.035386587836501164, 0.03108188954094345, 0.03412262553280417, 0.033304681230104875, 0.03491128448825475, 0.02905148321365697, 0.032209513414529184, 0.03204706033836567, 0.034234863076221804, 0.03178407106707756, 0.034761182095325395, 0.02889584119513199, 0.03225402060282771, 0.028814987232684674, 0.030884976451184938, 0.03147289970966684, 0.02795329474331677, 0.029088459438477716, 0.02965381834740425, 0.030695742265771837, 0.0327375945924939, 0.026591344860812566, 0.029826529730259017, 0.03105306805388791, 0.02616328941316458, 0.02713373886253827, 0.027718767112750738, 0.02937728449698686, 0.026774043112261586, 0.025945069300403318, 0.02857007325159016, 0.024684764130496514, 0.025751727068212465, 0.02671600942334134, 0.02556506183355368, 0.02621736438558999, 0.03234585724708783, 0.02581357909967777, 0.026364766175247488]\n",
      "最终测试精度: 0.2380983829412904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [01:34<04:58,  3.93s/it]"
     ]
    }
   ],
   "source": [
    "# 初始化模型与参数\n",
    "# 这部分是我补充的\n",
    "\n",
    "\n",
    "init_model = mobilenetv2().to(device)\n",
    "initial_w = init_model.state_dict()\n",
    "\n",
    "# Servfer-only训练\n",
    "test_acc, train_loss = server_only(initial_w, global_round, gamma, E)\n",
    "\n",
    "# 打印训练过程中的结果\n",
    "print(\"Server only 训练完成！\")\n",
    "print(\"各轮平均测试精度:\", test_acc)\n",
    "print(\"各轮平均训练损失:\", train_loss)\n",
    "print(\"最终测试精度:\", test_acc[-1] if len(test_acc) > 0 else \"无数据\")\n",
    "\n",
    "\n",
    "init_model = mobilenetv2().to(device)\n",
    "initial_w = init_model.state_dict()\n",
    "\n",
    "# fedavg训练\n",
    "test_acc, train_loss = fedavg(initial_w, global_round, eta, K, M)\n",
    "\n",
    "# 打印训练过程中的结果\n",
    "print(\"fedavg训练完成！\")\n",
    "print(\"各轮平均测试精度:\", test_acc)\n",
    "print(\"各轮平均训练损失:\", train_loss)\n",
    "print(\"最终测试精度:\", test_acc[-1] if len(test_acc) > 0 else \"无数据\")\n",
    "\n",
    "\n",
    "\n",
    "init_model = mobilenetv2().to(device)\n",
    "initial_w = init_model.state_dict()\n",
    "\n",
    "# hybridfl训练\n",
    "test_acc, train_loss = hybridFL(initial_w, global_round, eta, K, M)\n",
    "\n",
    "# 打印训练过程中的结果\n",
    "print(\"hrbridFL训练完成！\")\n",
    "print(\"各轮平均测试精度:\", test_acc)\n",
    "print(\"各轮平均训练损失:\", train_loss)\n",
    "print(\"最终测试精度:\", test_acc[-1] if len(test_acc) > 0 else \"无数据\")\n",
    "\n",
    "\n",
    "init_model = mobilenetv2().to(device)\n",
    "initial_w = init_model.state_dict()\n",
    "\n",
    "# CLG_SGD训练\n",
    "test_acc, train_loss = CLG_SGD(initial_w, global_round, eta, gamma, K, E, M)\n",
    "\n",
    "# 打印训练过程中的结果\n",
    "print(\"CLG_SGD 训练完成！\")\n",
    "print(\"各轮平均测试精度:\", test_acc)\n",
    "print(\"各轮平均训练损失:\", train_loss)\n",
    "print(\"最终测试精度:\", test_acc[-1] if len(test_acc) > 0 else \"无数据\")\n",
    "\n",
    "\n",
    "init_model = mobilenetv2().to(device)\n",
    "initial_w = init_model.state_dict()\n",
    "\n",
    "# Fed_C训练\n",
    "test_acc, train_loss = Fed_C(initial_w, global_round, eta, gamma, K, E, M)\n",
    "\n",
    "# 打印训练过程中的结果\n",
    "print(\"Fed_C 训练完成！\")\n",
    "print(\"各轮平均测试精度:\", test_acc)\n",
    "print(\"各轮平均训练损失:\", train_loss)\n",
    "print(\"最终测试精度:\", test_acc[-1] if len(test_acc) > 0 else \"无数据\")\n",
    "\n",
    "\n",
    "init_model = mobilenetv2().to(device)\n",
    "initial_w = init_model.state_dict()\n",
    "\n",
    "# Fed_S训练\n",
    "test_acc, train_loss = Fed_S(initial_w, global_round, eta, gamma, K, E, M)\n",
    "\n",
    "# 打印训练过程中的结果\n",
    "print(\"Fed_S 训练完成！\")\n",
    "print(\"各轮平均测试精度:\", test_acc)\n",
    "print(\"各轮平均训练损失:\", train_loss)\n",
    "print(\"最终测试精度:\", test_acc[-1] if len(test_acc) > 0 else \"无数据\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
