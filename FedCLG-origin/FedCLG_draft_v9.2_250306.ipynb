{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict, defaultdict\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.functional as func\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from utils.language_utils import word_to_indices, letter_to_vec\n",
    "from utils.ShakeSpeare_reduce import ShakeSpeare\n",
    "\n",
    "import math\n",
    "\n",
    "from models.lstm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v 9.2\n",
    "\n",
    "FedDU加上超参数du_C\n",
    "ablation study 中加入decay_rate, du_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # 解决由于多次加载 OpenMP 相关动态库而引起的冲突"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.executable)\n",
    "\n",
    "# print(torch.cuda.is_available())\n",
    "# print(torch.cuda.get_device_capability())\n",
    "\n",
    "# gpu_info = !nvidia-smi\n",
    "# env = !env\n",
    "# print(env)\n",
    "\n",
    "# gpu_info = '\\n'.join(gpu_info)\n",
    "# if gpu_info.find('failed') >= 0:\n",
    "#   print('Not connected to a GPU')\n",
    "# else:\n",
    "#   print(gpu_info)\n",
    "\n",
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearBottleNeck(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride, t=6, class_num=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * t, 1),\n",
    "            nn.BatchNorm2d(in_channels * t),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels * t, in_channels * t, 3, stride=stride, padding=1, groups=in_channels * t),\n",
    "            nn.BatchNorm2d(in_channels * t),\n",
    "            nn.ReLU6(inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels * t, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.stride = stride\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = self.residual(x)\n",
    "\n",
    "        if self.stride == 1 and self.in_channels == self.out_channels:\n",
    "            residual += x\n",
    "\n",
    "        return residual\n",
    "\n",
    "# MobileNetV2（比lenet更复杂的CNN网络）网络中的线性瓶颈结构，原文中用于CIFAR-100任务\n",
    "class MobileNetV2(nn.Module):\n",
    "\n",
    "    def __init__(self, class_num=20):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pre = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.stage1 = LinearBottleNeck(32, 16, 1, 1)\n",
    "        self.stage2 = self._make_stage(2, 16, 24, 2, 6)\n",
    "        self.stage3 = self._make_stage(3, 24, 32, 2, 6)\n",
    "        self.stage4 = self._make_stage(4, 32, 64, 2, 6)\n",
    "        self.stage5 = self._make_stage(3, 64, 96, 1, 6)\n",
    "        self.stage6 = self._make_stage(3, 96, 160, 1, 6)\n",
    "        self.stage7 = LinearBottleNeck(160, 320, 1, 6)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(320, 1280, 1),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(1280, class_num, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "        x = self.stage6(x)\n",
    "        x = self.stage7(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _make_stage(self, repeat, in_channels, out_channels, stride, t):\n",
    "\n",
    "        layers = []\n",
    "        layers.append(LinearBottleNeck(in_channels, out_channels, stride, t))\n",
    "\n",
    "        while repeat - 1:\n",
    "            layers.append(LinearBottleNeck(out_channels, out_channels, 1, t))\n",
    "            repeat -= 1\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "def mobilenetv2():\n",
    "    return MobileNetV2()\n",
    "\n",
    "\n",
    "# FedMut中采用的cnn模型\n",
    "class CNNCifar(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNCifar, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x, start_layer_idx=0, logit=False):\n",
    "        if start_layer_idx < 0:  #\n",
    "            return self.mapping(x, start_layer_idx=start_layer_idx, logit=logit)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        result = {'activation' : x}\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        result['hint'] = x\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        result['representation'] = x\n",
    "        x = self.fc3(x)\n",
    "        result['output'] = x\n",
    "        return result\n",
    "\n",
    "    def mapping(self, z_input, start_layer_idx=-1, logit=True):\n",
    "        z = z_input\n",
    "        z = self.fc3(z)\n",
    "\n",
    "        result = {'output': z}\n",
    "        if logit:\n",
    "            result['logit'] = z\n",
    "        return result\n",
    "    \n",
    "def cnncifar():\n",
    "    return CNNCifar()\n",
    "\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetCifar10(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNetCifar10, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        result = {}\n",
    "        x = self.layer1(x)\n",
    "        result['activation1'] = x\n",
    "        x = self.layer2(x)\n",
    "        result['activation2'] = x\n",
    "        x = self.layer3(x)\n",
    "        result['activation3'] = x\n",
    "        x = self.layer4(x)\n",
    "        result['activation4'] = x\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        result['representation'] = x\n",
    "        x = self.fc(x)\n",
    "        result['output'] = x\n",
    "\n",
    "        return result\n",
    "\n",
    "    def mapping(self, z_input, start_layer_idx=-1, logit=True):\n",
    "        z = z_input\n",
    "        z = self.fc(z)\n",
    "\n",
    "        result = {'output': z}\n",
    "        if logit:\n",
    "            result['logit'] = z\n",
    "        return result\n",
    "\n",
    "    def forward(self, x, start_layer_idx=0, logit=False):\n",
    "        if start_layer_idx < 0:  #\n",
    "            return self.mapping(x, start_layer_idx=start_layer_idx, logit=logit)\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def ResNet8(**kwargs):\n",
    "    return ResNetCifar10(BasicBlock, [1, 1, 1], **kwargs)\n",
    "\n",
    "def ResNet18_cifar10(**kwargs):\n",
    "    r\"\"\"ResNet-18 model from\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return ResNetCifar10(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def ResNet50_cifar10(**kwargs):\n",
    "    r\"\"\"ResNet-50 model from\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return ResNetCifar10(Bottleneck, [3, 4, 6, 3], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新的测试：针对整个测试数据集的测试\n",
    "def test_inference(net_glob, dataset_test):\n",
    "    # testing\n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test)\n",
    "\n",
    "    # print(\"Testing accuracy: {:.2f}\".format(acc_test))\n",
    "\n",
    "    return acc_test.item()\n",
    "\n",
    "def test_img(net_g, datatest):\n",
    "    net_g.eval()\n",
    "    # testing\n",
    "    # test loss代表在测试集上的平均损失（对测试数据的预测输出与真实标签的差距）\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    data_loader = DataLoader(datatest, batch_size=test_bc_size)\n",
    "    l = len(data_loader)\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, target) in enumerate(data_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            log_probs = net_g(data)['output']\n",
    "            # sum up batch loss\n",
    "            test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n",
    "            # get the index of the max log-probability\n",
    "            y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "            correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.00 * correct / len(data_loader.dataset)\n",
    "    if verbose:\n",
    "        print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(data_loader.dataset), accuracy))\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将CIFAR-100的100个类别转为20个类别（粒度更粗，降低任务复杂度）\n",
    "def sparse2coarse(targets):\n",
    "    \"\"\"Convert Pytorch CIFAR100 sparse targets to coarse targets.\n",
    "\n",
    "    Usage:\n",
    "        trainset = torchvision.datasets.CIFAR100(path)\n",
    "        trainset.targets = sparse2coarse(trainset.targets)\n",
    "    \"\"\"\n",
    "    coarse_labels = np.array([ 4,  1, 14,  8,  0,  6,  7,  7, 18,  3,\n",
    "                               3, 14,  9, 18,  7, 11,  3,  9,  7, 11,\n",
    "                               6, 11,  5, 10,  7,  6, 13, 15,  3, 15,\n",
    "                               0, 11,  1, 10, 12, 14, 16,  9, 11,  5,\n",
    "                               5, 19,  8,  8, 15, 13, 14, 17, 18, 10,\n",
    "                               16, 4, 17,  4,  2,  0, 17,  4, 18, 17,\n",
    "                               10, 3,  2, 12, 12, 16, 12,  1,  9, 19,\n",
    "                               2, 10,  0,  1, 16, 12,  9, 13, 15, 13,\n",
    "                              16, 19,  2,  4,  6, 19,  5,  5,  8, 19,\n",
    "                              18,  1,  2, 15,  6,  0, 17,  8, 14, 13])\n",
    "    return coarse_labels[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 共有6w个图像，其中5w训练，1w测试\n",
    "def CIFAR100():\n",
    "    '''Return Cifar100\n",
    "    '''\n",
    "    \n",
    "    # 参考FedMut进行正则化变化\n",
    "    trans_cifar100 = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    train_dataset = torchvision.datasets.CIFAR100(root='../data/CIFAR-100',\n",
    "                                            train=True,\n",
    "                                            transform=trans_cifar100,\n",
    "                                            download=True)\n",
    "    test_dataset = torchvision.datasets.CIFAR100(root='../data/CIFAR-100',\n",
    "                                            train=False,\n",
    "                                            transform=trans_cifar100,\n",
    "                                            download=True)\n",
    "    \n",
    "    # 将图片转换成 numpy 数组格式，并对标签做了 coarse 处理\n",
    "    total_img,total_label = [],[]\n",
    "    for imgs,labels in train_dataset:\n",
    "        total_img.append(imgs.numpy())\n",
    "        total_label.append(labels)\n",
    "    total_img = np.array(total_img)\n",
    "    total_label = np.array(sparse2coarse(total_label))\n",
    "\n",
    "    cifar = [total_img, total_label]\n",
    "    \n",
    "    return cifar, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(non_iid, client_num, class_num=20, iid_mode=False):\n",
    "    \"\"\"\n",
    "    生成客户端数据分布概率矩阵\n",
    "    参数:\n",
    "        non_iid: Dirichlet分布参数，值越小数据越不均匀\n",
    "        client_num: 客户端数量\n",
    "        class_num: 类别数量\n",
    "        iid_mode: 是否为IID分布模式\n",
    "    返回:\n",
    "        形状为(client_num, class_num)的概率矩阵\n",
    "    \"\"\"\n",
    "    if data_random_fix:\n",
    "        np.random.seed(seed_num)  # 固定种子，确保数据抽样一致\n",
    "    \n",
    "    if iid_mode:\n",
    "        # IID分布 - 每个客户端对每个类别有相同的概率\n",
    "        return np.ones((client_num, class_num)) / class_num\n",
    "    else:\n",
    "        # 非IID分布 - 使用Dirichlet分布\n",
    "        return np.random.dirichlet(np.repeat(non_iid, class_num), client_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全部用于构建训练集\n",
    "def create_data_all_train(prob, size_per_client, dataset, N=20):\n",
    "    total_each_class = size_per_client * np.sum(prob, 0)\n",
    "    data, label = dataset\n",
    "    \n",
    "\n",
    "    if data_random_fix:\n",
    "        np.random.seed(seed_num)  # 固定种子，确保数据抽样一致\n",
    "        random.seed(seed_num)\n",
    "\n",
    "    # 为每个类别随机采样数据\n",
    "    all_class_set = []\n",
    "    for i in range(N):\n",
    "        size = total_each_class[i]\n",
    "        sub_data = data[label == i]\n",
    "        sub_label = label[label == i]\n",
    "\n",
    "        num_samples = int(size)\n",
    "        if num_samples > len(sub_data):\n",
    "            print(f\"类别 {i} 的数据样本不足，采样数从 {num_samples} 调整为 {len(sub_data)}\")\n",
    "            num_samples = len(sub_data)\n",
    "        rand_indx = np.random.choice(len(sub_data), size=num_samples, replace=False).astype(int)\n",
    "        \n",
    "        sub2_data, sub2_label = sub_data[rand_indx], sub_label[rand_indx]\n",
    "        all_class_set.append((sub2_data, sub2_label))\n",
    "\n",
    "    index = [0] * N\n",
    "    clients = []\n",
    "\n",
    "    for m in range(prob.shape[0]):  # 遍历客户端\n",
    "        labels, images = [], []  # 训练数据\n",
    "\n",
    "        for n in range(N):\n",
    "            # 100%用于训练\n",
    "            start, end = index[n], index[n] + int(prob[m][n] * size_per_client)\n",
    "            image, label = all_class_set[n][0][start:end], all_class_set[n][1][start:end]\n",
    "\n",
    "            # 记录当前类别的数据分配进度\n",
    "            index[n] += int(prob[m][n] * size_per_client)\n",
    "\n",
    "            labels.extend(label)\n",
    "            images.extend(image)\n",
    "\n",
    "        clients.append((np.array(images), np.array(labels)))\n",
    "\n",
    "    return clients\n",
    "\n",
    "# 80%构建训练集，20%构建测试集\n",
    "def create_data(prob, size_per_client, dataset, N=20):\n",
    "    total_each_class = size_per_client * np.sum(prob, 0)\n",
    "    data, label = dataset\n",
    "\n",
    "    # Modify：我之后加上的\n",
    "    if data_random_fix:\n",
    "        np.random.seed(seed_num)  # 固定种子，确保数据抽样一致\n",
    "        random.seed(seed_num)\n",
    "\n",
    "    # 为每个类别随机采样数据\n",
    "    all_class_set = []\n",
    "    for i in range(N):\n",
    "        size = total_each_class[i]\n",
    "        sub_data = data[label == i]\n",
    "        sub_label = label[label == i]\n",
    "\n",
    "        rand_indx = np.random.choice(len(sub_data), size=int(size), replace=False).astype(int)\n",
    "        sub2_data, sub2_label = sub_data[rand_indx], sub_label[rand_indx]\n",
    "        all_class_set.append((sub2_data, sub2_label))\n",
    "\n",
    "    index = [0] * N\n",
    "    clients, test = [], []\n",
    "\n",
    "    for m in range(prob.shape[0]):  # 遍历客户端\n",
    "        labels, images = [], []  # 训练数据\n",
    "        tlabels, timages = [], [] # 测试数据\n",
    "\n",
    "        for n in range(N):\n",
    "            # 80%用于训练，20%用于测试\n",
    "            # 这里的int向下取整，会导致实际的数据量比计算略小\n",
    "            start, end = index[n], index[n] + int(prob[m][n] * size_per_client * 0.8)\n",
    "            test_start, test_end = end, index[n] + int(prob[m][n] * size_per_client)\n",
    "\n",
    "            image, label = all_class_set[n][0][start:end], all_class_set[n][1][start:end]\n",
    "            test_image, test_label = all_class_set[n][0][test_start:test_end], all_class_set[n][1][test_start:test_end]\n",
    "\n",
    "            # 记录当前类别的数据分配进度\n",
    "            index[n] += int(prob[m][n] * size_per_client)\n",
    "\n",
    "            labels.extend(label)\n",
    "            images.extend(image)\n",
    "\n",
    "            tlabels.extend(test_label)\n",
    "            timages.extend(test_image)\n",
    "\n",
    "        clients.append((np.array(images), np.array(labels)))\n",
    "        test.append((np.array(timages), np.array(tlabels)))\n",
    "\n",
    "    return clients, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 合并所有客户端的测试数据 （上面讲测试数据分成了不同的客户端）\n",
    "# 但并没有使用，用途不明\n",
    "def comb_client_test_func(client_test_data):\n",
    "    comb_client_test_image = []\n",
    "    comb_client_test_label = []\n",
    "    for i in range(client_num):\n",
    "        comb_client_test_image.extend(list(client_test_data[i][0]))\n",
    "        comb_client_test_label.extend(list(client_test_data[i][1]))\n",
    "    \n",
    "    # 将测试图片和标签合并为 numpy 数组\n",
    "    comb_client_test_image = np.array(comb_client_test_image)\n",
    "    comb_client_test_label = np.array(comb_client_test_label)\n",
    "    \n",
    "    label_count = Counter(comb_client_test_label)\n",
    "    print(\"测试集类别分布：\")\n",
    "    for label, count in sorted(label_count.items()):\n",
    "        print(f\"类别 {label}: {count} 个样本\")\n",
    "    \n",
    "    return [comb_client_test_image, comb_client_test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 样服务器子集的函数\n",
    "def select_server_subset(cifar, percentage=0.1, mode='iid', dirichlet_alpha=1.0):\n",
    "    \"\"\"\n",
    "    从 cifar 数据集中挑选服务器数据子集（cifar 已经是 [N, C, H, W] 格式）。\n",
    "    \n",
    "    参数：\n",
    "      - cifar: 一个列表，格式为 [images, labels]，images 形状为 [N, C, H, W]\n",
    "      - percentage: 挑选比例，例如 0.1 表示取 10% 的数据\n",
    "      - mode: 'iid' 表示各类别均匀采样；'non-iid' 表示使用 Dirichlet 分布采样\n",
    "      - dirichlet_alpha: 当 mode 为 'non-iid' 时的 Dirichlet 分布参数\n",
    "    返回：\n",
    "      - subset_images: 选出的图片数组（numpy.array）\n",
    "      - subset_labels: 选出的标签数组（numpy.array）\n",
    "    \"\"\"\n",
    "    images, labels = cifar\n",
    "    unique_classes = np.unique(labels)\n",
    "    total_num = len(labels)\n",
    "    server_total = int(total_num * percentage)\n",
    "    \n",
    "    selected_indices = []\n",
    "    \n",
    "    if mode == 'iid':\n",
    "        for cls in unique_classes:\n",
    "            cls_indices = np.where(labels == cls)[0]\n",
    "            num_cls = int(len(cls_indices) * percentage)\n",
    "            if num_cls > len(cls_indices):\n",
    "                num_cls = len(cls_indices)\n",
    "            sampled = np.random.choice(cls_indices, size=num_cls, replace=False)\n",
    "            selected_indices.extend(sampled)\n",
    "    elif mode == 'non-iid':\n",
    "        num_classes = len(unique_classes)\n",
    "        prob = np.random.dirichlet(np.repeat(dirichlet_alpha, num_classes))\n",
    "        cls_sample_numbers = {}\n",
    "        total_assigned = 0\n",
    "        for i, cls in enumerate(unique_classes):\n",
    "            n_cls = int(prob[i] * server_total)\n",
    "            cls_sample_numbers[cls] = n_cls\n",
    "            total_assigned += n_cls\n",
    "        diff = server_total - total_assigned\n",
    "        if diff > 0:\n",
    "            for cls in np.random.choice(unique_classes, size=diff, replace=True):\n",
    "                cls_sample_numbers[cls] += 1\n",
    "        \n",
    "        for cls in unique_classes:\n",
    "            cls_indices = np.where(labels == cls)[0]\n",
    "            n_sample = cls_sample_numbers[cls]\n",
    "            if n_sample > len(cls_indices):\n",
    "                n_sample = len(cls_indices)\n",
    "            sampled = np.random.choice(cls_indices, size=n_sample, replace=False)\n",
    "            selected_indices.extend(sampled)\n",
    "    else:\n",
    "        raise ValueError(\"mode 参数必须为 'iid' 或 'non-iid'\")\n",
    "    \n",
    "    selected_indices = np.array(selected_indices)\n",
    "    np.random.shuffle(selected_indices)\n",
    "    \n",
    "    subset_images = images[selected_indices]\n",
    "    subset_labels = labels[selected_indices]\n",
    "    \n",
    "    return subset_images, subset_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本地训练并更新权重，返回更新后的模型权重、平均训练损失以及第一个迭代的梯度信息\n",
    "def update_weights(model_weight, dataset, learning_rate, local_epoch):\n",
    "    if origin_model == 'resnet':\n",
    "        model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        model = CharLSTM().to(device)\n",
    "    elif origin_model == \"cnn\":\n",
    "        model = cnncifar().to(device)\n",
    "    \n",
    "    model.load_state_dict(model_weight)\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if origin_model == 'resnet' or origin_model == 'cnn':\n",
    "        Tensor_set = TensorDataset(torch.Tensor(dataset[0]).to(device), torch.Tensor(dataset[1]).to(device))\n",
    "    elif origin_model == 'lstm':\n",
    "        Tensor_set = TensorDataset(torch.LongTensor(dataset[0]).to(device), torch.Tensor(dataset[1]).to(device))\n",
    "    \n",
    "    data_loader = DataLoader(Tensor_set, batch_size=bc_size, shuffle=True)\n",
    "\n",
    "    first_iter_gradient = None  # 初始化变量来保存第一个iter的梯度\n",
    "\n",
    "    for iter in range(local_epoch):\n",
    "        batch_loss = []\n",
    "        for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs['output'], labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss.append(loss.item()/images.shape[0])\n",
    "\n",
    "            # 保存第一个iter的梯度\n",
    "            if iter == 0 and batch_idx == 0:\n",
    "                first_iter_gradient = {}\n",
    "                for name, param in model.named_parameters():\n",
    "                    first_iter_gradient[name] = param.grad.clone()\n",
    "                # 保存 BatchNorm 层的 running mean 和 running variance\n",
    "                for name, module in model.named_modules():\n",
    "                    if isinstance(module, nn.BatchNorm2d):\n",
    "                        first_iter_gradient[name + '.running_mean'] = module.running_mean.clone()\n",
    "                        first_iter_gradient[name + '.running_var'] = module.running_var.clone()\n",
    "\n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "\n",
    "    return model.state_dict(), sum(epoch_loss) / len(epoch_loss), first_iter_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型权重的差异，并根据学习率 lr 对权重差异进行缩放\n",
    "def weight_differences(n_w, p_w, lr):\n",
    "    w_diff = copy.deepcopy(n_w)\n",
    "    for key in w_diff.keys():\n",
    "        if 'num_batches_tracked' in key:\n",
    "            continue\n",
    "        w_diff[key] = (p_w[key] - n_w[key]) * lr\n",
    "    return w_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 也是本地训练，不过引入了Fed-C的权重修正机制\n",
    "def update_weights_correction(model_weight, dataset, learning_rate, local_epoch, c_i, c_s):\n",
    "    if origin_model == 'resnet':\n",
    "        model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        model = CharLSTM().to(device)\n",
    "    elif origin_model == \"cnn\":\n",
    "        model = cnncifar().to(device)\n",
    "        \n",
    "    model.load_state_dict(model_weight)\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if origin_model == 'resnet' or origin_model == 'cnn':\n",
    "        Tensor_set = TensorDataset(torch.Tensor(dataset[0]).to(device), torch.Tensor(dataset[1]).to(device))\n",
    "    elif origin_model == 'lstm':\n",
    "        Tensor_set = TensorDataset(torch.LongTensor(dataset[0]).to(device), torch.Tensor(dataset[1]).to(device))\n",
    "        \n",
    "    data_loader = DataLoader(Tensor_set, batch_size=bc_size, shuffle=True)\n",
    "\n",
    "    for iter in range(local_epoch):\n",
    "        batch_loss = []\n",
    "        for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "            model.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs['output'], labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss.append(loss.sum().item()/images.shape[0])\n",
    "        epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "        corrected_graident = weight_differences(c_i, c_s, learning_rate)\n",
    "        orginal_model_weight = model.state_dict()\n",
    "        corrected_model_weight = weight_differences(corrected_graident, orginal_model_weight, 1)  # 这里缩放权重为1\n",
    "        model.load_state_dict(corrected_model_weight)\n",
    "\n",
    "    return model.state_dict(),  sum(epoch_loss) / len(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(w):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        if 'num_batches_tracked' in key:\n",
    "            continue\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], len(w))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline: server-only\n",
    "def server_only(initial_w, global_round, gamma, E):\n",
    "    if origin_model == 'resnet':\n",
    "        test_model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        test_model = CharLSTM().to(device)\n",
    "    elif origin_model == \"cnn\":\n",
    "        test_model = cnncifar().to(device)\n",
    "        \n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        # Server side local training\n",
    "\n",
    "                \n",
    "        update_server_w, round_loss, _ = update_weights(train_w, server_data, gamma, E)\n",
    "        train_w = update_server_w\n",
    "        test_model.load_state_dict(train_w)\n",
    "        train_loss.append(round_loss)\n",
    "        \n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "    \n",
    "        # Test Accuracy\n",
    "        # test_a = 0\n",
    "        # for i in client_test_data:\n",
    "        #     ac = test_inference(test_model,i)[0]\n",
    "        #     test_a = test_a + ac\n",
    "        # test_a = test_a/len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "        # print(test_a)\n",
    "    return test_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fedavg(initial_w, global_round, eta, K, M):\n",
    "    if origin_model == 'resnet':\n",
    "        test_model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        test_model = CharLSTM().to(device)\n",
    "    elif origin_model == \"cnn\":\n",
    "        test_model = cnncifar().to(device)\n",
    "        \n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    for round in tqdm(range(global_round)):\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "        for i in sampled_client:\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        train_w = average_weights(local_weights)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "        \n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "            \n",
    "        # test_a = 0\n",
    "        # for i in client_test_data:\n",
    "        #     ac = test_inference(test_model,i)[0]\n",
    "        #     test_a = test_a + ac\n",
    "        # test_a = test_a/len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "#         print(test_a)\n",
    "    return test_acc, train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybridFL(initial_w, global_round, eta, K, M):\n",
    "    \"\"\"\n",
    "    HybridFL算法：FedAvg改进，服务器也作为一个普通客户端参与训练。\n",
    "    \n",
    "    参数:\n",
    "    - initial_w: 初始模型权重\n",
    "    - global_round: 全局训练轮数\n",
    "    - eta: 学习率\n",
    "    - K: 本地训练轮数\n",
    "    - M: 每轮采样的客户端数量\n",
    "    \"\"\"\n",
    "    \n",
    "    if origin_model == 'resnet':\n",
    "        test_model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        test_model = CharLSTM().to(device)\n",
    "    elif origin_model == \"cnn\":\n",
    "        test_model = cnncifar().to(device)\n",
    "        \n",
    "    train_w = copy.deepcopy(initial_w)     # 当前全局权重\n",
    "    test_acc = []                          # 保存每轮测试精度\n",
    "    train_loss = []                        # 保存每轮训练损失\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        local_weights, local_loss = [], []  # 存储每个客户端/服务器的权重和损失\n",
    "\n",
    "        # 随机采样 M 个客户端\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "\n",
    "        # 客户端本地训练\n",
    "        for i in sampled_client:\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        # 服务器参与训练\n",
    "        update_server_w, server_round_loss, _ = update_weights(train_w, server_data, eta, K)\n",
    "        local_weights.append(update_server_w)   # 将服务器权重加入列表\n",
    "        local_loss.append(server_round_loss)    # 将服务器损失加入列表\n",
    "\n",
    "        # 权重聚合\n",
    "        train_w = average_weights(local_weights)\n",
    "\n",
    "        # 评估模型性能\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss) / len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "    \n",
    "        \n",
    "        # test_a = 0\n",
    "        # for i in client_test_data:  # 遍历所有客户端测试数据\n",
    "        #     ac = test_inference(test_model, i)[0]\n",
    "        #     test_a += ac\n",
    "        # test_a = test_a / len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "        \n",
    "        # # 打印每轮的结果\n",
    "        # print(f\"Round {round + 1}: Test Accuracy = {test_a:.4f}, Train Loss = {loss_avg:.4f}\")\n",
    "    \n",
    "    return test_acc, train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CLG_SGD(initial_w, global_round, eta, gamma, K, E, M):\n",
    "    if origin_model == 'resnet':\n",
    "        test_model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        test_model = CharLSTM().to(device)\n",
    "    elif origin_model == \"cnn\":\n",
    "        test_model = cnncifar().to(device)\n",
    "        \n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # 学习率衰减，这里默认注释掉了\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # 从总共client_num客户端中选择M个训练\n",
    "        sampled_client = random.sample(range(client_num), M)\n",
    "        for i in sampled_client:\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "        train_w = average_weights(local_weights)\n",
    "        # Server side local training\n",
    "    \n",
    "        \n",
    "        update_server_w, round_loss, _ = update_weights(train_w, server_data, gamma, E)\n",
    "        train_w = update_server_w\n",
    "        local_loss.append(round_loss)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)   # 计算所有客户端和服务器一起的平均损失\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "    \n",
    "        # test_a = 0\n",
    "        # # 遍历客户端测试数据，计算平均准确率\n",
    "        # for i in client_test_data:\n",
    "        #     ac = test_inference(test_model,i)[0]\n",
    "        #     test_a = test_a + ac\n",
    "        # test_a = test_a/len(client_test_data)\n",
    "        # test_acc.append(test_a)\n",
    "#         print(test_a)\n",
    "    return test_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与FedDUAP复现代码对应\n",
    "\n",
    "def KL_divergence(p1, p2):\n",
    "    \"\"\"\n",
    "    计算KL散度，与参考代码一致\n",
    "    \"\"\"\n",
    "    d = 0\n",
    "    for i in range(len(p1)):\n",
    "        if p2[i] == 0 or p1[i] == 0:\n",
    "            continue\n",
    "        d += p1[i] * math.log(p1[i]/p2[i], 2)  # 使用以2为底的对数\n",
    "    return d\n",
    "\n",
    "def calculate_js_divergence(p1, p2):\n",
    "    \"\"\"\n",
    "    计算Jensen-Shannon散度，与参考代码完全一致\n",
    "    \"\"\"\n",
    "    # 创建中点分布p3\n",
    "    p3 = []\n",
    "    for i in range(len(p1)):\n",
    "        p3.append((p1[i] + p2[i])/2)\n",
    "    \n",
    "    # 计算JS散度 = (KL(p1||p3) + KL(p2||p3))/2\n",
    "    return KL_divergence(p1, p3)/2 + KL_divergence(p2, p3)/2\n",
    "\n",
    "def ratio_combine(w1, w2, ratio=0):\n",
    "    \"\"\"\n",
    "    将两个权重进行加权平均，ratio表示w2的占比\n",
    "    对应参考代码中的ratio_combine函数\n",
    "    \"\"\"\n",
    "    w = copy.deepcopy(w1)\n",
    "    for key in w.keys():\n",
    "        if 'num_batches_tracked' in key:\n",
    "            continue\n",
    "        w[key] = (w2[key] - w1[key]) * ratio + w1[key]\n",
    "    return w\n",
    "\n",
    "def ratio_minus(w1, P, ratio=0):\n",
    "    \"\"\"\n",
    "    从w1减去P乘以ratio\n",
    "    对应参考代码中的ratio_minus函数\n",
    "    \"\"\"\n",
    "    w = copy.deepcopy(w1)\n",
    "    for key in w.keys():\n",
    "        if 'num_batches_tracked' in key:\n",
    "            continue\n",
    "        w[key] = w1[key] - P[key] * ratio\n",
    "    return w\n",
    "\n",
    "\n",
    "def FedDU_modify(initial_w, global_round, eta, gamma, K, E, M):\n",
    "    \"\"\"\n",
    "    Federated Dynamic Update算法实现，确保与参考代码完全一致\n",
    "    \"\"\"\n",
    "    if origin_model == 'resnet':\n",
    "        test_model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        test_model = CharLSTM().to(device)\n",
    "    elif origin_model == \"cnn\":\n",
    "        test_model = cnncifar().to(device)\n",
    "\n",
    "    # 初始化模型权重\n",
    "    train_w = copy.deepcopy(initial_w)\n",
    "    test_model.load_state_dict(train_w)\n",
    "    \n",
    "    # 结果记录\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    # 服务器更新的最小步长\n",
    "    server_min = 0\n",
    "    \n",
    "    # 收集所有客户端数据标签以计算全局分布\n",
    "    all_client_labels = []\n",
    "    for i in range(client_num):\n",
    "        all_client_labels.extend(client_data[i][1])\n",
    "    all_client_labels = np.array(all_client_labels)\n",
    "    \n",
    "    # 获取所有数据中的唯一类别\n",
    "    unique_classes = np.unique(all_client_labels)\n",
    "    num_classes = len(unique_classes)\n",
    "    \n",
    "    # 计算全局分布 (列表格式)\n",
    "    P = [0] * num_classes\n",
    "    for i, cls in enumerate(unique_classes):\n",
    "        P[i] = np.sum(all_client_labels == cls) / len(all_client_labels)\n",
    "    \n",
    "    # 获取服务器数据信息\n",
    "    server_labels = np.array(server_data[1])\n",
    "    n_0 = len(server_labels)  # 服务器数据量\n",
    "    \n",
    "    # 计算服务器分布 (列表格式)\n",
    "    P_0 = [0] * num_classes\n",
    "    for i, cls in enumerate(unique_classes):\n",
    "        P_0[i] = np.sum(server_labels == cls) / n_0 if n_0 > 0 else 0\n",
    "    \n",
    "    # 计算服务器数据的非IID程度\n",
    "    D_P_0 = calculate_js_divergence(P_0, P)\n",
    "    \n",
    "    # 输出初始设置\n",
    "    print(\"FedDU初始设置:\")\n",
    "    print(f\"  服务器数据量: {n_0}\")\n",
    "    print(f\"  服务器数据非IID度: {D_P_0:.6f}\")\n",
    "    print(f\"  衰减率: {decay_rate}\")\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        local_weights, local_losses = [], []\n",
    "        \n",
    "        # 随机选择M个客户端\n",
    "        sampled_clients = random.sample(range(client_num), M)\n",
    "        \n",
    "        # 记录当前轮次选择的客户端数据总量\n",
    "        num_current = 0\n",
    "        for i in sampled_clients:\n",
    "            num_current += len(client_data[i][0])\n",
    "        \n",
    "        # 客户端本地训练\n",
    "        for i in sampled_clients:\n",
    "            update_client_w, client_round_loss, _ = update_weights(train_w, client_data[i], eta, K)\n",
    "            local_weights.append(update_client_w)\n",
    "            local_losses.append(client_round_loss)\n",
    "        \n",
    "        # 聚合客户端模型\n",
    "        w_t_half = average_weights(local_weights)\n",
    "        \n",
    "        # 计算选定客户端数据的分布 (列表格式)\n",
    "        selected_client_labels = []\n",
    "        for i in sampled_clients:\n",
    "            selected_client_labels.extend(client_data[i][1])\n",
    "        selected_client_labels = np.array(selected_client_labels)\n",
    "        \n",
    "        P_t_prime = [0] * num_classes\n",
    "        for i, cls in enumerate(unique_classes):\n",
    "            P_t_prime[i] = np.sum(selected_client_labels == cls) / len(selected_client_labels) if len(selected_client_labels) > 0 else 0\n",
    "        \n",
    "        # 计算选定客户端数据的非IID程度\n",
    "        D_P_t_prime = calculate_js_divergence(P_t_prime, P)\n",
    "        \n",
    "    \n",
    "        # 评估聚合模型的准确率\n",
    "        test_model.load_state_dict(w_t_half)\n",
    "        acc_t = test_inference(test_model, test_dataset) / 100.0  # 转换为[0,1]比例\n",
    "        \n",
    "        # 计算每个客户端的平均迭代次数\n",
    "        avg_iter = (num_current * K) / (M * bc_size)\n",
    "        \n",
    "        # 计算alpha (动态更新系数)\n",
    "        epsilon = 1e-10  # 避免除零\n",
    "        alpha = (1 - acc_t) * (n_0 * D_P_t_prime) / (n_0 * D_P_t_prime + num_current * D_P_0 + epsilon)\n",
    "        alpha = alpha * (decay_rate ** round) * du_C\n",
    "        \n",
    "        # 计算服务器更新迭代次数\n",
    "        server_iter = max(server_min, int(alpha * avg_iter))\n",
    "        \n",
    "        # 只有当alpha大于阈值时才进行服务器更新\n",
    "        if alpha > 0.001:\n",
    "            # 服务器数据训练实际迭代次数\n",
    "            actual_iter = math.ceil(n_0 / bc_size) * E\n",
    "            server_iter = min(actual_iter, server_iter)\n",
    "            \n",
    "            # 服务器更新\n",
    "            update_server_w, round_loss, _ = update_weights(copy.deepcopy(w_t_half), server_data, gamma, E)\n",
    "            local_losses.append(round_loss)\n",
    "            \n",
    "            # 权重更新，使用ratio_combine函数\n",
    "            train_w = ratio_combine(w_t_half, update_server_w, alpha)\n",
    "            \n",
    "            print(f\"Round {round} server fixing with iters/actual_iters/alpha {server_iter}/{actual_iter}/{alpha:.4f}\")\n",
    "        else:\n",
    "            # 如果alpha不够大，直接使用客户端聚合模型\n",
    "            train_w = copy.deepcopy(w_t_half)\n",
    "            # 仍然计算服务器损失用于记录\n",
    "            _, round_loss, _ = update_weights(copy.deepcopy(w_t_half), server_data, gamma, E)\n",
    "            local_losses.append(round_loss)\n",
    "        \n",
    "        # 定期打印调试信息\n",
    "        if round % 5 == 0 or round == global_round - 1:\n",
    "            print(f\"\\nRound {round} 详情:\")\n",
    "            print(f\"  准确率: {acc_t*100:.2f}%\")\n",
    "            print(f\"  客户端数据量: {num_current}\")\n",
    "            print(f\"  D(P'_t): {D_P_t_prime:.6f}\")\n",
    "            print(f\"  D(P_0): {D_P_0:.6f}\")\n",
    "            print(f\"  alpha: {alpha:.4f}\")\n",
    "            print(f\"  平均迭代次数: {avg_iter:.2f}\")\n",
    "            print(f\"  服务器迭代次数: {server_iter}\")\n",
    "        \n",
    "        # 评估模型\n",
    "        test_model.load_state_dict(train_w)\n",
    "        loss_avg = sum(local_losses) / len(local_losses)\n",
    "        train_loss.append(loss_avg)\n",
    "        \n",
    "        # 在所有测试数据上测试\n",
    "        current_acc = test_inference(test_model, test_dataset)\n",
    "        test_acc.append(current_acc)\n",
    "        \n",
    "        if round % 10 == 0:\n",
    "            print(f\"Round {round}, Test Accuracy: {current_acc:.2f}%, Train Loss: {loss_avg:.4f}\")\n",
    "    \n",
    "    return test_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FedMut(net_glob, global_round, eta, K, M):\n",
    "    \n",
    "    net_glob.train()\n",
    "    \n",
    "    if origin_model == 'resnet':\n",
    "        test_model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        test_model = CharLSTM().to(device)\n",
    "    elif origin_model == \"cnn\":\n",
    "        test_model = cnncifar().to(device)\n",
    "        \n",
    "    train_w = copy.deepcopy(net_glob.state_dict())\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    w_locals = []\n",
    "    for i in range(M):\n",
    "        w_locals.append(copy.deepcopy(net_glob.state_dict()))\n",
    "        \n",
    "    max_rank = 0\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        w_old = copy.deepcopy(net_glob.state_dict())\n",
    "        \n",
    "        # 学习率衰减，这里默认注释掉了\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # 从总共client_num客户端中选择M个训练\n",
    "        idxs_users = np.random.choice(range(client_num), M, replace=False)\n",
    "        for i, idx in enumerate(idxs_users):\n",
    "            net_glob.load_state_dict(w_locals[i])\n",
    "            \n",
    "            update_client_w, client_round_loss, _ = update_weights(copy.deepcopy(net_glob.state_dict()), client_data[idx], eta, K)\n",
    "            w_locals[i] = copy.deepcopy(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        # Global Model Generation\n",
    "        w_agg = Aggregation(w_locals, None)  \n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_agg)\n",
    "        \n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(w_agg)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)   # 计算所有客户端的平均损失\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "\n",
    "        # 按照server训练的方向，进行mutation\n",
    "        w_delta = FedSub(w_agg, w_old, 1.0)\n",
    "        # 计算模型更新w_delta的L2范数（平方和），衡量模型更新程度的大小\n",
    "        rank = delta_rank(w_delta)\n",
    "        # print(rank)\n",
    "        if rank > max_rank:\n",
    "            max_rank = rank\n",
    "        alpha = radius  # 论文中的alpha，衡量Mutation的幅度\n",
    "        # alpha = min(max(args.radius, max_rank/rank),(10.0-args.radius) * (1 - iter/args.epochs) + args.radius)\n",
    "        w_locals = mutation_spread(\n",
    "            round, w_agg, M, w_delta, alpha\n",
    "        )\n",
    "\n",
    "    return test_acc, train_loss   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mutation方向设置为server更新的方向\n",
    "def CLG_Mut(net_glob, global_round, eta, gamma, K, E, M):\n",
    "    \n",
    "    net_glob.train()\n",
    "    \n",
    "    if origin_model == 'resnet':\n",
    "        test_model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        test_model = CharLSTM().to(device)\n",
    "    elif origin_model == \"cnn\":\n",
    "        test_model = cnncifar().to(device)\n",
    "        \n",
    "    train_w = copy.deepcopy(net_glob.state_dict())\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    w_locals = []\n",
    "    for i in range(M):\n",
    "        w_locals.append(copy.deepcopy(net_glob.state_dict()))\n",
    "    \n",
    "    max_rank = 0\n",
    "    w_old = copy.deepcopy(net_glob.state_dict())\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # 学习率衰减，这里默认注释掉了\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # 从总共client_num客户端中选择M个训练\n",
    "        idxs_users = np.random.choice(range(client_num), M, replace=False)\n",
    "        for i, idx in enumerate(idxs_users):\n",
    "            net_glob.load_state_dict(w_locals[i])\n",
    "            \n",
    "            update_client_w, client_round_loss, _ = update_weights(copy.deepcopy(net_glob.state_dict()), client_data[idx], eta, K)\n",
    "            w_locals[i] = copy.deepcopy(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        # Global Model Generation\n",
    "        w_agg = Aggregation(w_locals, None)  \n",
    "        \n",
    "        # Server side local training\n",
    "        update_server_w, round_loss, _ = update_weights(w_agg, server_data, gamma, E)\n",
    "        local_loss.append(round_loss)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(update_server_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)   # 计算所有客户端和服务器一起的平均损失\n",
    "\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "\n",
    "        # 按照server训练的方向，进行mutation\n",
    "        w_delta = FedSub(update_server_w, w_agg, 1.0)\n",
    "        # 计算模型更新w_delta的L2范数（平方和），衡量模型更新程度的大小\n",
    "        rank = delta_rank(w_delta)\n",
    "        # print(rank)\n",
    "        if rank > max_rank:\n",
    "            max_rank = rank\n",
    "        alpha = radius  # 论文中的alpha，衡量Mutation的幅度\n",
    "        # alpha = min(max(args.radius, max_rank/rank),(10.0-args.radius) * (1 - iter/args.epochs) + args.radius)\n",
    "        w_locals = mutation_spread(\n",
    "            round, update_server_w, M, w_delta, alpha\n",
    "        )\n",
    "\n",
    "    return test_acc, train_loss\n",
    "\n",
    "\n",
    "# 将mutation的方向设置为新方向（server更新之后）减去上一轮全局方向（其余不变）\n",
    "def CLG_Mut_2(net_glob, global_round, eta, gamma, K, E, M):\n",
    "    \n",
    "    net_glob.train()\n",
    "    \n",
    "    if origin_model == 'resnet':\n",
    "        test_model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        test_model = CharLSTM().to(device)\n",
    "    elif origin_model == \"cnn\":\n",
    "        test_model = cnncifar().to(device)\n",
    "        \n",
    "    train_w = copy.deepcopy(net_glob.state_dict())\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    w_locals = []\n",
    "    for i in range(M):\n",
    "        w_locals.append(copy.deepcopy(net_glob.state_dict()))\n",
    "    \n",
    "    max_rank = 0\n",
    "    w_old = copy.deepcopy(net_glob.state_dict())\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        w_old = copy.deepcopy(net_glob.state_dict())\n",
    "        \n",
    "        # 学习率衰减，这里默认注释掉了\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # 从总共client_num客户端中选择M个训练\n",
    "        idxs_users = np.random.choice(range(client_num), M, replace=False)\n",
    "        for i, idx in enumerate(idxs_users):\n",
    "            net_glob.load_state_dict(w_locals[i])\n",
    "            \n",
    "            update_client_w, client_round_loss, _ = update_weights(copy.deepcopy(net_glob.state_dict()), client_data[idx], eta, K)\n",
    "            w_locals[i] = copy.deepcopy(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        # Global Model Generation\n",
    "        w_agg = Aggregation(w_locals, None)  \n",
    "        \n",
    "        # Server side local training\n",
    "        update_server_w, round_loss, _ = update_weights(w_agg, server_data, gamma, E)\n",
    "        local_loss.append(round_loss)\n",
    "        \n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(update_server_w)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(update_server_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)   # 计算所有客户端和服务器一起的平均损\n",
    "\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "\n",
    "        # 按照server训练的方向，进行mutation\n",
    "        w_delta = FedSub(update_server_w, w_old, 1.0)\n",
    "        # 计算模型更新w_delta的L2范数（平方和），衡量模型更新程度的大小\n",
    "        rank = delta_rank(w_delta)\n",
    "        # print(rank)\n",
    "        if rank > max_rank:\n",
    "            max_rank = rank\n",
    "        alpha = radius  # 论文中的alpha，衡量Mutation的幅度\n",
    "        # alpha = min(max(args.radius, max_rank/rank),(10.0-args.radius) * (1 - iter/args.epochs) + args.radius)\n",
    "        w_locals = mutation_spread(\n",
    "            round, update_server_w, M, w_delta, alpha\n",
    "        )\n",
    "\n",
    "    return test_acc, train_loss\n",
    "\n",
    "\n",
    "# 将mutation的方向设置为client训练更新的方向\n",
    "def CLG_Mut_3(net_glob, global_round, eta, gamma, K, E, M):\n",
    "    \n",
    "    net_glob.train()\n",
    "    \n",
    "    if origin_model == 'resnet':\n",
    "        test_model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        test_model = CharLSTM().to(device)\n",
    "    elif origin_model == \"cnn\":\n",
    "        test_model = cnncifar().to(device)\n",
    "        \n",
    "    train_w = copy.deepcopy(net_glob.state_dict())\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    w_locals = []\n",
    "    for i in range(M):\n",
    "        w_locals.append(copy.deepcopy(net_glob.state_dict()))\n",
    "    \n",
    "    max_rank = 0\n",
    "    w_old = copy.deepcopy(net_glob.state_dict())\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        w_old = copy.deepcopy(net_glob.state_dict())\n",
    "        \n",
    "        # 学习率衰减，这里默认注释掉了\n",
    "        # if eta > 0.001:\n",
    "        #     eta = eta * 0.99\n",
    "        # if gamma > 0.001:\n",
    "        #     gamma = gamma * 0.99\n",
    "        local_weights, local_loss = [], []\n",
    "        # Client side local training\n",
    "        # 从总共client_num客户端中选择M个训练\n",
    "        idxs_users = np.random.choice(range(client_num), M, replace=False)\n",
    "        for i, idx in enumerate(idxs_users):\n",
    "            net_glob.load_state_dict(w_locals[i])\n",
    "            \n",
    "            update_client_w, client_round_loss, _ = update_weights(copy.deepcopy(net_glob.state_dict()), client_data[idx], eta, K)\n",
    "            w_locals[i] = copy.deepcopy(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "\n",
    "        # Global Model Generation\n",
    "        w_agg = Aggregation(w_locals, None)  \n",
    "        \n",
    "        # Server side local training\n",
    "        update_server_w, round_loss, _ = update_weights(w_agg, server_data, gamma, E)\n",
    "        local_loss.append(round_loss)\n",
    "        \n",
    "        \n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(update_server_w)\n",
    "\n",
    "        # Test Accuracy\n",
    "        test_model.load_state_dict(update_server_w)\n",
    "        loss_avg = sum(local_loss)/ len(local_loss)\n",
    "        train_loss.append(loss_avg)   # 计算所有客户端和服务器一起的平均损\n",
    "\n",
    "\n",
    "        # 新的测试（针对全部测试数据进行）\n",
    "        test_acc.append(test_inference(test_model, test_dataset))\n",
    "\n",
    "        # 按照client训练的方向，进行mutation\n",
    "        w_delta = FedSub(w_agg, w_old, 1.0)\n",
    "        # 计算模型更新w_delta的L2范数（平方和），衡量模型更新程度的大小\n",
    "        rank = delta_rank(w_delta)\n",
    "        # print(rank)\n",
    "        if rank > max_rank:\n",
    "            max_rank = rank\n",
    "        alpha = radius  # 论文中的alpha，衡量Mutation的幅度\n",
    "        # alpha = min(max(args.radius, max_rank/rank),(10.0-args.radius) * (1 - iter/args.epochs) + args.radius)\n",
    "        w_locals = mutation_spread(\n",
    "            round, update_server_w, M, w_delta, alpha\n",
    "        )\n",
    "\n",
    "    return test_acc, train_loss\n",
    "\n",
    "\n",
    "def mutation_spread(iter, w_glob, m, w_delta, alpha):\n",
    "\n",
    "    w_locals_new = []\n",
    "    ctrl_cmd_list = []\n",
    "    ctrl_rate = mut_acc_rate * (\n",
    "        1.0 - min(iter * 1.0 / mut_bound, 1.0)\n",
    "    )  # 论文中的βt，随着iter逐渐从β0减小到0\n",
    "\n",
    "    # k代表模型中的参数数量，对每个参数按照client数量分配v（论文中是按照每一层分配）\n",
    "    for k in w_glob.keys():\n",
    "        ctrl_list = []\n",
    "        for i in range(0, int(m / 2)):\n",
    "            ctrl = random.random()  # 随机数，范围：[0,1)\n",
    "            # 这里分ctrl感觉没什么必要，shuffle后都会随机掉\n",
    "            if ctrl > 0.5:\n",
    "                ctrl_list.append(1.0)\n",
    "                ctrl_list.append(1.0 * (-1.0 + ctrl_rate))\n",
    "            else:\n",
    "                ctrl_list.append(1.0 * (-1.0 + ctrl_rate))\n",
    "                ctrl_list.append(1.0)\n",
    "        random.shuffle(ctrl_list)  # 打乱列表\n",
    "        ctrl_cmd_list.append(ctrl_list)\n",
    "    cnt = 0\n",
    "    for j in range(m):\n",
    "        w_sub = copy.deepcopy(w_glob)\n",
    "        if not (cnt == m - 1 and m % 2 == 1):\n",
    "            ind = 0\n",
    "            for k in w_sub.keys():\n",
    "                w_sub[k] = w_sub[k] + w_delta[k] * ctrl_cmd_list[ind][j] * alpha\n",
    "                ind += 1\n",
    "        cnt += 1\n",
    "        w_locals_new.append(w_sub)\n",
    "\n",
    "    return w_locals_new\n",
    "\n",
    "\n",
    "\n",
    "# 加权平均聚合，lens代表了权重，如果没有定义就是普通平均（FedMut就每定义）\n",
    "def Aggregation(w, lens):\n",
    "    w_avg = None\n",
    "    if lens == None:\n",
    "        total_count = len(w)\n",
    "        lens = []\n",
    "        for i in range(len(w)):\n",
    "            lens.append(1.0)\n",
    "    else:\n",
    "        total_count = sum(lens)\n",
    "\n",
    "    for i in range(0, len(w)):\n",
    "        if i == 0:\n",
    "            w_avg = copy.deepcopy(w[0])\n",
    "            for k in w_avg.keys():\n",
    "                w_avg[k] = w[i][k] * lens[i]\n",
    "        else:\n",
    "            for k in w_avg.keys():\n",
    "                w_avg[k] += w[i][k] * lens[i]\n",
    "\n",
    "    for k in w_avg.keys():\n",
    "        w_avg[k] = torch.div(w_avg[k], total_count)\n",
    "\n",
    "    return w_avg\n",
    "\n",
    "\n",
    "\n",
    "def FedSub(w, w_old, weight):\n",
    "    w_sub = copy.deepcopy(w)\n",
    "    for k in w_sub.keys():\n",
    "        w_sub[k] = (w[k] - w_old[k]) * weight\n",
    "\n",
    "    return w_sub\n",
    "\n",
    "def delta_rank(delta_dict):\n",
    "    cnt = 0\n",
    "    dict_a = torch.Tensor(0)\n",
    "    s = 0\n",
    "    for p in delta_dict.keys():\n",
    "        a = delta_dict[p]\n",
    "        a = a.view(-1)\n",
    "        if cnt == 0:\n",
    "            dict_a = a\n",
    "        else:\n",
    "            dict_a = torch.cat((dict_a, a), dim=0)\n",
    "\n",
    "        cnt += 1\n",
    "        # print(sim)\n",
    "    s = torch.norm(dict_a, dim=0)\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FedDU_Mut(net_glob, global_round, eta, gamma, K, E, M):\n",
    "    \"\"\"\n",
    "    FedDU-Mut算法：融合CLG_Mut_2与FedDU，使用动态服务器参与度和mutation策略\n",
    "    \n",
    "    参数:\n",
    "    - net_glob: 初始模型\n",
    "    - global_round: 全局训练轮数\n",
    "    - eta: 客户端学习率\n",
    "    - gamma: 服务器学习率\n",
    "    - K: 客户端本地训练轮数\n",
    "    - E: 服务器本地训练轮数\n",
    "    - M: 每轮采样的客户端数量\n",
    "    \"\"\"\n",
    "    \n",
    "    net_glob.train()\n",
    "    \n",
    "    if origin_model == 'resnet':\n",
    "        test_model = ResNet18_cifar10().to(device)\n",
    "    elif origin_model == \"lstm\":\n",
    "        test_model = CharLSTM().to(device)\n",
    "    elif origin_model == \"cnn\":\n",
    "        test_model = cnncifar().to(device)\n",
    "    \n",
    "    train_w = copy.deepcopy(net_glob.state_dict())\n",
    "    test_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    # 初始化本地模型列表，用于mutation\n",
    "    w_locals = []\n",
    "    for i in range(M):\n",
    "        w_locals.append(copy.deepcopy(net_glob.state_dict()))\n",
    "    \n",
    "    # 记录每轮的最大更新幅度\n",
    "    max_rank = 0\n",
    "    w_old = copy.deepcopy(net_glob.state_dict())\n",
    "    \n",
    "    # 服务器更新的最小步长\n",
    "    server_min = 0\n",
    "    \n",
    "    # 收集所有客户端数据标签以计算全局分布\n",
    "    all_client_labels = []\n",
    "    for i in range(client_num):\n",
    "        all_client_labels.extend(client_data[i][1])\n",
    "    all_client_labels = np.array(all_client_labels)\n",
    "    \n",
    "    # 获取所有数据中的唯一类别\n",
    "    unique_classes = np.unique(all_client_labels)\n",
    "    num_classes = len(unique_classes)\n",
    "    \n",
    "    # 计算全局分布 (列表格式)\n",
    "    P = [0] * num_classes\n",
    "    for i, cls in enumerate(unique_classes):\n",
    "        P[i] = np.sum(all_client_labels == cls) / len(all_client_labels)\n",
    "    \n",
    "    # 获取服务器数据信息\n",
    "    server_labels = np.array(server_data[1])\n",
    "    n_0 = len(server_labels)  # 服务器数据量\n",
    "    \n",
    "    # 计算服务器分布 (列表格式)\n",
    "    P_0 = [0] * num_classes\n",
    "    for i, cls in enumerate(unique_classes):\n",
    "        P_0[i] = np.sum(server_labels == cls) / n_0 if n_0 > 0 else 0\n",
    "    \n",
    "    # 计算服务器数据的非IID程度\n",
    "    D_P_0 = calculate_js_divergence(P_0, P)\n",
    "    \n",
    "    # 输出初始设置\n",
    "    print(\"FedDU-Mut初始设置:\")\n",
    "    print(f\"  服务器数据量: {n_0}\")\n",
    "    print(f\"  服务器数据非IID度: {D_P_0:.6f}\")\n",
    "    print(f\"  衰减率: {decay_rate}\")\n",
    "    print(f\"  Mutation幅度(radius): {radius}\")\n",
    "    \n",
    "    for round in tqdm(range(global_round)):\n",
    "        # 保存当前全局模型作为基准\n",
    "        w_old = copy.deepcopy(net_glob.state_dict())\n",
    "        \n",
    "        local_weights, local_loss = [], []\n",
    "        \n",
    "        # 客户端侧训练 - 从总共client_num客户端中选择M个训练\n",
    "        idxs_users = np.random.choice(range(client_num), M, replace=False)\n",
    "        \n",
    "        # 记录当前轮次选择的客户端数据总量\n",
    "        selected_client_labels = []\n",
    "        num_current = 0\n",
    "        \n",
    "        for i, idx in enumerate(idxs_users):\n",
    "            # 加载mutation后的初始模型\n",
    "            net_glob.load_state_dict(w_locals[i])\n",
    "            \n",
    "            # 客户端本地训练\n",
    "            update_client_w, client_round_loss, _ = update_weights(\n",
    "                copy.deepcopy(net_glob.state_dict()), \n",
    "                client_data[idx], \n",
    "                eta, \n",
    "                K\n",
    "            )\n",
    "            w_locals[i] = copy.deepcopy(update_client_w)\n",
    "            local_loss.append(client_round_loss)\n",
    "            \n",
    "            # 收集客户端标签和数据量\n",
    "            selected_client_labels.extend(client_data[idx][1])\n",
    "            num_current += len(client_data[idx][0])\n",
    "\n",
    "        # 模型聚合 - FedAvg过程\n",
    "        w_agg = Aggregation(w_locals, None)  \n",
    "        \n",
    "        # 将聚合模型加载到全局模型中\n",
    "        net_glob.load_state_dict(w_agg)\n",
    "        \n",
    "        # 计算选定客户端数据的分布 (列表格式)\n",
    "        selected_client_labels = np.array(selected_client_labels)\n",
    "        P_t_prime = [0] * num_classes\n",
    "        for i, cls in enumerate(unique_classes):\n",
    "            P_t_prime[i] = np.sum(selected_client_labels == cls) / len(selected_client_labels) if len(selected_client_labels) > 0 else 0\n",
    "        \n",
    "        # 计算选定客户端数据的非IID程度\n",
    "        D_P_t_prime = calculate_js_divergence(P_t_prime, P)\n",
    "        \n",
    "        \n",
    "        # 评估聚合模型的准确率\n",
    "        test_model.load_state_dict(w_agg)\n",
    "        acc_t = test_inference(test_model, test_dataset) / 100.0  # 转换为[0,1]比例\n",
    "        \n",
    "        # 计算每个客户端的平均迭代次数\n",
    "        avg_iter = (num_current * K) / (M * bc_size)\n",
    "        \n",
    "        # 计算alpha (动态更新系数)\n",
    "        epsilon = 1e-10  # 避免除零\n",
    "        alpha = (1 - acc_t) * (n_0 * D_P_t_prime) / (n_0 * D_P_t_prime + num_current * D_P_0 + epsilon)\n",
    "        alpha = alpha * (decay_rate ** round) * du_C\n",
    "        \n",
    "        # 计算服务器更新迭代次数\n",
    "        server_iter = max(server_min, int(alpha * avg_iter))\n",
    "        \n",
    "        # 只有当alpha大于阈值时才进行服务器更新\n",
    "        if alpha > 0.001:\n",
    "            # 服务器数据训练实际迭代次数\n",
    "            actual_iter = math.ceil(n_0 / bc_size) * E\n",
    "            server_iter = min(actual_iter, server_iter)\n",
    "            \n",
    "            # 服务器更新\n",
    "            update_server_w, round_loss, _ = update_weights(copy.deepcopy(w_agg), server_data, gamma, E)\n",
    "            local_loss.append(round_loss)\n",
    "            \n",
    "            # 权重更新，使用ratio_combine函数\n",
    "            final_model = ratio_combine(w_agg, update_server_w, alpha)\n",
    "            net_glob.load_state_dict(final_model)\n",
    "            \n",
    "            print(f\"Round {round}: Server fixing with alpha={alpha:.4f}, iters={server_iter}/{actual_iter}\")\n",
    "        else:\n",
    "            # 如果alpha不够大，直接使用客户端聚合模型\n",
    "            final_model = copy.deepcopy(w_agg)\n",
    "            net_glob.load_state_dict(final_model)\n",
    "            \n",
    "            # 仍然计算服务器损失用于记录\n",
    "            _, round_loss, _ = update_weights(copy.deepcopy(w_agg), server_data, gamma, E)\n",
    "            local_loss.append(round_loss)\n",
    "        \n",
    "        # 测试模型性能\n",
    "        test_model.load_state_dict(final_model)\n",
    "        loss_avg = sum(local_loss) / len(local_loss)\n",
    "        train_loss.append(loss_avg)\n",
    "        \n",
    "        # 在所有测试数据上测试\n",
    "        current_acc = test_inference(test_model, test_dataset)\n",
    "        test_acc.append(current_acc)\n",
    "        \n",
    "        # 按照CLG_Mut_2策略进行mutation（使用全局更新方向作为mutation方向）\n",
    "        w_delta = FedSub(final_model, w_old, 1.0)\n",
    "        \n",
    "        # 计算模型更新w_delta的L2范数，衡量模型更新程度\n",
    "        rank = delta_rank(w_delta)\n",
    "        if rank > max_rank:\n",
    "            max_rank = rank\n",
    "            \n",
    "        # Mutation扩散，为下一轮准备初始模型\n",
    "        w_locals = mutation_spread(round, final_model, M, w_delta, radius)\n",
    "        \n",
    "        # 定期打印信息\n",
    "        if round % 10 == 0:\n",
    "            print(f\"Round {round}: Acc={current_acc:.2f}%, Loss={loss_avg:.4f}, D_P_t={D_P_t_prime:.4f}, Alpha={alpha:.4f}\")\n",
    "\n",
    "    return test_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机性（这里暂时不采用，先固定为false）\n",
    "data_random_fix = False  # 是否固定数据采样的随机性\n",
    "seed_num = 42\n",
    "# 新采用的全局随机性机制\n",
    "random_fix = True\n",
    "seed = 2\n",
    "\n",
    "GPU = 0  # 决定使用哪个gpu 0或1\n",
    "verbose = False  # 调试模式，输出一些中间信息\n",
    "\n",
    "client_num = 100\n",
    "size_per_client = 400  # 每个客户端的数据量（训练）\n",
    "is_iid = False  # True表示client数据IID分布，False表示Non-IID分布\n",
    "non_iid = 0.1  # Dirichlet 分布参数，数值越小数据越不均匀可根据需要调整\n",
    "\n",
    "server_iid = True # True代表server数据iid分布，否则为Non-iid分布（默认为0.5）\n",
    "server_percentage = 0.1  # 服务器端用于微调的数据比例\n",
    "\n",
    "# 模型相关\n",
    "origin_model = 'resnet' # 采用模型\n",
    "dataset = 'cifar10'\n",
    "\n",
    "momentum = 0.5\n",
    "weight_decay = 0  # 模型权重衰减参数，强制参数向0靠拢（和学习率衰减不一样！）这个是给我的原始代码中就是这样（设为0表示不引入）\n",
    "bc_size = 50\n",
    "test_bc_size = 128\n",
    "num_classes = 10  # 分别数量，CIFAR100中是20, CIFAR10是10\n",
    "\n",
    "# 联邦训练的超参数\n",
    "global_round = 2  # 全局训练轮数，可根据需要调整\n",
    "eta = 0.01  # 客户端端学习率，从{0.01, 0.1, 1}中调优\n",
    "gamma = 0.01  # 服务器端学习率 从{0.005， 0.05， 0.5中调有}\n",
    "K = 5  # 客户端本地训练轮数，从1，3，5中选\n",
    "E = 1  # 服务器本地训练轮数，从1，3，5中选\n",
    "M = 10  # 每一轮抽取客户端\n",
    "\n",
    "# FedMut中参数\n",
    "radius = 5.0  # alpha，控制mutation的幅度\n",
    "mut_acc_rate = 0.5  # 论文中的β0\n",
    "mut_bound = 50  # Tb\n",
    "\n",
    "# FedDU中参数\n",
    "du_C = 1\n",
    "decay_rate = 0.995  # 稍微提高衰减率以降低波动\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_random_seed(seed):\n",
    "    \"\"\"\n",
    "        set random seed\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:\" + str(GPU) if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# 固定随机数\n",
    "if random_fix:\n",
    "    set_random_seed(seed)\n",
    "\n",
    "if dataset == 'cifar100':\n",
    "# 准备CIFAR100数据集\n",
    "    cifar, test_dataset = CIFAR100()\n",
    "    prob = get_prob(non_iid, client_num, class_num=20, iid_mode=is_iid)\n",
    "    client_data = create_data_all_train(prob, size_per_client, cifar, N=20)   # 这里改为全部构建训练集\n",
    "\n",
    "    # 将测试标签转换为粗类别\n",
    "    test_dataset.targets = sparse2coarse(test_dataset.targets)\n",
    "    test_dataset.targets = test_dataset.targets.astype(int)\n",
    "\n",
    "\n",
    "    #CIFAR1--IID 挑选服务器子集：\n",
    "    if server_iid:\n",
    "        server_images, server_labels = select_server_subset(cifar, percentage=server_percentage, mode='iid')\n",
    "    else:\n",
    "        server_images, server_labels = select_server_subset(cifar, percentage=server_percentage,\n",
    "                                                        mode='non-iid', dirichlet_alpha=0.5)\n",
    "\n",
    "    init_model = ResNet18_cifar10().to(device)\n",
    "    initial_w = copy.deepcopy(init_model.state_dict())\n",
    "elif dataset =='shake':\n",
    "    # 准备shakespeare数据集\n",
    "    train_dataset = ShakeSpeare(True)\n",
    "    test_dataset = ShakeSpeare(False)\n",
    "\n",
    "    total_shake,total_label = [],[]\n",
    "    for item,labels in train_dataset:\n",
    "        total_shake.append(item.numpy())\n",
    "        total_label.append(labels)\n",
    "    total_shake = np.array(total_shake)\n",
    "    total_label = np.array(total_label)\n",
    "\n",
    "    shake = [total_shake, total_label]\n",
    "\n",
    "    # 构建每个client的数据量\n",
    "    dict_users = train_dataset.get_client_dic()\n",
    "\n",
    "    # 统计类别数量\n",
    "    unique_classes = np.unique(total_label)\n",
    "    num_classes = len(unique_classes)\n",
    "    print(\"shake数据集中类别数量：\", num_classes)\n",
    "    # 对于每个类别计算样本数量\n",
    "    class_counts = [np.sum(total_label == cls) for cls in unique_classes]\n",
    "    # 将数量转换成字符串后，用逗号隔开，并打印（只输出数字）\n",
    "    print(\", \".join(map(str, class_counts)))\n",
    "\n",
    "    # 统计客户端数量\n",
    "    num_clients = len(dict_users)\n",
    "    print(\"shake数据集中客户端数量：\", num_clients)\n",
    "\n",
    "    # 构建client_data\n",
    "    client_data = []\n",
    "    for client in sorted(dict_users.keys()):\n",
    "        indices = np.array(list(dict_users[client]), dtype=np.int64)\n",
    "        client_images = total_shake[indices]\n",
    "        client_labels = total_label[indices]\n",
    "        client_data.append((client_images, client_labels))\n",
    "\n",
    "    # Shake 挑选服务器子集，通过 Dirichlet 分布参数控制（例如 dirichlet_alpha=0.5）：\n",
    "    if server_iid:\n",
    "        server_images, server_labels = select_server_subset(shake, percentage=server_percentage,\n",
    "                                                      mode='iid')\n",
    "    else:\n",
    "        server_images, server_labels = select_server_subset(shake, percentage=server_percentage,\n",
    "                                                        mode='non-iid', dirichlet_alpha=0.5)\n",
    "\n",
    "    # Shakespeare —— 用FedMut中提出的LSTM网络\n",
    "    init_model = CharLSTM().to(device)\n",
    "    initial_w = copy.deepcopy(init_model.state_dict())\n",
    "elif dataset == \"cifar10\":\n",
    "    trans_cifar10_train = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "    trans_cifar10_val = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        \"./data/cifar10\", train=True, download=True, transform=trans_cifar10_train\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        \"./data/cifar10\", train=False, download=True, transform=trans_cifar10_val\n",
    "    )\n",
    "    \n",
    "    # 将训练集图像数据与标签转换成 numpy 数组（与 CIFAR-100 部分类似）\n",
    "    total_img, total_label = [], []\n",
    "    for img, label in train_dataset:\n",
    "        total_img.append(np.array(img))\n",
    "        total_label.append(label)\n",
    "    total_img = np.array(total_img)\n",
    "    total_label = np.array(total_label)\n",
    "    cifar = [total_img, total_label]\n",
    "\n",
    "    # 使用IID参数调用get_prob\n",
    "    prob = get_prob(non_iid, client_num, class_num=10, iid_mode=is_iid)\n",
    "    \n",
    "    # 构造每个客户端的本地数据（这里依然使用 create_data_all_train）\n",
    "    client_data = create_data_all_train(prob, size_per_client, cifar, N=10)\n",
    "    \n",
    "    \n",
    "    # 从训练集挑选出服务器训练使用的数据子集\n",
    "    if server_iid:\n",
    "        server_images, server_labels = select_server_subset(\n",
    "            cifar, percentage=server_percentage, mode=\"iid\"\n",
    "        )\n",
    "    else:\n",
    "        server_images, server_labels = select_server_subset(\n",
    "            cifar, percentage=server_percentage, mode=\"non-iid\", dirichlet_alpha=0.5\n",
    "        )\n",
    "    \n",
    "    if origin_model == 'cnn':    \n",
    "        # 初始化基于 CNN 的模型，这里使用你已定义好的 CNNCifar 网络\n",
    "        init_model = cnncifar().to(device)\n",
    "    elif origin_model == 'resnet':\n",
    "        init_model = ResNet18_cifar10().to(device)\n",
    "    \n",
    "    initial_w = copy.deepcopy(init_model.state_dict())\n",
    "\n",
    "\n",
    "#  打印数据集情况\n",
    "all_images = []\n",
    "all_labels = []\n",
    "for data in client_data:\n",
    "    all_images.extend(data[0])\n",
    "    all_labels.extend(data[1])\n",
    "comb_client_data = [np.array(all_images), np.array(all_labels)]\n",
    "\n",
    "# 输出comb_client_data情况\n",
    "imgs, lbls = comb_client_data\n",
    "lbls = np.array(lbls)\n",
    "total_count = len(lbls)\n",
    "unique_classes, counts = np.unique(lbls, return_counts=True)\n",
    "\n",
    "num_classes = int(unique_classes.max()) + 1  # 列表长度应该为最大类别\n",
    "class_counts = [0] * num_classes\n",
    "\n",
    "for cls, cnt in zip(unique_classes, counts):\n",
    "    class_counts[cls] = cnt\n",
    "\n",
    "# 打印格式：Total: 总数 类别0计数 类别1计数 ... 类别19计数\n",
    "print(\"Traning Client Total: {}\".format(\" \".join([str(total_count)] + [str(c) for c in class_counts])))\n",
    "\n",
    "\n",
    "# 打印每个客户端训练数据情况（只输出前10个）\n",
    "for i, (imgs, lbls) in enumerate(client_data[:10]):\n",
    "    lbls = np.array(lbls)\n",
    "    total_count = len(lbls)\n",
    "    unique_classes, counts = np.unique(lbls, return_counts=True)\n",
    "    \n",
    "    num_classes = int(unique_classes.max()) + 1  # 列表长度应该为最大类别\n",
    "    class_counts = [0] * num_classes\n",
    "    \n",
    "    for cls, cnt in zip(unique_classes, counts):\n",
    "        class_counts[cls] = cnt\n",
    "    # 打印格式：Client i: 总数 类别0计数 类别1计数 ... 类别19计数\n",
    "    print(\"Client {}: {}\".format(i, \" \".join([str(total_count)] + [str(c) for c in class_counts])))\n",
    "    \n",
    "\n",
    "# 为了与后续代码兼容，这里将 server_data 定义为一个列表：[images, labels]\n",
    "server_data = [server_images, server_labels]\n",
    "\n",
    "# # 输出测试集数据\n",
    "# total_count = len(test_dataset)\n",
    "# labels = np.array(test_dataset.label)\n",
    "# _, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "# print(f\"测试集总数量 {total_count}\")\n",
    "# print(\", \".join(str(c) for c in counts))\n",
    "\n",
    "# 打印服务器数据情况\n",
    "s_imgs, s_lbls = server_data\n",
    "s_lbls = np.array(s_lbls)\n",
    "total_count = len(s_lbls)\n",
    "unique_classes, counts = np.unique(s_lbls, return_counts=True)\n",
    "\n",
    "num_classes = int(unique_classes.max()) + 1  # 列表长度应该为最大类别+1\n",
    "class_counts = [0] * num_classes\n",
    "\n",
    "for cls, cnt in zip(unique_classes, counts):\n",
    "    class_counts[cls] = cnt\n",
    "# 输出格式: Server: 总数 类别0计数 类别1计数 ... 类别19计数\n",
    "print(\"Server: {}\".format(\" \".join([str(total_count)] + [str(c) for c in class_counts])))\n",
    "# print(\"  前5个标签: \", lbls[:5])\n",
    "# print(\"  前5个数据形状: \", [server_data[0][j].shape for j in range(min(5, len(server_data[0])))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 初始化结果存储字典\n",
    "# results_test_acc = {}\n",
    "# results_train_loss = {}\n",
    "\n",
    "# # # CLG_Mut 训练\n",
    "# # test_acc_CLG_Mut, train_loss_CLG_Mut = CLG_Mut(copy.deepcopy(init_model), global_round, eta, gamma, K, E, M)\n",
    "# # results_test_acc['CLG_Mut'] = test_acc_CLG_Mut\n",
    "# # results_train_loss['CLG_Mut'] = train_loss_CLG_Mut\n",
    "\n",
    "# # # CLG_Mut_2 训练\n",
    "# # test_acc_CLG_Mut_2, train_loss_CLG_Mut_2 = CLG_Mut_2(copy.deepcopy(init_model), global_round, eta, gamma, K, E, M)\n",
    "# # results_test_acc['CLG_Mut_2'] = test_acc_CLG_Mut_2\n",
    "# # results_train_loss['CLG_Mut_2'] = train_loss_CLG_Mut_2\n",
    "\n",
    "# # # CLG_Mut_3 训练\n",
    "# # test_acc_CLG_Mut_3, train_loss_CLG_Mut_3 = CLG_Mut_3(copy.deepcopy(init_model), global_round, eta, gamma, K, E, M)\n",
    "# # results_test_acc['CLG_Mut_3'] = test_acc_CLG_Mut_3\n",
    "# # results_train_loss['CLG_Mut_3'] = train_loss_CLG_Mut_3\n",
    "\n",
    "# # # FedMut 训练\n",
    "# # test_acc_FedMut, train_loss_FedMut = FedMut(copy.deepcopy(init_model), global_round, eta, K, M)\n",
    "# # results_test_acc['FedMut'] = test_acc_FedMut\n",
    "# # results_train_loss['FedMut'] = train_loss_FedMut\n",
    "\n",
    "# # # Server-only 训练\n",
    "# # test_acc_server_only, train_loss_server_only = server_only(initial_w, global_round, gamma, E)\n",
    "# # results_test_acc['Server_only'] = test_acc_server_only\n",
    "# # results_train_loss['Server_only'] = train_loss_server_only\n",
    "\n",
    "# # # FedAvg 训练\n",
    "# # test_acc_fedavg, train_loss_fedavg = fedavg(initial_w, global_round, eta, K, M)\n",
    "# # results_test_acc['FedAvg'] = test_acc_fedavg\n",
    "# # results_train_loss['FedAvg'] = train_loss_fedavg\n",
    "\n",
    "# # # CLG_SGD 训练\n",
    "# # test_acc_CLG_SGD, train_loss_CLG_SGD = CLG_SGD(initial_w, global_round, eta, gamma, K, E, M)\n",
    "# # results_test_acc['CLG_SGD'] = test_acc_CLG_SGD\n",
    "# # results_train_loss['CLG_SGD'] = train_loss_CLG_SGD\n",
    "\n",
    "# # FedDU 训练\n",
    "# test_acc_CLG_SGD, train_loss_CLG_SGD = FedDU_modify(initial_w, global_round, eta, gamma, K, E, M)\n",
    "# results_test_acc['FedDU'] = test_acc_CLG_SGD\n",
    "# results_train_loss['FedDU'] = train_loss_CLG_SGD\n",
    "\n",
    "# # FedDU-Mut 训练\n",
    "# test_acc_FedDU_Mut, train_loss_FedDU_Mut = FedDU_Mut(copy.deepcopy(init_model), global_round, eta, gamma, K, E, M)\n",
    "# results_test_acc['FedDU_Mut'] = test_acc_FedDU_Mut\n",
    "# results_train_loss['FedDU_Mut'] = train_loss_FedDU_Mut\n",
    "\n",
    "# # 如果存在至少20轮训练，则输出第二十轮的测试精度和训练损失\n",
    "# for algo in results_test_acc:\n",
    "#     if len(results_test_acc[algo]) >= 20:\n",
    "#         print(f\"{algo} - 第二十轮测试精度: {results_test_acc[algo][19]:.2f}%, 第二十轮训练损失: {results_train_loss[algo][19]:.4f}\")\n",
    "\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # 打印最终训练结果\n",
    "# for algo in results_test_acc:\n",
    "#     print(f\"{algo} - 最终测试精度: {results_test_acc[algo][-1]:.2f}%, 最终训练损失: {results_train_loss[algo][-1]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import matplotlib\n",
    "# import platform\n",
    "# import datetime\n",
    "\n",
    "\n",
    "# # 定义训练轮数\n",
    "# rounds = range(1, global_round + 1)\n",
    "\n",
    "# # 设置绘图风格（可选）\n",
    "# plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# # 获取当前时间戳，格式为 YYYYmmdd_HHMMSS\n",
    "# timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# # Plot Test Accuracy Comparison\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# for algo, acc in results_test_acc.items():\n",
    "#     plt.plot(rounds, acc, label=algo)\n",
    "# plt.xlabel('Training Rounds', fontsize=14)\n",
    "# plt.ylabel('Test Accuracy (%)', fontsize=14)\n",
    "# plt.title('Test Accuracy Comparison of Different Algorithms', fontsize=16)\n",
    "# plt.legend(fontsize=12)\n",
    "# plt.xticks(fontsize=12)\n",
    "# plt.yticks(fontsize=12)\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f'output/test_accuracy_{origin_model}_{timestamp}.png')  # 保存图像\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # Plot Train Loss Comparison\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# for algo, loss in results_train_loss.items():\n",
    "#     plt.plot(rounds, loss, label=algo)\n",
    "# plt.xlabel('Training Rounds', fontsize=14)\n",
    "# plt.ylabel('Train Loss', fontsize=14)\n",
    "# plt.title('Train Loss Comparison of Different Algorithms', fontsize=16)\n",
    "# plt.legend(fontsize=12)\n",
    "# plt.xticks(fontsize=12)\n",
    "# plt.yticks(fontsize=12)\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f'output/train_loss_{origin_model}_{timestamp}.png')  # 保存图像\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ablation study for raidus\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import matplotlib\n",
    "# import platform\n",
    "# import datetime\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "# # 确保output目录存在\n",
    "# os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# # 定义要测试的radius值\n",
    "# radius_values = [3, 4, 5, 6, 7]\n",
    "\n",
    "# # 创建结果存储结构 - 增加第20轮的结果\n",
    "# all_results = {\n",
    "#     \"radius\": [],\n",
    "#     # 第20轮结果\n",
    "#     # \"CLG_Mut_r20_acc\": [],\n",
    "#     # \"CLG_Mut_r20_loss\": [],\n",
    "#     \"CLG_Mut_2_r20_acc\": [],\n",
    "#     \"CLG_Mut_2_r20_loss\": [],\n",
    "#     \"CLG_Mut_3_r20_acc\": [],\n",
    "#     \"CLG_Mut_3_r20_loss\": [],\n",
    "#     # \"FedMut_r20_acc\": [],\n",
    "#     # \"FedMut_r20_loss\": [],\n",
    "#     \"FedDU_Mut_r20_acc\": [],\n",
    "#     \"FedDU_Mut_r20_loss\": [],\n",
    "#     # 最终轮结果\n",
    "#     # \"CLG_Mut_final_acc\": [],\n",
    "#     # \"CLG_Mut_final_loss\": [],\n",
    "#     \"CLG_Mut_2_final_acc\": [],\n",
    "#     \"CLG_Mut_2_final_loss\": [],\n",
    "#     \"CLG_Mut_3_final_acc\": [],\n",
    "#     \"CLG_Mut_3_final_loss\": [],\n",
    "#     # \"FedMut_final_acc\": [],\n",
    "#     # \"FedMut_final_loss\": [],\n",
    "#     \"FedDU_Mut_final_acc\": [],\n",
    "#     \"FedDU_Mut_final_loss\": [],\n",
    "# }\n",
    "\n",
    "# # 为每个radius值运行实验\n",
    "# for radius_val in radius_values:\n",
    "#     print(f\"\\n--- Running experiments with radius = {radius_val} ---\")\n",
    "\n",
    "#     # 更新全局radius参数\n",
    "#     radius = radius_val\n",
    "\n",
    "#     # 初始化结果存储字典\n",
    "#     results_test_acc = {}\n",
    "#     results_train_loss = {}\n",
    "\n",
    "#     # # CLG_Mut 训练\n",
    "#     # test_acc_CLG_Mut, train_loss_CLG_Mut = CLG_Mut(\n",
    "#     #     copy.deepcopy(init_model), global_round, eta, gamma, K, E, M\n",
    "#     # )\n",
    "#     # results_test_acc[\"CLG_Mut\"] = test_acc_CLG_Mut\n",
    "#     # results_train_loss[\"CLG_Mut\"] = train_loss_CLG_Mut\n",
    "\n",
    "#     # CLG_Mut_2 训练\n",
    "#     test_acc_CLG_Mut_2, train_loss_CLG_Mut_2 = CLG_Mut_2(\n",
    "#         copy.deepcopy(init_model), global_round, eta, gamma, K, E, M\n",
    "#     )\n",
    "#     results_test_acc[\"CLG_Mut_2\"] = test_acc_CLG_Mut_2\n",
    "#     results_train_loss[\"CLG_Mut_2\"] = train_loss_CLG_Mut_2\n",
    "\n",
    "#     # CLG_Mut_3 训练\n",
    "#     test_acc_CLG_Mut_3, train_loss_CLG_Mut_3 = CLG_Mut_3(\n",
    "#         copy.deepcopy(init_model), global_round, eta, gamma, K, E, M\n",
    "#     )\n",
    "#     results_test_acc[\"CLG_Mut_3\"] = test_acc_CLG_Mut_3\n",
    "#     results_train_loss[\"CLG_Mut_3\"] = train_loss_CLG_Mut_3\n",
    "\n",
    "#     # # FedMut 训练\n",
    "#     # test_acc_FedMut, train_loss_FedMut = FedMut(\n",
    "#     #     copy.deepcopy(init_model), global_round, eta, K, M\n",
    "#     # )\n",
    "#     # results_test_acc[\"FedMut\"] = test_acc_FedMut\n",
    "#     # results_train_loss[\"FedMut\"] = train_loss_FedMut\n",
    "    \n",
    "#     # 添加FedDU_Mut训练\n",
    "#     test_acc_FedDU_Mut, train_loss_FedDU_Mut = FedDU_Mut(copy.deepcopy(init_model), global_round, eta, gamma, K, E, M)\n",
    "#     results_test_acc[\"FedDU_Mut\"] = test_acc_FedDU_Mut\n",
    "#     results_train_loss[\"FedDU_Mut\"] = train_loss_FedDU_Mut\n",
    "\n",
    "#     # 保存当前radius结果\n",
    "#     all_results[\"radius\"].append(radius_val)\n",
    "\n",
    "#     # 保存第20轮结果 (第19个索引，因为索引从0开始)\n",
    "#     if len(results_test_acc[\"CLG_Mut_2\"]) >= 20:\n",
    "#         # all_results[\"CLG_Mut_r20_acc\"].append(results_test_acc[\"CLG_Mut\"][19])\n",
    "#         # all_results[\"CLG_Mut_r20_loss\"].append(results_train_loss[\"CLG_Mut\"][19])\n",
    "#         all_results[\"CLG_Mut_2_r20_acc\"].append(results_test_acc[\"CLG_Mut_2\"][19])\n",
    "#         all_results[\"CLG_Mut_2_r20_loss\"].append(results_train_loss[\"CLG_Mut_2\"][19])\n",
    "#         all_results[\"CLG_Mut_3_r20_acc\"].append(results_test_acc[\"CLG_Mut_3\"][19])\n",
    "#         all_results[\"CLG_Mut_3_r20_loss\"].append(results_train_loss[\"CLG_Mut_3\"][19])\n",
    "#         # all_results[\"FedMut_r20_acc\"].append(results_test_acc[\"FedMut\"][19])\n",
    "#         # all_results[\"FedMut_r20_loss\"].append(results_train_loss[\"FedMut\"][19])\n",
    "#         all_results[\"FedDU_Mut_r20_acc\"].append(results_test_acc[\"FedDU_Mut\"][19])\n",
    "#         all_results[\"FedDU_Mut_r20_loss\"].append(results_train_loss[\"FedDU_Mut\"][19])\n",
    "#     else:\n",
    "#         # 如果训练轮次不足20轮，使用最后一轮的结果代替空值\n",
    "#         # all_results[\"CLG_Mut_r20_acc\"].append(results_test_acc[\"CLG_Mut\"][-1])\n",
    "#         # all_results[\"CLG_Mut_r20_loss\"].append(results_train_loss[\"CLG_Mut\"][-1])\n",
    "#         all_results[\"CLG_Mut_2_r20_acc\"].append(results_test_acc[\"CLG_Mut_2\"][-1])\n",
    "#         all_results[\"CLG_Mut_2_r20_loss\"].append(results_train_loss[\"CLG_Mut_2\"][-1])\n",
    "#         all_results[\"CLG_Mut_3_r20_acc\"].append(results_test_acc[\"CLG_Mut_3\"][-1])\n",
    "#         all_results[\"CLG_Mut_3_r20_loss\"].append(results_train_loss[\"CLG_Mut_3\"][-1])\n",
    "#         # all_results[\"FedMut_r20_acc\"].append(results_test_acc[\"FedMut\"][-1])\n",
    "#         # all_results[\"FedMut_r20_loss\"].append(results_train_loss[\"FedMut\"][-1])\n",
    "#         all_results[\"FedDU_Mut_r20_acc\"].append(results_test_acc[\"FedDU_Mut\"][-1])\n",
    "#         all_results[\"FedDU_Mut_r20_loss\"].append(results_train_loss[\"FedDU_Mut\"][-1])\n",
    "\n",
    "#     # 保存最终轮结果\n",
    "#     # all_results[\"CLG_Mut_final_acc\"].append(results_test_acc[\"CLG_Mut\"][-1])\n",
    "#     # all_results[\"CLG_Mut_final_loss\"].append(results_train_loss[\"CLG_Mut\"][-1])\n",
    "#     all_results[\"CLG_Mut_2_final_acc\"].append(results_test_acc[\"CLG_Mut_2\"][-1])\n",
    "#     all_results[\"CLG_Mut_2_final_loss\"].append(results_train_loss[\"CLG_Mut_2\"][-1])\n",
    "#     all_results[\"CLG_Mut_3_final_acc\"].append(results_test_acc[\"CLG_Mut_3\"][-1])\n",
    "#     all_results[\"CLG_Mut_3_final_loss\"].append(results_train_loss[\"CLG_Mut_3\"][-1])\n",
    "#     # all_results[\"FedMut_final_acc\"].append(results_test_acc[\"FedMut\"][-1])\n",
    "#     # all_results[\"FedMut_final_loss\"].append(results_train_loss[\"FedMut\"][-1])\n",
    "#     all_results[\"FedDU_Mut_final_acc\"].append(results_test_acc[\"FedDU_Mut\"][-1])\n",
    "#     all_results[\"FedDU_Mut_final_loss\"].append(results_train_loss[\"FedDU_Mut\"][-1])\n",
    "\n",
    "#     # 打印当前radius的结果\n",
    "#     print(f\"\\nResults for radius = {radius_val}:\")\n",
    "#     for algo in results_test_acc:\n",
    "#         if len(results_test_acc[algo]) >= 20:\n",
    "#             print(\n",
    "#                 f\"{algo} - 第20轮测试精度: {results_test_acc[algo][19]:.2f}%, \"\n",
    "#                 f\"最终测试精度: {results_test_acc[algo][-1]:.2f}%, \"\n",
    "#                 f\"最终训练损失: {results_train_loss[algo][-1]:.4f}\"\n",
    "#             )\n",
    "#         else:\n",
    "#             print(\n",
    "#                 f\"{algo} - 最终测试精度: {results_test_acc[algo][-1]:.2f}%, \"\n",
    "#                 f\"最终训练损失: {results_train_loss[algo][-1]:.4f}\"\n",
    "#             )\n",
    "\n",
    "#     # ===== 为当前radius值绘制训练过程图 =====\n",
    "#     timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     rounds = range(1, global_round + 1)\n",
    "\n",
    "#     # 绘制测试精度图\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     for algo, acc in results_test_acc.items():\n",
    "#         plt.plot(rounds, acc, label=algo)\n",
    "#     plt.xlabel(\"Training Rounds\", fontsize=14)\n",
    "#     plt.ylabel(\"Test Accuracy (%)\", fontsize=14)\n",
    "#     plt.title(f\"Test Accuracy Comparison (radius={radius_val})\", fontsize=16)\n",
    "#     plt.legend(fontsize=12)\n",
    "#     plt.xticks(fontsize=12)\n",
    "#     plt.yticks(fontsize=12)\n",
    "#     plt.grid(True)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(\n",
    "#         f\"output/test_accuracy_radius{radius_val}_{origin_model}_{timestamp}.png\"\n",
    "#     )\n",
    "#     plt.show()\n",
    "\n",
    "#     # 绘制训练损失图\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     for algo, loss in results_train_loss.items():\n",
    "#         plt.plot(rounds, loss, label=algo)\n",
    "#     plt.xlabel(\"Training Rounds\", fontsize=14)\n",
    "#     plt.ylabel(\"Train Loss\", fontsize=14)\n",
    "#     plt.title(f\"Train Loss Comparison (radius={radius_val})\", fontsize=16)\n",
    "#     plt.legend(fontsize=12)\n",
    "#     plt.xticks(fontsize=12)\n",
    "#     plt.yticks(fontsize=12)\n",
    "#     plt.grid(True)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f\"output/train_loss_radius{radius_val}_{origin_model}_{timestamp}.png\")\n",
    "#     plt.show()\n",
    "\n",
    "# # 创建DataFrame\n",
    "# results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# # 设置精度显示格式\n",
    "# pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "# # 打印表格\n",
    "# print(\"\\n----- Final Results Table -----\")\n",
    "# print(results_df.to_string(index=False))\n",
    "\n",
    "# # 保存到CSV文件\n",
    "# timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# results_df.to_csv(\n",
    "#     f\"output/ablation/radius_comparison_{origin_model}_{timestamp}.csv\", index=False\n",
    "# )\n",
    "\n",
    "# # 确保所有结果数据都是数值型，不包含None\n",
    "# for col in results_df.columns:\n",
    "#     if col != \"radius\":\n",
    "#         results_df[col] = results_df[col].fillna(0)  # 将None替换为0\n",
    "\n",
    "# # 创建第20轮精度对比图\n",
    "# plt.figure(figsize=(14, 8))\n",
    "# # plt.plot(results_df[\"radius\"], results_df[\"CLG_Mut_r20_acc\"], \"o-\", label=\"CLG_Mut\")\n",
    "# plt.plot(results_df[\"radius\"], results_df[\"CLG_Mut_2_r20_acc\"], \"s-\", label=\"CLG_Mut_2\")\n",
    "# plt.plot(results_df[\"radius\"], results_df[\"CLG_Mut_3_r20_acc\"], \"^-\", label=\"CLG_Mut_3\")\n",
    "# # plt.plot(results_df[\"radius\"], results_df[\"FedMut_r20_acc\"], \"d-\", label=\"FedMut\")\n",
    "# plt.plot(results_df[\"radius\"], results_df[\"FedDU_Mut_r20_acc\"], \"*-\", label=\"FedDU_Mut\")\n",
    "# plt.xlabel(\"Radius Value\", fontsize=14)\n",
    "# plt.ylabel(\"Round 20 Test Accuracy (%)\", fontsize=14)\n",
    "# plt.title(f\"Effect of Radius on Round 20 Accuracy ({origin_model})\", fontsize=16)\n",
    "# plt.legend(fontsize=12)\n",
    "# plt.grid(True)\n",
    "# plt.xticks(results_df[\"radius\"])\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\n",
    "#     f\"output/ablation/radius_r20_accuracy_comparison_{origin_model}_{timestamp}.png\"\n",
    "# )\n",
    "# plt.show()\n",
    "\n",
    "# # 创建最终精度对比图\n",
    "# plt.figure(figsize=(14, 8))\n",
    "# # plt.plot(results_df[\"radius\"], results_df[\"CLG_Mut_final_acc\"], \"o-\", label=\"CLG_Mut\")\n",
    "# plt.plot(\n",
    "#     results_df[\"radius\"], results_df[\"CLG_Mut_2_final_acc\"], \"s-\", label=\"CLG_Mut_2\"\n",
    "# )\n",
    "# plt.plot(\n",
    "#     results_df[\"radius\"], results_df[\"CLG_Mut_3_final_acc\"], \"^-\", label=\"CLG_Mut_3\"\n",
    "# )\n",
    "# # plt.plot(results_df[\"radius\"], results_df[\"FedMut_final_acc\"], \"d-\", label=\"FedMut\")\n",
    "# plt.plot(results_df[\"radius\"], results_df[\"FedDU_Mut_final_acc\"], \"*-\", label=\"FedDU_Mut\")\n",
    "# plt.xlabel(\"Radius Value\", fontsize=14)\n",
    "# plt.ylabel(\"Final Test Accuracy (%)\", fontsize=14)\n",
    "# plt.title(f\"Effect of Radius on Final Accuracy ({origin_model})\", fontsize=16)\n",
    "# plt.legend(fontsize=12)\n",
    "# plt.grid(True)\n",
    "# plt.xticks(results_df[\"radius\"])\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\n",
    "#     f\"output/ablation/radius_final_accuracy_comparison_{origin_model}_{timestamp}.png\"\n",
    "# )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ablation study for acc_rate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import platform\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 确保output目录存在\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# 定义要测试的mut_acc_rate值\n",
    "mut_acc_rate_values = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "\n",
    "# 创建结果存储结构 - 增加第20轮的结果\n",
    "all_results = {\n",
    "    \"mut_acc_rate\": [],\n",
    "    # 第20轮结果\n",
    "    # \"CLG_Mut_r20_acc\": [],\n",
    "    # \"CLG_Mut_r20_loss\": [],\n",
    "    \"CLG_Mut_2_r20_acc\": [],\n",
    "    \"CLG_Mut_2_r20_loss\": [],\n",
    "    \"CLG_Mut_3_r20_acc\": [],\n",
    "    \"CLG_Mut_3_r20_loss\": [],\n",
    "    # \"FedMut_r20_acc\": [],\n",
    "    # \"FedMut_r20_loss\": [],\n",
    "    \"FedDU_Mut_r20_acc\": [],\n",
    "    \"FedDU_Mut_r20_loss\": [],\n",
    "    # 最终轮结果\n",
    "    # \"CLG_Mut_final_acc\": [],\n",
    "    # \"CLG_Mut_final_loss\": [],\n",
    "    \"CLG_Mut_2_final_acc\": [],\n",
    "    \"CLG_Mut_2_final_loss\": [],\n",
    "    \"CLG_Mut_3_final_acc\": [],\n",
    "    \"CLG_Mut_3_final_loss\": [],\n",
    "    # \"FedMut_final_acc\": [],\n",
    "    # \"FedMut_final_loss\": [],\n",
    "    \"FedDU_Mut_final_acc\": [],\n",
    "    \"FedDU_Mut_final_loss\": [],\n",
    "}\n",
    "\n",
    "# 为每个mut_acc_rate值运行实验\n",
    "for mut_acc_rate_val in mut_acc_rate_values:\n",
    "    print(f\"\\n--- Running experiments with mut_acc_rate = {mut_acc_rate_val} ---\")\n",
    "\n",
    "    # 更新全局mut_acc_rate参数\n",
    "    mut_acc_rate = mut_acc_rate_val\n",
    "\n",
    "    # 初始化结果存储字典\n",
    "    results_test_acc = {}\n",
    "    results_train_loss = {}\n",
    "\n",
    "    # # CLG_Mut 训练\n",
    "    # test_acc_CLG_Mut, train_loss_CLG_Mut = CLG_Mut(\n",
    "    #     copy.deepcopy(init_model), global_round, eta, gamma, K, E, M\n",
    "    # )\n",
    "    # results_test_acc[\"CLG_Mut\"] = test_acc_CLG_Mut\n",
    "    # results_train_loss[\"CLG_Mut\"] = train_loss_CLG_Mut\n",
    "\n",
    "    # CLG_Mut_2 训练\n",
    "    test_acc_CLG_Mut_2, train_loss_CLG_Mut_2 = CLG_Mut_2(\n",
    "        copy.deepcopy(init_model), global_round, eta, gamma, K, E, M\n",
    "    )\n",
    "    results_test_acc[\"CLG_Mut_2\"] = test_acc_CLG_Mut_2\n",
    "    results_train_loss[\"CLG_Mut_2\"] = train_loss_CLG_Mut_2\n",
    "\n",
    "    # CLG_Mut_3 训练\n",
    "    test_acc_CLG_Mut_3, train_loss_CLG_Mut_3 = CLG_Mut_3(\n",
    "        copy.deepcopy(init_model), global_round, eta, gamma, K, E, M\n",
    "    )\n",
    "    results_test_acc[\"CLG_Mut_3\"] = test_acc_CLG_Mut_3\n",
    "    results_train_loss[\"CLG_Mut_3\"] = train_loss_CLG_Mut_3\n",
    "\n",
    "    # # FedMut 训练\n",
    "    # test_acc_FedMut, train_loss_FedMut = FedMut(\n",
    "    #     copy.deepcopy(init_model), global_round, eta, K, M\n",
    "    # )\n",
    "    # results_test_acc[\"FedMut\"] = test_acc_FedMut\n",
    "    # results_train_loss[\"FedMut\"] = train_loss_FedMut\n",
    "    \n",
    "    # 添加FedDU_Mut训练\n",
    "    test_acc_FedDU_Mut, train_loss_FedDU_Mut = FedDU_Mut(copy.deepcopy(init_model), global_round, eta, gamma, K, E, M)\n",
    "    results_test_acc[\"FedDU_Mut\"] = test_acc_FedDU_Mut\n",
    "    results_train_loss[\"FedDU_Mut\"] = train_loss_FedDU_Mut\n",
    "\n",
    "    # 保存当前mut_acc_rate结果\n",
    "    all_results[\"mut_acc_rate\"].append(mut_acc_rate_val)\n",
    "\n",
    "   # 保存第20轮结果 (第19个索引，因为索引从0开始)\n",
    "    if len(results_test_acc[\"CLG_Mut_2\"]) >= 20:\n",
    "        # all_results[\"CLG_Mut_r20_acc\"].append(results_test_acc[\"CLG_Mut\"][19])\n",
    "        # all_results[\"CLG_Mut_r20_loss\"].append(results_train_loss[\"CLG_Mut\"][19])\n",
    "        all_results[\"CLG_Mut_2_r20_acc\"].append(results_test_acc[\"CLG_Mut_2\"][19])\n",
    "        all_results[\"CLG_Mut_2_r20_loss\"].append(results_train_loss[\"CLG_Mut_2\"][19])\n",
    "        all_results[\"CLG_Mut_3_r20_acc\"].append(results_test_acc[\"CLG_Mut_3\"][19])\n",
    "        all_results[\"CLG_Mut_3_r20_loss\"].append(results_train_loss[\"CLG_Mut_3\"][19])\n",
    "        # all_results[\"FedMut_r20_acc\"].append(results_test_acc[\"FedMut\"][19])\n",
    "        # all_results[\"FedMut_r20_loss\"].append(results_train_loss[\"FedMut\"][19])\n",
    "        all_results[\"FedDU_Mut_r20_acc\"].append(results_test_acc[\"FedDU_Mut\"][19])\n",
    "        all_results[\"FedDU_Mut_r20_loss\"].append(results_train_loss[\"FedDU_Mut\"][19])\n",
    "    else:\n",
    "        # 如果训练轮次不足20轮，使用最后一轮的结果代替空值\n",
    "        # all_results[\"CLG_Mut_r20_acc\"].append(results_test_acc[\"CLG_Mut\"][-1])\n",
    "        # all_results[\"CLG_Mut_r20_loss\"].append(results_train_loss[\"CLG_Mut\"][-1])\n",
    "        all_results[\"CLG_Mut_2_r20_acc\"].append(results_test_acc[\"CLG_Mut_2\"][-1])\n",
    "        all_results[\"CLG_Mut_2_r20_loss\"].append(results_train_loss[\"CLG_Mut_2\"][-1])\n",
    "        all_results[\"CLG_Mut_3_r20_acc\"].append(results_test_acc[\"CLG_Mut_3\"][-1])\n",
    "        all_results[\"CLG_Mut_3_r20_loss\"].append(results_train_loss[\"CLG_Mut_3\"][-1])\n",
    "        # all_results[\"FedMut_r20_acc\"].append(results_test_acc[\"FedMut\"][-1])\n",
    "        # all_results[\"FedMut_r20_loss\"].append(results_train_loss[\"FedMut\"][-1])\n",
    "        all_results[\"FedDU_Mut_r20_acc\"].append(results_test_acc[\"FedDU_Mut\"][-1])\n",
    "        all_results[\"FedDU_Mut_r20_loss\"].append(results_train_loss[\"FedDU_Mut\"][-1])\n",
    "\n",
    "    # 保存最终轮结果\n",
    "    # all_results[\"CLG_Mut_final_acc\"].append(results_test_acc[\"CLG_Mut\"][-1])\n",
    "    # all_results[\"CLG_Mut_final_loss\"].append(results_train_loss[\"CLG_Mut\"][-1])\n",
    "    all_results[\"CLG_Mut_2_final_acc\"].append(results_test_acc[\"CLG_Mut_2\"][-1])\n",
    "    all_results[\"CLG_Mut_2_final_loss\"].append(results_train_loss[\"CLG_Mut_2\"][-1])\n",
    "    all_results[\"CLG_Mut_3_final_acc\"].append(results_test_acc[\"CLG_Mut_3\"][-1])\n",
    "    all_results[\"CLG_Mut_3_final_loss\"].append(results_train_loss[\"CLG_Mut_3\"][-1])\n",
    "    # all_results[\"FedMut_final_acc\"].append(results_test_acc[\"FedMut\"][-1])\n",
    "    # all_results[\"FedMut_final_loss\"].append(results_train_loss[\"FedMut\"][-1])\n",
    "    all_results[\"FedDU_Mut_final_acc\"].append(results_test_acc[\"FedDU_Mut\"][-1])\n",
    "    all_results[\"FedDU_Mut_final_loss\"].append(results_train_loss[\"FedDU_Mut\"][-1])\n",
    "\n",
    "    # 打印当前mut_acc_rate的结果\n",
    "    print(f\"\\nResults for mut_acc_rate = {mut_acc_rate_val}:\")\n",
    "    for algo in results_test_acc:\n",
    "        if len(results_test_acc[algo]) >= 20:\n",
    "            print(\n",
    "                f\"{algo} - 第20轮测试精度: {results_test_acc[algo][19]:.2f}%, \"\n",
    "                f\"最终测试精度: {results_test_acc[algo][-1]:.2f}%, \"\n",
    "                f\"最终训练损失: {results_train_loss[algo][-1]:.4f}\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"{algo} - 最终测试精度: {results_test_acc[algo][-1]:.2f}%, \"\n",
    "                f\"最终训练损失: {results_train_loss[algo][-1]:.4f}\"\n",
    "            )\n",
    "\n",
    "    # ===== 为当前mut_acc_rate值绘制训练过程图 =====\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rounds = range(1, global_round + 1)\n",
    "\n",
    "    # 绘制测试精度图\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for algo, acc in results_test_acc.items():\n",
    "        plt.plot(rounds, acc, label=algo)\n",
    "    plt.xlabel(\"Training Rounds\", fontsize=14)\n",
    "    plt.ylabel(\"Test Accuracy (%)\", fontsize=14)\n",
    "    plt.title(\n",
    "        f\"Test Accuracy Comparison (mut_acc_rate={mut_acc_rate_val})\", fontsize=16\n",
    "    )\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        f\"output/test_accuracy_mut_acc_rate{mut_acc_rate_val}_{origin_model}_{timestamp}.png\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # 绘制训练损失图\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for algo, loss in results_train_loss.items():\n",
    "        plt.plot(rounds, loss, label=algo)\n",
    "    plt.xlabel(\"Training Rounds\", fontsize=14)\n",
    "    plt.ylabel(\"Train Loss\", fontsize=14)\n",
    "    plt.title(f\"Train Loss Comparison (mut_acc_rate={mut_acc_rate_val})\", fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        f\"output/train_loss_mut_acc_rate{mut_acc_rate_val}_{origin_model}_{timestamp}.png\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "# 创建DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# 设置精度显示格式\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "# 打印表格\n",
    "print(\"\\n----- Final Results Table -----\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# 保存到CSV文件\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_df.to_csv(\n",
    "    f\"output/ablation/mut_acc_rate_comparison_{origin_model}_{timestamp}.csv\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "# 确保所有结果数据都是数值型，不包含None\n",
    "for col in results_df.columns:\n",
    "    if col != \"mut_acc_rate\":\n",
    "        results_df[col] = results_df[col].fillna(0)  # 将None替换为0\n",
    "\n",
    "# 创建第20轮精度对比图\n",
    "plt.figure(figsize=(14, 8))\n",
    "# plt.plot(\n",
    "#     results_df[\"mut_acc_rate\"], results_df[\"CLG_Mut_r20_acc\"], \"o-\", label=\"CLG_Mut\"\n",
    "# )\n",
    "plt.plot(\n",
    "    results_df[\"mut_acc_rate\"], results_df[\"CLG_Mut_2_r20_acc\"], \"s-\", label=\"CLG_Mut_2\"\n",
    ")\n",
    "plt.plot(\n",
    "    results_df[\"mut_acc_rate\"], results_df[\"CLG_Mut_3_r20_acc\"], \"^-\", label=\"CLG_Mut_3\"\n",
    ")\n",
    "# plt.plot(results_df[\"mut_acc_rate\"], results_df[\"FedMut_r20_acc\"], \"d-\", label=\"FedMut\")\n",
    "plt.plot(results_df[\"mut_acc_rate\"], results_df[\"FedDU_Mut_r20_acc\"], \"*-\", label=\"FedDU_Mut\")\n",
    "plt.xlabel(\"mut_acc_rate Value\", fontsize=14)\n",
    "plt.ylabel(\"Round 20 Test Accuracy (%)\", fontsize=14)\n",
    "plt.title(f\"Effect of mut_acc_rate on Round 20 Accuracy ({origin_model})\", fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.xticks(results_df[\"mut_acc_rate\"])\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    f\"output/ablation/mut_acc_rate_r20_accuracy_comparison_{origin_model}_{timestamp}.png\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# 创建最终精度对比图\n",
    "plt.figure(figsize=(14, 8))\n",
    "# plt.plot(\n",
    "#     results_df[\"mut_acc_rate\"], results_df[\"CLG_Mut_final_acc\"], \"o-\", label=\"CLG_Mut\"\n",
    "# )\n",
    "plt.plot(\n",
    "    results_df[\"mut_acc_rate\"],\n",
    "    results_df[\"CLG_Mut_2_final_acc\"],\n",
    "    \"s-\",\n",
    "    label=\"CLG_Mut_2\",\n",
    ")\n",
    "plt.plot(\n",
    "    results_df[\"mut_acc_rate\"],\n",
    "    results_df[\"CLG_Mut_3_final_acc\"],\n",
    "    \"^-\",\n",
    "    label=\"CLG_Mut_3\",\n",
    ")\n",
    "# plt.plot(\n",
    "#     results_df[\"mut_acc_rate\"], results_df[\"FedMut_final_acc\"], \"d-\", label=\"FedMut\"\n",
    "# )\n",
    "plt.plot(results_df[\"mut_acc_rate\"], results_df[\"FedDU_Mut_final_acc\"], \"*-\", label=\"FedDU_Mut\")\n",
    "plt.xlabel(\"mut_acc_rate Value\", fontsize=14)\n",
    "plt.ylabel(\"Final Test Accuracy (%)\", fontsize=14)\n",
    "plt.title(f\"Effect of mut_acc_rate on Final Accuracy ({origin_model})\", fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.xticks(results_df[\"mut_acc_rate\"])\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    f\"output/ablation/mut_acc_rate_final_accuracy_comparison_{origin_model}_{timestamp}.png\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 消融实验 - FedDU参数 (du_C)\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import datetime\n",
    "# import copy\n",
    "\n",
    "# # 确保output目录存在\n",
    "# os.makedirs(\"output/ablation\", exist_ok=True)\n",
    "\n",
    "# # 第一部分：测试不同的du_C值\n",
    "# print(\"\\n===== 开始du_C参数消融实验 =====\")\n",
    "\n",
    "# # 定义要测试的du_C值\n",
    "# du_C_values = [0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "# # 创建结果存储结构\n",
    "# du_C_results = {\n",
    "#     \"du_C\": [],\n",
    "#     # 第20轮结果\n",
    "#     # \"FedDU_r20_acc\": [],\n",
    "#     # \"FedDU_r20_loss\": [],\n",
    "#     \"FedDU_Mut_r20_acc\": [],\n",
    "#     \"FedDU_Mut_r20_loss\": [],\n",
    "#     # 最终轮结果\n",
    "#     # \"FedDU_final_acc\": [],\n",
    "#     # \"FedDU_final_loss\": [],\n",
    "#     \"FedDU_Mut_final_acc\": [],\n",
    "#     \"FedDU_Mut_final_loss\": [],\n",
    "# }\n",
    "\n",
    "\n",
    "# # 为每个du_C值运行实验\n",
    "# for du_C_val in du_C_values:\n",
    "#     print(f\"\\n--- 测试 du_C = {du_C_val} ---\")\n",
    "    \n",
    "#     # 更新全局du_C参数\n",
    "#     du_C = du_C_val\n",
    "    \n",
    "#     # 初始化结果存储字典\n",
    "#     results_test_acc = {}\n",
    "#     results_train_loss = {}\n",
    "    \n",
    "#     # FedDU 训练\n",
    "#     test_acc_FedDU, train_loss_FedDU = FedDU_modify(\n",
    "#         initial_w, global_round, eta, gamma, K, E, M\n",
    "#     )\n",
    "#     results_test_acc[\"FedDU\"] = test_acc_FedDU\n",
    "#     results_train_loss[\"FedDU\"] = train_loss_FedDU\n",
    "    \n",
    "#     # FedDU_Mut 训练\n",
    "#     test_acc_FedDU_Mut, train_loss_FedDU_Mut = FedDU_Mut(\n",
    "#         copy.deepcopy(init_model), global_round, eta, gamma, K, E, M\n",
    "#     )\n",
    "#     results_test_acc[\"FedDU_Mut\"] = test_acc_FedDU_Mut\n",
    "#     results_train_loss[\"FedDU_Mut\"] = train_loss_FedDU_Mut\n",
    "    \n",
    "#     # 保存当前du_C结果\n",
    "#     du_C_results[\"du_C\"].append(du_C_val)\n",
    "    \n",
    "#     # 保存第20轮结果 (第19个索引，因为索引从0开始)\n",
    "#     if len(results_test_acc[\"FedDU_Mut\"]) >= 20:\n",
    "#         # du_C_results[\"FedDU_r20_acc\"].append(results_test_acc[\"FedDU\"][19])\n",
    "#         # du_C_results[\"FedDU_r20_loss\"].append(results_train_loss[\"FedDU\"][19])\n",
    "#         du_C_results[\"FedDU_Mut_r20_acc\"].append(results_test_acc[\"FedDU_Mut\"][19])\n",
    "#         du_C_results[\"FedDU_Mut_r20_loss\"].append(results_train_loss[\"FedDU_Mut\"][19])\n",
    "#     else:\n",
    "#         # 如果训练轮次不足20轮，使用最后一轮的结果\n",
    "#         # du_C_results[\"FedDU_r20_acc\"].append(results_test_acc[\"FedDU\"][-1])\n",
    "#         # du_C_results[\"FedDU_r20_loss\"].append(results_train_loss[\"FedDU\"][-1])\n",
    "#         du_C_results[\"FedDU_Mut_r20_acc\"].append(results_test_acc[\"FedDU_Mut\"][-1])\n",
    "#         du_C_results[\"FedDU_Mut_r20_loss\"].append(results_train_loss[\"FedDU_Mut\"][-1])\n",
    "    \n",
    "#     # 保存最终轮结果\n",
    "#     # du_C_results[\"FedDU_final_acc\"].append(results_test_acc[\"FedDU\"][-1])\n",
    "#     # du_C_results[\"FedDU_final_loss\"].append(results_train_loss[\"FedDU\"][-1])\n",
    "#     du_C_results[\"FedDU_Mut_final_acc\"].append(results_test_acc[\"FedDU_Mut\"][-1])\n",
    "#     du_C_results[\"FedDU_Mut_final_loss\"].append(results_train_loss[\"FedDU_Mut\"][-1])\n",
    "    \n",
    "#     # 打印当前du_C的结果\n",
    "#     print(f\"\\nResults for du_C = {du_C_val}:\")\n",
    "#     for algo in results_test_acc:\n",
    "#         if len(results_test_acc[algo]) >= 20:\n",
    "#             print(\n",
    "#                 f\"{algo} - 第20轮测试精度: {results_test_acc[algo][19]:.2f}%, \"\n",
    "#                 f\"最终测试精度: {results_test_acc[algo][-1]:.2f}%, \"\n",
    "#                 f\"最终训练损失: {results_train_loss[algo][-1]:.4f}\"\n",
    "#             )\n",
    "#         else:\n",
    "#             print(\n",
    "#                 f\"{algo} - 最终测试精度: {results_test_acc[algo][-1]:.2f}%, \"\n",
    "#                 f\"最终训练损失: {results_train_loss[algo][-1]:.4f}\"\n",
    "#             )\n",
    "    \n",
    "#     # 绘制训练过程图\n",
    "#     timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     rounds = range(1, global_round + 1)\n",
    "    \n",
    "#     # 绘制测试精度图\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     for algo, acc in results_test_acc.items():\n",
    "#         plt.plot(rounds, acc, label=algo)\n",
    "#     plt.xlabel(\"Training Rounds\", fontsize=14)\n",
    "#     plt.ylabel(\"Test Accuracy (%)\", fontsize=14)\n",
    "#     plt.title(f\"Test Accuracy Comparison (du_C={du_C_val})\", fontsize=16)\n",
    "#     plt.legend(fontsize=12)\n",
    "#     plt.grid(True)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f\"output/test_accuracy_du_C{du_C_val}_{origin_model}_{timestamp}.png\")\n",
    "#     plt.show()\n",
    "    \n",
    "#     # 绘制训练损失图\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     for algo, loss in results_train_loss.items():\n",
    "#         plt.plot(rounds, loss, label=algo)\n",
    "#     plt.xlabel(\"Training Rounds\", fontsize=14)\n",
    "#     plt.ylabel(\"Train Loss\", fontsize=14)\n",
    "#     plt.title(f\"Train Loss Comparison (du_C={du_C_val})\", fontsize=16)\n",
    "#     plt.legend(fontsize=12)\n",
    "#     plt.grid(True)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f\"output/train_loss_du_C{du_C_val}_{origin_model}_{timestamp}.png\")\n",
    "#     plt.show()\n",
    "\n",
    "# # 创建DataFrame\n",
    "# du_C_df = pd.DataFrame(du_C_results)\n",
    "\n",
    "# # 设置精度显示格式\n",
    "# pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "# # 打印表格\n",
    "# print(\"\\n----- du_C参数结果表 -----\")\n",
    "# print(du_C_df.to_string(index=False))\n",
    "\n",
    "# # 保存到CSV文件\n",
    "# timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# du_C_df.to_csv(f\"output/ablation/du_C_comparison_{origin_model}_{timestamp}.csv\", index=False)\n",
    "\n",
    "# # 绘制第20轮精度对比图\n",
    "# plt.figure(figsize=(14, 8))\n",
    "# # plt.plot(du_C_df[\"du_C\"], du_C_df[\"FedDU_r20_acc\"], \"o-\", label=\"FedDU\")\n",
    "# plt.plot(du_C_df[\"du_C\"], du_C_df[\"FedDU_Mut_r20_acc\"], \"s-\", label=\"FedDU_Mut\")\n",
    "# plt.xlabel(\"du_C Value\", fontsize=14)\n",
    "# plt.ylabel(\"Round 20 Test Accuracy (%)\", fontsize=14)\n",
    "# plt.title(f\"Effect of du_C on Round 20 Accuracy ({origin_model})\", fontsize=16)\n",
    "# plt.legend(fontsize=12)\n",
    "# plt.grid(True)\n",
    "# plt.xticks(du_C_df[\"du_C\"])\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f\"output/ablation/du_C_r20_accuracy_comparison_{origin_model}_{timestamp}.png\")\n",
    "# plt.show()\n",
    "\n",
    "# # 绘制最终精度对比图\n",
    "# plt.figure(figsize=(14, 8))\n",
    "# # plt.plot(du_C_df[\"du_C\"], du_C_df[\"FedDU_final_acc\"], \"o-\", label=\"FedDU\")\n",
    "# plt.plot(du_C_df[\"du_C\"], du_C_df[\"FedDU_Mut_final_acc\"], \"s-\", label=\"FedDU_Mut\")\n",
    "# plt.xlabel(\"du_C Value\", fontsize=14)\n",
    "# plt.ylabel(\"Final Test Accuracy (%)\", fontsize=14)\n",
    "# plt.title(f\"Effect of du_C on Final Accuracy ({origin_model})\", fontsize=16)\n",
    "# plt.legend(fontsize=12)\n",
    "# plt.grid(True)\n",
    "# plt.xticks(du_C_df[\"du_C\"])\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f\"output/ablation/du_C_final_accuracy_comparison_{origin_model}_{timestamp}.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 消融实验 - FedDU参数 (decay_rate)\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import datetime\n",
    "# import copy\n",
    "\n",
    "\n",
    "# # 定义要测试的decay_rate值\n",
    "# decay_rate_values = [0.9, 0.99, 0.995, 0.999, 1]\n",
    "\n",
    "# # 创建结果存储结构\n",
    "# decay_rate_results = {\n",
    "#     \"decay_rate\": [],\n",
    "#     # 第20轮结果\n",
    "#     # \"FedDU_r20_acc\": [],\n",
    "#     # \"FedDU_r20_loss\": [],\n",
    "#     \"FedDU_Mut_r20_acc\": [],\n",
    "#     \"FedDU_Mut_r20_loss\": [],\n",
    "#     # 最终轮结果\n",
    "#     # \"FedDU_final_acc\": [],\n",
    "#     # \"FedDU_final_loss\": [],\n",
    "#     \"FedDU_Mut_final_acc\": [],\n",
    "#     \"FedDU_Mut_final_loss\": [],\n",
    "# }\n",
    "\n",
    "# # 为每个decay_rate值运行实验\n",
    "# for decay_val in decay_rate_values:\n",
    "#     print(f\"\\n--- 测试 decay_rate = {decay_val} ---\")\n",
    "    \n",
    "#     # 更新全局decay_rate参数\n",
    "#     decay_rate = decay_val\n",
    "    \n",
    "#     # 初始化结果存储字典\n",
    "#     results_test_acc = {}\n",
    "#     results_train_loss = {}\n",
    "    \n",
    "#     # # FedDU 训练\n",
    "#     # test_acc_FedDU, train_loss_FedDU = FedDU_modify(\n",
    "#     #     initial_w, global_round, eta, gamma, K, E, M\n",
    "#     # )\n",
    "#     # results_test_acc[\"FedDU\"] = test_acc_FedDU\n",
    "#     # results_train_loss[\"FedDU\"] = train_loss_FedDU\n",
    "    \n",
    "#     # FedDU_Mut 训练\n",
    "#     test_acc_FedDU_Mut, train_loss_FedDU_Mut = FedDU_Mut(\n",
    "#         copy.deepcopy(init_model), global_round, eta, gamma, K, E, M\n",
    "#     )\n",
    "#     results_test_acc[\"FedDU_Mut\"] = test_acc_FedDU_Mut\n",
    "#     results_train_loss[\"FedDU_Mut\"] = train_loss_FedDU_Mut\n",
    "    \n",
    "#     # 保存当前decay_rate结果\n",
    "#     decay_rate_results[\"decay_rate\"].append(decay_val)\n",
    "    \n",
    "#     # 保存第20轮结果\n",
    "#     if len(results_test_acc[\"FedDU_Mut\"]) >= 20:\n",
    "#         # decay_rate_results[\"FedDU_r20_acc\"].append(results_test_acc[\"FedDU\"][19])\n",
    "#         # decay_rate_results[\"FedDU_r20_loss\"].append(results_train_loss[\"FedDU\"][19])\n",
    "#         decay_rate_results[\"FedDU_Mut_r20_acc\"].append(results_test_acc[\"FedDU_Mut\"][19])\n",
    "#         decay_rate_results[\"FedDU_Mut_r20_loss\"].append(results_train_loss[\"FedDU_Mut\"][19])\n",
    "#     else:\n",
    "#         # 如果训练轮次不足20轮，使用最后一轮的结果\n",
    "#         # decay_rate_results[\"FedDU_r20_acc\"].append(results_test_acc[\"FedDU\"][-1])\n",
    "#         # decay_rate_results[\"FedDU_r20_loss\"].append(results_train_loss[\"FedDU\"][-1])\n",
    "#         decay_rate_results[\"FedDU_Mut_r20_acc\"].append(results_test_acc[\"FedDU_Mut\"][-1])\n",
    "#         decay_rate_results[\"FedDU_Mut_r20_loss\"].append(results_train_loss[\"FedDU_Mut\"][-1])\n",
    "    \n",
    "#     # 保存最终轮结果\n",
    "#     # decay_rate_results[\"FedDU_final_acc\"].append(results_test_acc[\"FedDU\"][-1])\n",
    "#     # decay_rate_results[\"FedDU_final_loss\"].append(results_train_loss[\"FedDU\"][-1])\n",
    "#     decay_rate_results[\"FedDU_Mut_final_acc\"].append(results_test_acc[\"FedDU_Mut\"][-1])\n",
    "#     decay_rate_results[\"FedDU_Mut_final_loss\"].append(results_train_loss[\"FedDU_Mut\"][-1])\n",
    "    \n",
    "#     # 打印当前decay_rate的结果\n",
    "#     print(f\"\\nResults for decay_rate = {decay_val}:\")\n",
    "#     for algo in results_test_acc:\n",
    "#         if len(results_test_acc[algo]) >= 20:\n",
    "#             print(\n",
    "#                 f\"{algo} - 第20轮测试精度: {results_test_acc[algo][19]:.2f}%, \"\n",
    "#                 f\"最终测试精度: {results_test_acc[algo][-1]:.2f}%, \"\n",
    "#                 f\"最终训练损失: {results_train_loss[algo][-1]:.4f}\"\n",
    "#             )\n",
    "          \n",
    "#         else:\n",
    "#             print(\n",
    "#                 f\"{algo} - 最终测试精度: {results_test_acc[algo][-1]:.2f}%, \"\n",
    "#                 f\"最终训练损失: {results_train_loss[algo][-1]:.4f}\"\n",
    "#             )\n",
    "    \n",
    "#     # 绘制训练过程图\n",
    "#     timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#     rounds = range(1, global_round + 1)\n",
    "    \n",
    "#     # 绘制测试精度图\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     for algo, acc in results_test_acc.items():\n",
    "#         plt.plot(rounds, acc, label=algo)\n",
    "#     plt.xlabel(\"Training Rounds\", fontsize=14)\n",
    "#     plt.ylabel(\"Test Accuracy (%)\", fontsize=14)\n",
    "#     plt.title(f\"Test Accuracy Comparison (decay_rate={decay_val})\", fontsize=16)\n",
    "#     plt.legend(fontsize=12)\n",
    "#     plt.grid(True)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f\"output/test_accuracy_decay_rate{decay_val}_{origin_model}_{timestamp}.png\")\n",
    "#     plt.show()\n",
    "    \n",
    "#     # 绘制训练损失图\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     for algo, loss in results_train_loss.items():\n",
    "#         plt.plot(rounds, loss, label=algo)\n",
    "#     plt.xlabel(\"Training Rounds\", fontsize=14)\n",
    "#     plt.ylabel(\"Train Loss\", fontsize=14)\n",
    "#     plt.title(f\"Train Loss Comparison (decay_rate={decay_val})\", fontsize=16)\n",
    "#     plt.legend(fontsize=12)\n",
    "#     plt.grid(True)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f\"output/train_loss_decay_rate{decay_val}_{origin_model}_{timestamp}.png\")\n",
    "#     plt.show()\n",
    "\n",
    "# # 创建DataFrame\n",
    "# decay_rate_df = pd.DataFrame(decay_rate_results)\n",
    "\n",
    "# # 打印表格\n",
    "# print(\"\\n----- decay_rate参数结果表 -----\")\n",
    "# print(decay_rate_df.to_string(index=False))\n",
    "\n",
    "# # 保存到CSV文件\n",
    "# timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# decay_rate_df.to_csv(f\"output/ablation/decay_rate_comparison_{origin_model}_{timestamp}.csv\", index=False)\n",
    "\n",
    "# # 绘制第20轮精度对比图\n",
    "# plt.figure(figsize=(14, 8))\n",
    "# # plt.plot(decay_rate_df[\"decay_rate\"], decay_rate_df[\"FedDU_r20_acc\"], \"o-\", label=\"FedDU\")\n",
    "# plt.plot(decay_rate_df[\"decay_rate\"], decay_rate_df[\"FedDU_Mut_r20_acc\"], \"s-\", label=\"FedDU_Mut\")\n",
    "# plt.xlabel(\"decay_rate Value\", fontsize=14)\n",
    "# plt.ylabel(\"Round 20 Test Accuracy (%)\", fontsize=14)\n",
    "# plt.title(f\"Effect of decay_rate on Round 20 Accuracy ({origin_model})\", fontsize=16)\n",
    "# plt.legend(fontsize=12)\n",
    "# plt.grid(True)\n",
    "# plt.xticks(decay_rate_df[\"decay_rate\"])\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f\"output/ablation/decay_rate_r20_accuracy_comparison_{origin_model}_{timestamp}.png\")\n",
    "# plt.show()\n",
    "\n",
    "# # 绘制最终精度对比图\n",
    "# plt.figure(figsize=(14, 8))\n",
    "# # plt.plot(decay_rate_df[\"decay_rate\"], decay_rate_df[\"FedDU_final_acc\"], \"o-\", label=\"FedDU\")\n",
    "# plt.plot(decay_rate_df[\"decay_rate\"], decay_rate_df[\"FedDU_Mut_final_acc\"], \"s-\", label=\"FedDU_Mut\")\n",
    "# plt.xlabel(\"decay_rate Value\", fontsize=14)\n",
    "# plt.ylabel(\"Final Test Accuracy (%)\", fontsize=14)\n",
    "# plt.title(f\"Effect of decay_rate on Final Accuracy ({origin_model})\", fontsize=16)\n",
    "# plt.legend(fontsize=12)\n",
    "# plt.grid(True)\n",
    "# plt.xticks(decay_rate_df[\"decay_rate\"])\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f\"output/ablation/decay_rate_final_accuracy_comparison_{origin_model}_{timestamp}.png\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
